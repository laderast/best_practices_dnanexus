[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Best Practices on the DNAnexus Platform",
    "section": "",
    "text": "Preface\nThis is a companion book to Bash for Bioinformatics.\nIn this book we attempt to highlight best practices on the DNAnexus platform given some previous knowledge."
  },
  {
    "objectID": "index.html#learning-objectives",
    "href": "index.html#learning-objectives",
    "title": "Best Practices on the DNAnexus Platform",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter reading this book and doing the exercises, you should be able to:\n\nExplain the object based filesystem on the DNAnexus platform and how it differs from Linux/Unix POSIX filesystems.\nExplain key details of the UKB Research Analysis Platform and how these details impact your work.\nUtilize Projects and Organizations to effectively manage data for your group.\nUse Spark to connect, extract, and manipulate Apollo Datasets on the platform.\nUtilize Hail to load, query, model, annotate, and visualize large-scale genomics data such as the Exome Data on the UK Biobank Research Analysis Platform.\nBuild native DNAnexus apps effectively that manage inputs, outputs, and work with batch mode.\nUtilize existing and build native DNAnexus workflows using Workflow Description Language (WDL).\nExecute NextFlow pipelines on the DNAnexus Platform.\n\nOur goal is to bring information together in a task-oriented format to achieve things on the platform."
  },
  {
    "objectID": "index.html#four-levels-of-using-dnanexus",
    "href": "index.html#four-levels-of-using-dnanexus",
    "title": "Best Practices on the DNAnexus Platform",
    "section": "Four Levels of Using DNAnexus",
    "text": "Four Levels of Using DNAnexus\nOne way to approach learning DNAnexus is to think about the skills you need to process a number of files. Ben Busby has noted there are 4 main skill levels in processing files on the DNAnexus platform:\n\n\n\n\n\n\n\n\nLevel\n# of Files\nSkill\n\n\n\n\n1\n1\nInteractive Analysis (Cloud Workstation, JupyterLab)\n\n\n2\n1-50 Files\ndx run, Swiss Army Knife\n\n\n3\n50-1000 Files\nBuilding your own apps\n\n\n4\n1000+ Files, multiple steps\nUsing WDL (Workflow Description Language)\n\n\n\nWe’ll be covering mostly level 3 and 4 in this book. But you will need to be at level 2 before you can tackle these topics.\nThe key is to gradually build on your skills."
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Best Practices on the DNAnexus Platform",
    "section": "Prerequisites",
    "text": "Prerequisites\nBefore you tackle this book, you should be able to accomplish the following:\n\nUtilize Basic Bash Skills for DNAnexus\nUnderstand the basic architecture of Cloud Computing\nKnow how to edit and utilize JSON Files\nBe familiar with dx-toolkit commands, including:\n\ndx run\ndx find data and dx find jobs/dx watch\n\n\nWe recommend reviewing Bash for Bioinformatics if you need to brush up on these prerequisite skills."
  },
  {
    "objectID": "index.html#want-to-be-a-contributor",
    "href": "index.html#want-to-be-a-contributor",
    "title": "Best Practices on the DNAnexus Platform",
    "section": "Want to be a Contributor?",
    "text": "Want to be a Contributor?\nThis is the first draft of this book. It’s not going to be perfect, and we need help.\nIf you find a problem/issue or have a question, you can file it as an issue using this link.\nIn your issue, please note the following:\n\nYour Name\nWhat your issue was\nWhich section, and line you found problematic or wouldn’t run\n\nIf you have large edits: If you’re quarto/GitHub savvy, you can fork and file a pull request for typos/edits. If you’re not, you can file an issue.\nJust be aware that this is not my primary job - I’ll try to be as responsive as I can.\nAs-Is Software Disclaimer\nThis content in this book is delivered “As-Is”. Notwithstanding anything to the contrary, DNAnexus will have no warranty, support, liability or other obligations with respect to Materials provided hereunder."
  },
  {
    "objectID": "index.html#licensing",
    "href": "index.html#licensing",
    "title": "Best Practices on the DNAnexus Platform",
    "section": "Licensing",
    "text": "Licensing\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "10-object-based-file-systems.html#learning-objectives",
    "href": "10-object-based-file-systems.html#learning-objectives",
    "title": "1  Object Based File Systems",
    "section": "1.1 Learning Objectives",
    "text": "1.1 Learning Objectives\nThe goal of this chapter is to show how the DNAnexus filesystem is different from your previous experiences, and to highlight ways to work successfully on the platform given this filesystem. Specifically you will be able to:\n\nCompare and contrast differences between object-based file systems and POSIX filesystems\nExplain the role of metadata and folders in organizing data and outputs\nSet up a project for reproducible analysis\nTag and utilize tags for selecting files and separating output from multiple jobs."
  },
  {
    "objectID": "10-object-based-file-systems.html#terminology",
    "href": "10-object-based-file-systems.html#terminology",
    "title": "1  Object Based File Systems",
    "section": "1.2 Terminology",
    "text": "1.2 Terminology\n\nPOSIX filesystem - What we think of when we think of Linux filesystems. A set of standards that define how files are created and managed. POSIX filesystems have paths and files.\nData - The actual file contents. For example, for a CSV file, the actual header and rows correspond to the data.\nMetadata - information that is not part of the data, but is associated with the data. For our CSV file, some examples of metadata are the file permissions (who can access the file), the creator, and the creation date.\nObject - A contiguous set of memory that contains both the data and metadata. Has a unique identifier.\nUnique Identifier - the actual “pointer” for accessing the file. Unique to the file object when it is created. Does not change for the entire lifecycle of an object.\nDatabase Engine - software that allows for rapid searching and retrieving objects."
  },
  {
    "objectID": "10-object-based-file-systems.html#review-posix-filesystems",
    "href": "10-object-based-file-systems.html#review-posix-filesystems",
    "title": "1  Object Based File Systems",
    "section": "1.3 Review: POSIX Filesystems",
    "text": "1.3 Review: POSIX Filesystems\nYou may be familiar with POSIX (Portable Operating System Interface) file systems as a way to organize your data in folders. The POSIX standard basically defines the filesystem API so that they will be compatible with other file systems.\nIn POSIX filesystems, the main way to find/refer to a file is through its path. What does this mean?\nFor example, if my file is called chr1.vcf.gz and it is located in the directory /Geno_Data, we’d refer to it using the full path:\n/Geno_Data/chr1.vcf.gz\nPaths are the main way we organize and group files for POSIX filesystems (even devices are mapped to paths, such as /dev/hdd1 for an external hard drive). This information is external to a file.\nMost importantly, we use directories to organize and group files logically. For example, we might have our R script and the data it analyzes in the same directory.\nFor a file in a POSIX-based filesystem, the path needs to be unique. If they are not, there are transactional rules that are part of the POSIX specification that define when to replace that file with a new one, or to save both versions. For example, we can’t have two files named:\n/Geno_Data/chr1.vcf.gz\nIn the same folder. That violates our ability to find a file in the system.\nhttps://www.computerweekly.com/feature/Posix-vs-object-storage-How-much-longer-for-Posix https://grimoire.carcano.ch/blog/posix-compliant-filesystems/"
  },
  {
    "objectID": "10-object-based-file-systems.html#object-based-filesystems-are-different",
    "href": "10-object-based-file-systems.html#object-based-filesystems-are-different",
    "title": "1  Object Based File Systems",
    "section": "1.4 Object Based Filesystems are different",
    "text": "1.4 Object Based Filesystems are different\nIn contrast, object-based filesystems do not organize data like a POSIX filesystem. Each file object (such as a csv file, or a BAM file) has a unique identifier that identifies the data object. This unique identifier (like file-aagejFHEJSEI) serves as the main pointer to locate the data object, rather than a path. We’ll see that we can define paths and folders for DNAnexus file objects, but they are stored as part of the metadata for file objects.\nFile objects also have metadata that can be attached to them. This metadata can be:\n\n\n\nMetadata Type\nExample\nCode Example\n\n\n\n\nID\nproject-XXXXX:file-YYYYYY\ndx mv file-YYYY raw_data/\n\n\nname\nchr1.tar.gz\ndx find data --name chr1.tar.gz\n\n\nPath\n/raw_data/chr1.tar.gz\n\n\n\nCreation Date\n\n\n\n\nTags\nreport, run1\n\n\n\nProperties\neid = 1153xxx\n\n\n\n\nOn the DNAnexus platform, both the filename and its path are considered metadata. This metadata is considered as part of the object, along with the data portion of the object.\nImportantly, folders are not considered objects on the platform - folders only exist within the metadata for file objects.\nThe other issue is that the metadata for an object has no requirements to be unique, unlike paths. Which means you can have duplicates with the same file name in the same folder.\nI know, this can be very distressing for most people. You can have two objects with the same file name, but they are considered distinct objects because they have unique identifiers.\nhttps://www.ibm.com/cloud/blog/object-vs-file-vs-block-storage"
  },
  {
    "objectID": "10-object-based-file-systems.html#comparing-posix-and-object-based-file-systems",
    "href": "10-object-based-file-systems.html#comparing-posix-and-object-based-file-systems",
    "title": "1  Object Based File Systems",
    "section": "1.5 Comparing POSIX and Object Based File Systems",
    "text": "1.5 Comparing POSIX and Object Based File Systems\n\n\n\nConcept\nPOSIX File System\nObject-Based System\n\n\n\n\nFile ID\nRepresented by Full Path\nRepresented by Object ID\n\n\nStorage\nData with limited metadata\nMetadata+Data\n\n\nPath/Filename\nMust be Unique\nCan be duplicated\n\n\nMetadata\nLimited\nRich, can be freely modified"
  },
  {
    "objectID": "10-object-based-file-systems.html#tracing-the-journey-of-a-file-object-onto-the-platform",
    "href": "10-object-based-file-systems.html#tracing-the-journey-of-a-file-object-onto-the-platform",
    "title": "1  Object Based File Systems",
    "section": "1.6 Tracing the journey of a file object onto the platform",
    "text": "1.6 Tracing the journey of a file object onto the platform\nWhen a file uploaded, file objects go through three stages before they are available. These stages are:\n\nOpen - Files are still being transferred via dx upload or the Upload Agent ua.\nClosing - File objects stay in this state for no longer than 8-10 seconds.\nClosed - Files are now available to be utilized on the platform, either with an app, workflow, or downloaded.\n\n% dx describe file-FpQKQpQ0Fgk3gQZz3gPXQj7x\nResult 1:\nID                    file-FpQKQpQ0Fgk3gQZz3gPXQj7x\nClass                 file\nProject               project-GJ496B00FZfxPV5VG36FybvY\nFolder                /data\nName                  NA12878.bai\nState                 closed\nVisibility            visible\nTypes                 -\nProperties            -\nTags                  -\nOutgoing links        -\nCreated               Wed Apr 22 17:59:22 2020\nCreated by            emiai\n via the job          job-FpQGX780FgkG4bGz86zZk04V\nLast modified         Thu Oct 13 15:38:04 2022\nMedia type            application/octet-stream\narchivalState         \"live\"\nSize                  2.09 MB, sponsored by DNAnexus\ncloudAccount          \"cloudaccount-dnanexus\""
  },
  {
    "objectID": "10-object-based-file-systems.html#copying-files-from-one-project-to-another",
    "href": "10-object-based-file-systems.html#copying-files-from-one-project-to-another",
    "title": "1  Object Based File Systems",
    "section": "1.7 Copying Files from One Project to Another",
    "text": "1.7 Copying Files from One Project to Another\nCopying has an important definition on the platform: it means copying a file from one project to another project. It doesn’t refer to duplicating a file within a project.\nWhen we copy a file from one project to another, a new physical copy is not made. The reference to the original file is copied. The data is identical and points to the same location on the platform, and the metadata is copied to the new project.\nThis is quite nice in that you are not doubly charged for storage on the platform. You do have the ability to clone files into a different project - if you clone a file, a new physical copy of the data is created.\nOnce the metadata is copied into the new project, there is no syncing of the metadata between the two projects. User 1 is free to modify the metadata in Project A (the original project) and changes are not made to the metadata in Project B (the derived project).\n\n\n\n\n\n\nRegions on the DNAnexus Platform\n\n\n\nAll projects live within a region. As DNAnexus is built on both AWS and Azure, they map to regions within these service providers. Check here for a table of how the DNAnexus region maps to the service providers.\nFor example, the UKB RAP region (eu-west-2) is unique and only limited to projects within the scope of UK Biobank.\nhttps://documentation.dnanexus.com/developer/api/regions\n\n\n\n1.7.1 Advantages of Object Based Filesystems\nThe DNAnexus platform is only one example of an object-based filesystem. Other examples include Amazon S3 (Simple Storage Service), Microsoft Azure Blob Storage, and Google Cloud Storage.\nWhy does the world run on object based filesystems? There are a lot of advantages.\n\nHighly scalable. This is the main reason given for using an object-based system. Given that unique identifier, the data part of the object can be very large.\nFast Retrieval. Object-based filesystems let us work with arbitrarily large file sizes, and we can actually stream files to and from workers.\nImproved search speed. You can attach various database engines to a set of objects and rapidly search through them. An example of such an engine is Snowflake.\nFile operations are simplified. Compared to POSIX filesystems, there are only a few object filesystem commands: PUT, GET, DELETE, POST, HEAD.\n\n\n\n1.7.2 Disadvantages of Object Based Filesystems\nComing from folder-based/POSIX filesystems, it can be a bit of a mind-bender getting used to object-based filesystems. Some of the disadvantages of Object Based Filesystems include:\n\nObjects are immutable. Once an object is created, you are not able to modify it or edit it in place. If you modify a file on a worker, you can’t overwrite the original file object. A new file object must be created. (If you work with JupyterLab and save a notebook, a new file object is created in the place of the old file object. The old file object persists in a folder called .Notebook_archive.)\nYou have to be careful when generating outputs. This can be difficult if you have to resume a set of failed jobs in a new run. You can end up with two different objects with the same filename, and it can be some work to disambiguate these objects.\nIt’s confusing. You can actually have two files with the same filename in the same folder, because it is part of the changeable metadata. Disambiguating these two files without using file-ids can be difficult. There are rules that govern this.\nMetadata is much more important with file management. Seriously, use tags for everything, including jobs and files. It will make working with multiple files much easier. And if you are on UKB RAP, leverage the file property metadata (eid and field_id) to help you select the files you want to process."
  },
  {
    "objectID": "10-object-based-file-systems.html#always-add-project-ids-to-your-file-ids",
    "href": "10-object-based-file-systems.html#always-add-project-ids-to-your-file-ids",
    "title": "1  Object Based File Systems",
    "section": "1.8 Always add project IDs to your File IDs",
    "text": "1.8 Always add project IDs to your File IDs\nIn Bash for Bioinformatics, we already discovered one way of working with files on the platform: dx find data. This is our main tool for selecting files with metadata.\nWhen we work with files outside of our current project, we might reference it by a file-id. Using file IDs by themselves are a global operation and we need to be careful when we use this!\nWhy is this so? There is a search tree when we use a file ID that is not in our project and without a project context. The search over metadata is looking for a file object based on just file ID.\n\nLook for the file in the current project\nLook at all files across all other projects\n\nIf you want to use the platform effectively, you want to avoid #2 at all costs, especially when working with a lot of files. The metadata server will take much longer to find your files because it needs to scan more projects.\nThe lesson here is when using file-ids, it is safer to put the project-id in front of your file id such as below:\nproject-XXXXX:file-YYYYYYYYY"
  },
  {
    "objectID": "10-object-based-file-systems.html#batch-tagging",
    "href": "10-object-based-file-systems.html#batch-tagging",
    "title": "1  Object Based File Systems",
    "section": "1.9 Batch Tagging",
    "text": "1.9 Batch Tagging\nSo, we need to tag files. Remember, we can leverage xargs for tagging multiple files. In this example, we pipe the output of dx find data into xargs to batch tag them.\n\ndx find data --name \"*.bam\" --brief | xargs -I% sh -c \"dx tag % 'bam'\"\n\nAfter we do this, we can check whether our operation was successful. We can run:\n\ndx find data --tag bam --brief\n\nAnd here is our response:\nproject-GJ496B00FZfxPV5VG36FybvY:file-BZ9YGzj0x05b66kqQv51011q\nproject-GJ496B00FZfxPV5VG36FybvY:file-BZ9YGpj0x05xKxZ42QPqZkJY\nproject-GJ496B00FZfxPV5VG36FybvY:file-BQbXVY0093Jk1KVY1J082y7v\nproject-GJ496B00FZfxPV5VG36FybvY:file-FpQKQk00FgkGV3Vb3jJ8xqGV\n\nUsing the dx find data/xargs/dx tag combination with various input parameters to dx find data such as --name, --created-before, --created-after, --class, will help us to batch tag files and other objects on the platform."
  },
  {
    "objectID": "10-object-based-file-systems.html#use-case-use-tags-for-archiving",
    "href": "10-object-based-file-systems.html#use-case-use-tags-for-archiving",
    "title": "1  Object Based File Systems",
    "section": "1.10 Use Case: Use tags for archiving",
    "text": "1.10 Use Case: Use tags for archiving\nLet’s do something concrete and useful in our project: tag files we no longer need for archiving.\nSay there are files we want to archive. We can use dx tag or the DNAnexus UI to tag these files with a specific tag, such as to_archive. This can be done by users.\nAn administrator can then run a monthly job that archives the files with these tags using dx api <project-id> archive --tag to_archive."
  },
  {
    "objectID": "10-object-based-file-systems.html#what-about-dxfuse",
    "href": "10-object-based-file-systems.html#what-about-dxfuse",
    "title": "1  Object Based File Systems",
    "section": "1.11 What about dxFUSE?",
    "text": "1.11 What about dxFUSE?\n(See Bash for Bioinformatics for a introduction to dxFUSE.)\nYou might ask about the role of dxFUSE with the Object Based Filesystem.\nIn short, dxFUSE makes the Object Based Filesystem of DNAnexus act like a POSIX filesystem. Specifically, if there are multiple objects with the same name within a folder, it provides a way to specify these objects using file paths.\nOne important thing to remember: dxFUSE is only mounted once when you start the JupyterLab App. You will have to use the dxfuse command to remount it to see any files you’ve added to project storage after opening JupyterLab.\ndxfuse -\n\n1.11.1 dxFUSE: -limitedWrite\nFor the most part, dxFUSE is read-only from project storage. However, it has a -limitedWrite option that is mostly used from writing from Spark into project storage.\ndxfuse -limitedWrite\nOnce you have remounted with -limitedWrite, you’ll be able to specify file:///mnt/project urls in your .write() command. This is especially useful when calculating and saving BGEN Index files."
  },
  {
    "objectID": "12-project_management.html",
    "href": "12-project_management.html",
    "title": "2  Project and File Management",
    "section": "",
    "text": "3 Project Administration\nIn this section, we will talk about roles and what operations can be enabled/disabled in a project."
  },
  {
    "objectID": "12-project_management.html#learning-objectives",
    "href": "12-project_management.html#learning-objectives",
    "title": "2  Project and File Management",
    "section": "2.1 Learning Objectives",
    "text": "2.1 Learning Objectives\n\nExplain the role of organizations in project and member management\nUtilize different management strategies for administering your data to users\nDescribe the roles that can be assigned to projects and how they interact with them\nAnnotate your data with essential metadata such as tags and properties"
  },
  {
    "objectID": "12-project_management.html#checklist-for-successful-project-management",
    "href": "12-project_management.html#checklist-for-successful-project-management",
    "title": "2  Project and File Management",
    "section": "2.2 Checklist for Successful Project Management",
    "text": "2.2 Checklist for Successful Project Management\nBased on past customers and how they interact with projects, I’ve tried to put together a checklist for new customers starting from scratch.\nWe assume that you will have multiple users, and that there is an administrator is in charge of the organization.\n\n\n\nTask\nPerson Responsible\n\n\n\n\nCreation of an Org\nDNAnexus Support\n\n\nSetting up billing for org\nDNAnexus Support\n\n\nAssigning an Admin for the Org\nOrg Admin\n\n\nEnable Smart Reuse in Org\nOrg Admin\n\n\nCreating a base project for incoming data\nOrg Admin\n\n\nUploading Data into base project\nOrg Admin/Uploader/Project Admin\n\n\nInitial Portal Setup\nxVantage Team\n\n\nEnable Single Sign On\nDNAnexus Team\n\n\nCreating User Accounts for the Org\nOrg Admin\n\n\nCreating projects/Copying Data from the base project\nProject Admin\n\n\nSetting Project Permissions (Downloads/Uploads/etc)\nProject Admin\n\n\nAssigning Users to projects with Roles\nProject Admin\n\n\n\n\n\n\n\n\n\nOrg/Project Terminology\n\n\n\nHere is a very useful link in case you get lost in terms of all of the terminology associated with projects and orgs: https://documentation.dnanexus.com/getting-started/key-concepts/organizations#glossary-of-org-terms"
  },
  {
    "objectID": "12-project_management.html#starting-point-the-organization-or-org",
    "href": "12-project_management.html#starting-point-the-organization-or-org",
    "title": "2  Project and File Management",
    "section": "2.3 Starting Point: the Organization (or Org)",
    "text": "2.3 Starting Point: the Organization (or Org)\nBefore anything, you will need to create or have an organization (also known as an org) created for your group. This is usually done through contacting support (support@dnanexus.com).\nWhat is an organization on the platform? It is an entity on the platform that is associated with multiple users. There are a lot of operations at the Org level that make effective data and project management possible.\nCreating an org is important because an organization owns and administers the following:\n\nControl membership within your organization and access levels\nSet up for centralized Billing for your org\nDefault app access level in the organization\nDefault project access level in the organization\nEnable Smart Reuse for Organization\nAdminister the portal for your own organization\nAccess to files within projects administered by the org\nCreating an Audit Trail for the Organization\n\nThe Org Administrator has power over all projects and members in the Org. They are able to:\n\nAdd and remove members to an org and change their access level\nMake themselves members of any project in the Org\nChange the Spending Limit of the Org\nEnable PHI Data Protections\nEnable Smart Reuse"
  },
  {
    "objectID": "12-project_management.html#org-owned-projects",
    "href": "12-project_management.html#org-owned-projects",
    "title": "2  Project and File Management",
    "section": "2.4 Org-owned projects",
    "text": "2.4 Org-owned projects\nIn general, you will want to create projects within your org. This can be done by the org administrators. This simplifies billing for both file storage and compute.\nUsers can also create their own projects, but if they want to tie their project to the org’s billing, they will need to transfer ownership to the organization.\n\n\n\n\n\n\nPHI, Projects, and Orgs\n\n\n\nThere are many cases, especially when protected health information (PHI) is involved, where having a single organization doesn’t make sense.\nControlling such data through its own organization may make much more sense for your group. For very large datasets, we recommend working with your Customer Success representative for implementation strategies.\nAdditionally, project admins can enable PHI Restrictions within a project.\nEnabling an audit trail for the org also becomes very important, in order to be compliant with regulations such as 21 CFR Part 11. In short, an org admin will create a project for which the audit logs will be stored,"
  },
  {
    "objectID": "12-project_management.html#a-base-project-a-data-project-for-all-your-orgs-data",
    "href": "12-project_management.html#a-base-project-a-data-project-for-all-your-orgs-data",
    "title": "2  Project and File Management",
    "section": "2.5 A Base Project: A Data Project for All Your Org’s Data",
    "text": "2.5 A Base Project: A Data Project for All Your Org’s Data\n\n\n\n\n\n\nflowchart LR\n  A(Admin) -- manages--> B[Base Project\\ncontains all\\ndata]\n  B --Dispensed\\nby Admin-->C[Project A]\n  B --Dispensed\\nby Admin-->D[Project B]\n  B --Dispensed\\nby Admin-->E[Project C]\n  F(Uploader) --Uploads Data --> B\n\n\n\n\n\n\n\n\nFigure 2.1: Using a base project to manage your Org’s data. All other projects will be derived from this base project.\n\n\nOne strategy for administering projects within your organization is to have a base project that contains all of your data (Figure 2.1). Tagging incoming data as they are uploaded can simplify project management. Then using these tags, you can copy the relevant data files to your separate projects.\nThe primary advantage of having a base project is that it allows for centralized data management. The org administrator / base project administrator controls access to all files within the org.\nA related advantage of using base projects has to do with file deletion. Once a file is deleted in a project, it is not recoverable, unless they are also in the base project. Thus, having a base project can provide an overall safety net for the underlying files.\nOne final advantage of having a base project is that you can grant upload access to specific users to the base project. This is really helpful for when you have a sequencing group that needs to get raw data into your project."
  },
  {
    "objectID": "12-project_management.html#uploading-batch-files-with-the-upload-agent",
    "href": "12-project_management.html#uploading-batch-files-with-the-upload-agent",
    "title": "2  Project and File Management",
    "section": "2.6 Uploading Batch Files with the Upload Agent",
    "text": "2.6 Uploading Batch Files with the Upload Agent\nNow that we’ve created a base project, we’ll need to get our files into it. In our checklist, this can be done by the org admin, or an org user who has uploader access to the base project.\nThe DNAnexus Upload Agent software can be downloaded, installed, and used for automated batch uploading.\nThe Upload Agent is recommended over dx upload for large sets of files, because it is multi-threaded, and supports resuming in case an upload is interrupted. It will upload 1000 files at a time.\nIn particular, when uploading batches of files, such as everything that is in a folder, we recommend using the --tag or --property options to set metadata, such as tags."
  },
  {
    "objectID": "12-project_management.html#copying-files-from-one-project-to-another",
    "href": "12-project_management.html#copying-files-from-one-project-to-another",
    "title": "2  Project and File Management",
    "section": "2.7 Copying Files from One Project to Another",
    "text": "2.7 Copying Files from One Project to Another\n[Figure Here]\nCopying has very specific definition on the DNAnexus platform: it means copying a file from one project to another project. Copying doesn’t refer to duplicating a file within a project. You may be used to creating aliases or symbolic links within a project.\nWhen we copy a file from one project to another, a new physical copy is not made. The reference to the original file is copied. The data is identical and points to the same location on the platform, and the metadata is copied to the new project.\nThis is quite nice in that you are not doubly charged for storage on the platform within your org. You do have the ability to clone files into a different project - that way a new physical copy of the data is created.\n\n\n\n\n\n\nA Metadata Multiverse\n\n\n\nOnce the metadata is copied into the new project, there is no syncing of the metadata between the two projects.\nUser 1 is free to modify the metadata in Project A and changes are not made to the metadata in Project B. However, the underlying data does not change.\nRemember that the metadata for file objects includes file names, so you can actually change the file name in project B for the file object that you copied. This is not recommended, but it is possible.\nRegardless, the underlying file id for the file object will remain the same.\n\n\n\n\n\n\n\n\nWill I still be charged?\n\n\n\nWhat happens when you have two copies of a file in two different org owned projects? What happens when one of these projects is deleted?\nThere is a process on the platform that is scans file-ids and whether they exist in a project for your org. If a reference to that file-id still exists in your project, then you will still be charged for it.\nThis is also why having a file archiving strategy is important when managing costs on the platform."
  },
  {
    "objectID": "12-project_management.html#full-diagram",
    "href": "12-project_management.html#full-diagram",
    "title": "2  Project and File Management",
    "section": "2.8 Full Diagram",
    "text": "2.8 Full Diagram\n\n\n\n\nflowchart LR\n  A(Admin) -- manages--> B[Base Project\\ncontains all\\ndata]\n  B --Dispensed\\nby Admin-->C[Project A]\n  B --Dispensed\\nby Admin-->D[Project B]\n  B --Dispensed\\nby Admin-->E[Project C]\n  C --viewer-->G(Person 1)\n  D --admin-->G\n  E --contributor-->H(Person 2)"
  },
  {
    "objectID": "12-project_management.html#some-rules-of-thumb-for-data-files-in-a-project",
    "href": "12-project_management.html#some-rules-of-thumb-for-data-files-in-a-project",
    "title": "2  Project and File Management",
    "section": "2.9 Some Rules of Thumb for Data Files in a Project",
    "text": "2.9 Some Rules of Thumb for Data Files in a Project\nGiven this structure, you will want to avoid having too many files within a project.\nA good rule of thumb is to shoot for around 10,000 file objects total in a project. Going larger than this may impact the speed of your file searches.\n\n\n\n\n\n\nDestructive Processes on the DNAnexus platform\n\n\n\nThere are certain operations on the platform that are destructive; that is, there is no ability to undo that operation:\n\nFile / App / Workflow deletion\nProject deletion\n\nMaking your org users aware of these operations is important.\nThere is also the option to disallow deletion within a project to avoid issues like this. This is recommended especially in audit log projects to insure integrity of the audit logs."
  },
  {
    "objectID": "12-project_management.html#project-level-roles",
    "href": "12-project_management.html#project-level-roles",
    "title": "2  Project and File Management",
    "section": "3.1 Project Level Roles",
    "text": "3.1 Project Level Roles\n\n\n\n\n\n\nflowchart LR\n  B[Base Project\\ncontains all data] --Dispensed\\nby Admin-->C[Project A]\n  B --Dispensed\\nby Admin-->D[Project B]\n  B --Dispensed\\nby Admin-->E[Project C]\n  C ---|viewer|G(Person 1)\n  D ---|admin|G\n  E ---|contributor|H(Person 2)\n\n\n\n\n\n\n\n\nFigure 3.1: Possible roles within a project include Admin, Contributor, Uploader, and Viewer.\n\n\nThere are multiple roles that can be assigned to members of a project. In order of access (with each role inheriting privileges of the ones above):\n\n\n\nRole\nDescription\n\n\n\n\nViewer\nCan View Files and Apps within a Project\n\n\nUploader\nCan upload files within a project; limited file management\n\n\nContributor\nCan manage files and run apps/workflows\n\n\nAdmin\nTop level - can manage membership and delete project\n\n\n\nProject Administrators can also modify project-related flags for a project under the Settings Tab in a project. This includes:\n\nEnable PHI restrictions for a project, which supersedes any other flags set below\nEnable / Disallow file operations, including copying, uploading, and downloading\nTransfer Project Billing\nProject Deletion"
  },
  {
    "objectID": "12-project_management.html#a-suggested-project-structure",
    "href": "12-project_management.html#a-suggested-project-structure",
    "title": "2  Project and File Management",
    "section": "3.2 A Suggested Project Structure",
    "text": "3.2 A Suggested Project Structure\nThe DNAnexus platform is an object-based filesystem. That technically means that folders aren’t needed. However, they are extremely helpful in helping you group your work.\nFor example, when I’m starting a project I usually have the following folder structure:\nraw_data/    ## raw files\noutputs/     ## processed Files\napplets/     ## project-specific applets\nworkflows/.  ## project-specific workflows\nnotebooks/.  ## Jupyter Notebooks"
  },
  {
    "objectID": "12-project_management.html#tags-know-them-love-them-use-them",
    "href": "12-project_management.html#tags-know-them-love-them-use-them",
    "title": "2  Project and File Management",
    "section": "3.3 Tags: Know Them, Love Them, Use Them",
    "text": "3.3 Tags: Know Them, Love Them, Use Them\nOne of the ways to become a power user on the DNAnexus platform is to utilize tags when you generate output. This is important for reproducible analysis."
  },
  {
    "objectID": "12-project_management.html#dx-tag",
    "href": "12-project_management.html#dx-tag",
    "title": "2  Project and File Management",
    "section": "3.4 dx tag",
    "text": "3.4 dx tag\nWell, we’ve uploaded our files but forgot to tag them. We can apply tags to files using the dx tag command:\n% dx tag file-FpQKQk00FgkGV3Vb3jJ8xqGV blah\nIf we do a dx describe:\n% dx describe file-FpQKQk00FgkGV3Vb3jJ8xqGV\nResult 1:\nID                  file-FpQKQk00FgkGV3Vb3jJ8xqGV\nTags:               blah"
  },
  {
    "objectID": "12-project_management.html#always-add-a-project-context-to-your-file-ids",
    "href": "12-project_management.html#always-add-a-project-context-to-your-file-ids",
    "title": "2  Project and File Management",
    "section": "3.5 Always Add a Project Context to your File IDs",
    "text": "3.5 Always Add a Project Context to your File IDs\nWe’ve already discovered one way of selecting multiple files on the platform: dx find data (sec-dx-find). This is our main tool for selecting files with metadata.\nWhen we work with files outside of our current project, we might reference it by using a bare file-id. In general, we need to be careful when we use bare file IDs!\nWhy is this so? There is a search tree when we use a file ID that is not in our project and without a project context. The search over metadata is looking for a file object based on just file ID. Usually, when we provide a bare file ID, this search goes like the following:\n\nLook for the file in the current project\nLook at all files across all other projects (Very computationally expensive)\n\nIf you want to use the platform effectively, you want to avoid 2) at all costs, especially when working with a lot of files. The metadata server will take much longer to find your files.\nThe lesson here is when using file-ids from a different project, it is much safer (and faster overall) to put the project-id in front of your file id such as below:\nproject-XXXXX:file-YYYYYYYYY"
  },
  {
    "objectID": "13-spark-on-dnanexus.html#learning-objectives",
    "href": "13-spark-on-dnanexus.html#learning-objectives",
    "title": "3  Spark on the DNAnexus Platform",
    "section": "3.1 Learning Objectives",
    "text": "3.1 Learning Objectives\nBy the end of this chapter, you should be able to:\n\nDescribe the basic use cases for Spark on the DNAnexus platform.\nDescribe the basic components of a Spark Cluster and how data is partitioned and processed by tasks\nExplain the multiple filesystems involved with working with Spark JupyterLab on the DNAnexus Platform and how to transfer files between them\nArticulate when to use a Spark Cluster to query an Apollo Dataset\nMonitor Spark Jobs in the Spark UI on a cluster\nDescribe Lazy evaluation in a Spark cluster and how chains of commands are compiled into execution plans in the Spark cluster.\n\nIn this Chapter, we will discuss some of the intricacies behind working with Spark and on the DNAnexus platform.\nI highly recommend that you go over the Cloud Computing Concepts chapter from Bash for Bioinformatics before you start tackling this in case you need a quick refresher."
  },
  {
    "objectID": "13-spark-on-dnanexus.html#do-i-need-spark",
    "href": "13-spark-on-dnanexus.html#do-i-need-spark",
    "title": "3  Spark on the DNAnexus Platform",
    "section": "3.2 Do I need Spark?",
    "text": "3.2 Do I need Spark?\nAs we’ll discover, Spark is made for working with very large datasets. The datasets themselves might be very long (have billions of rows), or very wide (millions of columns).\nVery large data that is used for Phenotyping, such as Electronic Health Record (EHR) or other Real World Data (RWD) can definitely benefit from working with Spark. When this data is ingested into Spark, it is available as an Apollo Dataset and can be used in large scale queries or visualization tools such as Cohort Browser.\nGenomic Data is another good use-case. Whole Genome Sequencing outputs usually have hundreds of millions of variants, and with datasets such as UK Biobank, this is multiplied by a very large number of participants. We’ll see in the next chapter that a framework exists for genomic analysis built on Spark called Hail.\n\n\n\n\n\n\nSpark/Hail on The Core Platform\n\n\n\nPlease note that working with Spark on the DNAnexus platform requires an Apollo License if you are on the Core Platform. For more information, please contact DNAnexus sales.\nIf you are on UKB RAP, the Pheno Data is stored as a Spark Database/Dataset, so you do not have to have an Apollo license to access it. On RAP, you also have access to the Hail configurations of Spark JupyterLab as well."
  },
  {
    "objectID": "13-spark-on-dnanexus.html#spark-concepts",
    "href": "13-spark-on-dnanexus.html#spark-concepts",
    "title": "3  Spark on the DNAnexus Platform",
    "section": "3.3 Spark Concepts",
    "text": "3.3 Spark Concepts\nIn this section, we will discuss the basic concepts that you need to know before using Spark successfully on the platform.\nOn the platform, Spark is used to work with ingested Pheno Data either indirectly using the dx extract_dataset functionality, or by working directly with the Spark Database using Libraries such as PySpark (for Python) or sparklyr (for R), or using Spark SQL to query the contents of a database.\n\n\n\n\n\n\nPar-what-now?\n\n\n\nYou may have heard of Parquet files and wondered how they relate to database objects on the platform. They are a way to store data in a format called columnar storage.\nIt turns out it is faster for retrieval and searching to store data not by rows, but by columns. This is really helpful because the data is sorted by data type and it’s easier to traverse for this reason.\nApache Spark is the scalable solution to using columnar formats such as Parquet. There are a number of database engines that are fast at searching and traversing these types of files. Some examples are Snowflake and Apache Arrow.\nOn the DNAnexus platform, certain data objects (called Dataset Objects and Database objects) are actually stored in Parquet format, for rapid searching and querying using Apache Spark.\n\n\n\n3.3.1 What is Spark?\nApache Spark is a framework for working with large datasets that won’t fit into memory on a single computer. Instead, we use what’s called a Spark Cluster, which is a cluster of machines that we request when we start Spark JupyterLab. Specifically, we request the Spark Cluster as part of a Spark JupyterLab configuration.\n\n\n\n\n\nSpark Architecture.\n\n\nFigure 3.1: ?(caption)\n\n\nThe main pieces of a Spark Cluster are the Driver Node, a Cluster manager (which manages resources on the Worker Nodes), and the Worker Nodes. Individual worker nodes have processes known as Executors that will run on their portion of the data (Figure 3.1).\nSpark lets us analyse large datasets by partitioning the data into smaller pieces that are made available to the cluster via the Hadoop File System. Each executor gains access to a set of these data partitions, and they execute tasks assigned to by the Cluster Manager on their set of partitions.\nThe Driver Node directs the individual executors by assigning tasks. These individual tasks are part of an overall execution plan.\nThis link is a good introduction to Spark Terminology."
  },
  {
    "objectID": "13-spark-on-dnanexus.html#the-three-or-four-filesystems",
    "href": "13-spark-on-dnanexus.html#the-three-or-four-filesystems",
    "title": "3  Spark on the DNAnexus Platform",
    "section": "3.4 The Three (or Four) Filesystems",
    "text": "3.4 The Three (or Four) Filesystems\n\n\n\n\n\nflowchart TD\n    A[Project Storage] -->|dx download|B[Driver Node]\n    A -->|dxFUSE|C \n    B -->|hdfs dfs -put|C[HDFS\\nSpark Cluster]\n    B -->|dx upload|A\n    C -->|hdfs dfs -get|B\n    D[DNAX\\nS3 Storage] -->|dnax://|C\n    C -->|dnax://|D\n    C -->|dxFUSE -limitedWrite|A\n\n\n\n\n\nFigure 3.2: The four filesystems.\n\n\n\n\nUsing Spark on DNAnexus effectively requires us to be familiar with at least 3 different file systems (Figure 3.2):\n\ndxFUSE file system - used to directly access project storage. We access this using file:///mnt/project/ in our scripts. For example, if we need to access a file named chr1.tar.gz, we refer to it as file:///mnt/project/chr1.tar.gz. Mounted at the start of JupyterLab; to access\nHadoop File System (HDFS) - Distributed File System for the Spark Cluster. We access this using the hdfs:// protocol. This is the filesystem that Spark uses by default. When we load Spark DataFrames, this is where the Parquet files belonging to them live.\nDNAX S3 Bucket - contains ingested data and other data loaded and saved to a database in Spark. This is a separate S3 bucket. We interact with ingested Pheno and Geno Data using a special protocol called dnax://. Ingested data that is stored here is accessed through connecting to the DNAX database that holds it. Part of why Cohort Browser works with very large data is that the ingested data is stored in this bucket.\nDriver Node File System - one waypoint for getting things into and out of the Hadoop File System and to/from Project Storage. Since we run JupyterLab on the Driver Node, we use hdfs dfs -put/hdfs dfs -get to transfer to/from the Driver Node to the Hadoop File system (see below), and use dx download to work with Project Storage files, and dx upload to transfer files back into project storage.\n\nWhenever possible, we want to avoid using the Driver node to transfer files to any of the other filesystems. That is because we are limited to the disk space and memory of the Driver node, and it is easy to run out of memory and disk space when using it to transfer files. To overcome this, there are direct connections between HDFS and the Project Storage using dxFUSE that will let us stream the data.\nOverall advice when working in a Spark Cluster:\n\nTry to avoid transfers through the Driver node (hdfs dfs -get/dx upload), as you are limited to the memory/disk space of the Driver node. You can specify a file path directly from project storage by putting a file:///mnt/project/ before the file path (Figure 3.3).\n\n\n\n\n\n\nflowchart TD\n    subgraph F[From Project Storage]\n        A[Project Storage] -->|dxFUSE|C[HDFS\\nSpark Cluster]\n    end\n    C --> D[DNAX\\nS3 Storage]\n\n\n\n\n\nFigure 3.3: From Project Storage to HDFS\n\n\n\n\n\nWhen possible, use dxFUSE with glob wildcards (such as *.tar.gz or chr*) to directly stream files into HDFS (Spark Cluster Storage) from project storage by using file:///mnt/project/ URLs (Figure 3.4). For example, we could load a set of pVCF files as a Hail MatrixTable with hl.read_vcf(\"file:///mnt/project/data/pVCF/*.vcf.gz\"))\n\n\n\n\n\n\nflowchart TD\n    subgraph F[From Project Storage]\n        A[Project Storage] -->|dxFUSE|C[HDFS\\nSpark Cluster]\n    end\n    C --> D[DNAX\\nS3 Storage]\n\n\n\n\n\nFigure 3.4: Load from Project Storage directly into HDFS.\n\n\n\n\n\nIf you need to get files out of HDFS into Project Storage, use dxFUSE in -limitedWrite mode to write into project storage by passing in file:///mnt/project/ URLs to the .write() methods in Spark/Hail. You will need to remount dxFUSE in the JupyterLab container to do this (Figure 3.5).\n\n\n\n\n\n\nflowchart TD\n    subgraph To Project Storage\n      A[Project Storage]\n      C[HDFS\\nSpark Cluster] -->|dxFUSE -limitedWrite|A\n    end\n\n\n\n\n\nFigure 3.5: From HDFS to Project Storage.\n\n\n\n\n\nIf you need to persist your Spark DataFrame, use dnax:///<db_name>/<table_name> with .write() to write it into DNAX storage (for example, mt.write(\"dnax:///my_db/geno.mt\")) (Figure 3.6). When you need to retrieve a stored table in DNAX, use the same dnax:/// URL to load it from DNAX (for example, hl.read_matrix_table(\"dnax:///my_db/geno.mt\").\n\n\n\n\n\n\nflowchart TD\n    C[HDFS\\nSpark Cluster] -->|dnax://my_db/geno.mt|D\n    D[DNAX\\nS3 Storage] -->|dnax://my_db/geno.mt|C\n\n\n\n\n\nFigure 3.6: Saving and Loading into DNAX.\n\n\n\n\n\nMore advanced usage - you can connect Spark to other S3 buckets using the s3:// protocol. You may need to authenticate before connecting.\n\n\n\n\n\n\n\nAccessing Spark Databases on DNAnexus\n\n\n\nOne issue I see a lot of people get tripped up with is with Database Access. One thing to know is that Spark databases cannot be moved from project to project. Once the data is ingested into a database object, it can’t be copied to a new project.\nFor a user to access the data in a Spark Database, they must have at least VIEW level access to the project that contains it on the platform. This includes anyone who is utilizing your database/dataset in Cohort Browser.\nNote that Apollo Datasets (not Databases) can be copied to new projects. However, to actually access the data in Cohort Browser, the project that contains the database needs to be viewable by the user.\n\n\n\n\n\n\n\n\nSpark Cluster JupyterLab vs. Spark Apps\n\n\n\nOur main way of using Spark on DNAnexus is using the Spark JupyterLab app. This app will let you specify the instance types used in the Spark Cluster, the number of nodes in your Spark cluster, as well as additional features such as Hail or Variant Effect Predictor. This should be your main way of working with Spark DataFrames that have been ingested into Apollo.\nYou can also build Spark Applets that request a Spark Cluster. This is a more complicated venture, as it may require installing software on the worker nodes using a bootstrapping script, distributing data using hdfs dfs -put, and submitting Spark Jobs using dx-spark-submit."
  },
  {
    "objectID": "13-spark-on-dnanexus.html#balancing-cores-and-memory",
    "href": "13-spark-on-dnanexus.html#balancing-cores-and-memory",
    "title": "3  Spark on the DNAnexus Platform",
    "section": "3.5 Balancing Cores and Memory",
    "text": "3.5 Balancing Cores and Memory\nCentral to both Spark and Hail is tuning the number of cores and memory in each of our instances. This also needs to be balanced with the partition size for our data. Data can be repartitioned, although this may be an expensive operation.\nIn general, the number of data partitions should be greater than the number of cores. Why? If there are less partitions than cores, then the remaining cores will be unused.\nThere are a lot of other tuning considerations to keep in mind."
  },
  {
    "objectID": "13-spark-on-dnanexus.html#the-spark-ui",
    "href": "13-spark-on-dnanexus.html#the-spark-ui",
    "title": "3  Spark on the DNAnexus Platform",
    "section": "3.6 The Spark UI",
    "text": "3.6 The Spark UI\nWhen you start up Spark JupyterLab, you will see that it will open at a particular url which has the format https://JOB-ID.dnanexus.com. To open the Spark UI, use that same URL, but with a :8081 at the end of it. For example, the UI url would be https://JOB-ID.dnanexus.com:8081.\nNote that the Spark UI is not launched until you connect to Spark or Hail.\nThe Spark UI is really helpful in understanding how Spark translates a series of operations into a Plan, and how it executes that plan."
  },
  {
    "objectID": "13-spark-on-dnanexus.html#spark-and-lazy-evaluation",
    "href": "13-spark-on-dnanexus.html#spark-and-lazy-evaluation",
    "title": "3  Spark on the DNAnexus Platform",
    "section": "3.7 Spark and Lazy Evaluation",
    "text": "3.7 Spark and Lazy Evaluation\nOne thing that is extremely important to know is that Spark executes operations in a Lazy manner. This is opposed to a language like R or Python which executes commands right away (a greedy manner).\nThere are a number of commands that make Spark immediately start calculating a result, including:\n\n.count()\n.show()\n.write()\n.collect()\n\nWhen we run Spark operations, we are usually building chains of operations. For example, we might want to .filter() on a column, .select() columns of data, or aggregate (summarize) columns.\nSpark builds up these chains of commands until it sees it needs to execute an operation. For example:\ndf.filter(df(\"sex_code\")==2).select().show(5)"
  },
  {
    "objectID": "13-spark-on-dnanexus.html#extracting-phenodata-with-dx-extract_dataset",
    "href": "13-spark-on-dnanexus.html#extracting-phenodata-with-dx-extract_dataset",
    "title": "3  Spark on the DNAnexus Platform",
    "section": "3.8 Extracting PhenoData with dx extract_dataset",
    "text": "3.8 Extracting PhenoData with dx extract_dataset\nFirst of all, you may not need to use Spark JupyterLab if you just want to extract Pheno Data from a Dataset and you know the fields.\nThere is a utility within dx-toolkit (the CLI tools for working with the DNAnexus platform) called dx extract_dataset that will retrieve pheno data in the form of a CSV. It uses Spark, but the Spark is executed by the Thrift Server (the same Spark Cluster that also serves the data for Cohort Browser).\nYou will need a list of entity/field ids to retrieve them. For more info, please refer to the R Notebook (Chapter 4) and the Python Notebook ().\nHowever, there are good reasons to work with the Spark Database directly. Number one is that your Cohort query will not run on the Thrift Server. If your query is complex and takes two minutes to execute, the Thrift Server will timeout. In that case, you will need to run your own Spark JupyterLab cluster and extract the data using pyspark (for Python users), or sparklyr (for R users) itself."
  },
  {
    "objectID": "13-spark-on-dnanexus.html#connecting-with-pyspark-for-python-users",
    "href": "13-spark-on-dnanexus.html#connecting-with-pyspark-for-python-users",
    "title": "3  Spark on the DNAnexus Platform",
    "section": "3.9 Connecting with PySpark (For Python Users)",
    "text": "3.9 Connecting with PySpark (For Python Users)\nThe first thing we’ll do to connect to the Spark database is connect to it by starting a Spark Session. Make sure you only do this once. If you try to connect twice, Spark will throw an error.\nIf this happens, make sure to restart your notebook kernel.\n```{python}\nimport pyspark\nsc = pyspark.SparkContext()\nspark = pyspark.sql.SparkSession(sc)\n```\n\n3.9.1 Running SparkSQL Queries in PySpark\nThe basic template for running Spark Queries is this:\n```{python}\nretrieve_sql = 'select .... from .... '\ndf = spark.sql(retrieve_sql)\n```\n\n\n3.9.2 Koalas is the Pandas version of Spark\nIf you are familiar with Pandas, the Koalas module (from Databricks) provides a Pandas-like interface to SparkSQL queries. This way, you don’t have to execute native Spark commands or SparkSQL queries.\nOnce you have a Spark DataFrame, you can convert it to a Koalas one by using the .to_koalas() method.\n```{python}\ndf_koalas = df.to_koalas()\n```"
  },
  {
    "objectID": "13-spark-on-dnanexus.html#connecting-with-sparklyr-for-r-users",
    "href": "13-spark-on-dnanexus.html#connecting-with-sparklyr-for-r-users",
    "title": "3  Spark on the DNAnexus Platform",
    "section": "3.10 Connecting with sparklyr (For R Users)",
    "text": "3.10 Connecting with sparklyr (For R Users)\nYou’ll need to install the package sparklyr along with its dependencies to work with the Spark DataFrames directly with R.\n```{r}\nlibrary(DBI)\nlibrary(sparklyr)\nport <- Sys.getenv(\"SPARK_MASTER_PORT\")\nmaster <- paste(\"spark://master:\", port, sep = '')\nsc = spark_connect(master)\n```\n```{r}\nretrieve_sql <- 'select .... from .... '\ndf = dbGetQuery(sc, retrieve_sql)\n```"
  },
  {
    "objectID": "dx_extract_dataset_R.html#preparing-your-environment",
    "href": "dx_extract_dataset_R.html#preparing-your-environment",
    "title": "4  dx extract_dataset in R",
    "section": "4.1 Preparing your environment",
    "text": "4.1 Preparing your environment\n\n4.1.1 Launch spec:\n\nApp name: JupyterLab with Python, R, Stata, ML ()\nKernel: R\nInstance type: Spark Cluster - mem1_ssd1_v2_x2, 4 nodes\nSnapshot: /.Notebook_snapshots/jupyter_snapshot.gz\nCost: < $0.2\nRuntime: =~ 10 min\nData description: Input for this notebook is a v3.0 Dataset or Cohort object ID\n\n\n\n4.1.2 Install dxpy\nextract_dataset requires dxpy version >= 0.329.0. If running the command from your local environment (i.e. off of the DNAnexus platform), it may be required to also install pandas. For example, pip3 install -U dxpy[pandas]\n\nsystem(\"pip3 show dxpy\", intern = TRUE)\n\n\n\n4.1.3 Install tidyverse for data processing\nQuick note - you will need to read the licenses for the tidyverse in order to make sure whether you and your group are comfortable with the licensing terms.\nIf you loaded the snapshot in this project, all of these packages and dependencies are also installed, so you don’t need to install them again.\n\ninstall.packages(c(\"readr\", \"stringr\", \"dplyr\", \"glue\", \"reactable\", \"janitor\", \"remotes\"))\nremotes::install_github(\"laderast/xvhelper\")\n\nInstalling packages into ‘/usr/local/lib/R/site-library’\n(as ‘lib’ is unspecified)\n\nalso installing the dependencies ‘bit’, ‘bit64’, ‘progress’, ‘hms’, ‘vroom’, ‘tzdb’, ‘vctrs’, ‘reactR’, ‘snakecase’\n\n\nDownloading GitHub repo laderast/xvhelper@HEAD\n\n\n\nRcpp       (1.0.9 -> 1.0.10) [CRAN]\nutf8       (1.2.2 -> 1.2.3 ) [CRAN]\nfansi      (1.0.3 -> 1.0.4 ) [CRAN]\nstringi    (1.7.8 -> 1.7.12) [CRAN]\ntibble     (3.1.8 -> 3.2.0 ) [CRAN]\npurrr      (0.3.5 -> 1.0.1 ) [CRAN]\ncli        (3.4.1 -> 3.6.0 ) [CRAN]\ntimechange (0.1.1 -> 0.2.0 ) [CRAN]\npng        (NA    -> 0.1-8 ) [CRAN]\nhere       (NA    -> 1.0.1 ) [CRAN]\nRcppTOML   (NA    -> 0.2.2 ) [CRAN]\ntidyr      (1.2.1 -> 1.3.0 ) [CRAN]\nlubridate  (1.9.0 -> 1.9.2 ) [CRAN]\nreticulate (NA    -> 1.28  ) [CRAN]\n\n\nInstalling 14 packages: Rcpp, utf8, fansi, stringi, tibble, purrr, cli, timechange, png, here, RcppTOML, tidyr, lubridate, reticulate\n\nInstalling packages into ‘/usr/local/lib/R/site-library’\n(as ‘lib’ is unspecified)\n\n\n\n── R CMD build ─────────────────────────────────────────────────────────────────\n* checking for file ‘/tmp/RtmpJskvRy/remotes734410c4e0/laderast-xvhelper-6e0873a/DESCRIPTION’ ... OK\n* preparing ‘xvhelper’:\n* checking DESCRIPTION meta-information ... OK\n* checking for LF line-endings in source and make files and shell scripts\n* checking for empty or unneeded directories\n* building ‘xvhelper_0.0.100.tar.gz’\n\n\n\nInstalling package into ‘/usr/local/lib/R/site-library’\n(as ‘lib’ is unspecified)\n\n\n\n\n\n4.1.4 Import packages\n\nlibrary(dplyr)\nlibrary(readr)\nlibrary(stringr)\nlibrary(glue)\nlibrary(reactable)\nlibrary(xvhelper)\n\n\n\n4.1.5 1. Assign environment variables\n\n# The referenced Dataset is private and provided only to demonstrate an example input. The user will need to supply a permissible and valid record-id\n# Assign project-id of dataset\n# Assign dataset record-id\nrid <- 'record-G5Ky4Gj08KQYQ4P810fJ8qPp'\n# Assign joint dataset project-id:record-id\ndataset <- rid\n\n\n\n4.1.6 2. Call “dx extract_dataset” using a supplied dataset\nWe’ll use the {glue} package to put our bash commands together for dx extract_dataset, and use system() to execute our bash code.\nglue::glue() has the advantage of not needing to paste() together strings. The string substitution is cleaner.\n\ncmd <- glue::glue(\"dx extract_dataset {dataset} -ddd\")\n\ncmd\n\nLet’s execute our command using system() and then we will list the files that result using list.files(). We generate three files in the directory in JupyterLab storage:\n\ndataset_name.codings.csv\ndataset_name.data_dictionary.csv\ndataset_name.entity_dictionary.csv\n\n\nsystem(cmd)\nlist.files()\n\n\n4.1.6.1 Preview data in the three dictionary (*.csv) files\n\n#codings_file <- system(\"ls *.codings.csv\", intern = TRUE)\ncodings_file <- list.files(pattern=\"*.codings.csv\")\ncodings_df <- read_csv(codings_file, show_col_types = FALSE)\nhead(codings_df)\n\n\nentity_dict_file <- system(\"ls *.entity_dictionary.csv\", intern=TRUE)\nentity_dict_df <- read_csv(entity_dict_file, show_col_types = FALSE)\nhead(entity_dict_df)\n\n\n\n\n4.1.7 Understanding the Data Dictionary File\nThe data dictionary is the glue for the entire dataset. It maps:\n\nEntity to Fields\nFields to Codings\nEntity to Entity\n\nWe’ll use the data dictionary to understand how to building our list of fields, and later, we’ll join it to the codings file to build a list of fields and their coded values.\nThere are more columns to the data dictionary, but let’s first see the entity, name, title, and type columns:\n\n#data_dict_file <- system(\"ls *.data_dictionary.csv\", intern=TRUE)\ndata_dict_file <- list.files(pattern=\"*.data_dictionary.csv\")\ndata_dict_df <- read_csv(data_dict_file, show_col_types = FALSE)\ndata_dict_df <- data_dict_df \n\ndata_dict_df %>%\n        select(entity, name, title, type) %>%\n        head()\n\n\n\n4.1.8 3. Parse returned metadata and extract entity/field names\nLet’s search for some fields. We want the following fields:\n\nCoffee intake | instance 0\nSex (Gender)\nSmoked cigarette or pipe within last hour | Instance 0\n\nWe can use the {reactable} package to make a searchable table of the data dictionary. This will help in finding fields.\nNote the search box in the top right of the table - when we have many fields, we can use the search box to find fields of interest. Try searching for Coffee intake and see what fields pop up.\n\ndata_dict_df <- data_dict_df %>%\n    relocate(name, title) %>%\n    mutate(ent_field = glue::glue(\"{entity}.{name}\"))\n\nbasic_data_dict <- data_dict_df |>\n                    select(title, name, entity, ent_field, coding_name, is_multi_select, is_sparse_coding)\n\nreactable::reactable(basic_data_dict, searchable = TRUE)\n\nAnother strategy for searching fields: we can use grepl within dplyr::filter() to search for fields that match our criteria.\nNote we’re chaining the grepl statements with an OR |.\nWe’re also concatenating entity and name to a new variable, ent_field, which we’ll use when we specify our list of fields.\n\nfiltered_dict <- data_dict_df %>%\n    filter(grepl(\"Coffee type\", title) | \n           grepl(\"Sex\", title) | \n           grepl(\"Smoked\", title) | \n           grepl(\"Age at recruitment\", title) |\n           grepl(\"main ICD10\", title) |\n           grepl(\"Types of transport\", title)\n          ) %>%\n    arrange(title) \n\nfiltered_dict %>%\n    select(name, title, ent_field)\n\nLet’s use this subset of fields - we’ll pull the ent_field column, and paste it together into a single comma delimited string using paste:\n\nfield_list <- filtered_dict %>%\n    pull(ent_field)\n\n#field_list <- field_list[200:210]\nfield_list <- paste(field_list, collapse = \",\")\nfield_list\n\n\n\n4.1.9 4. Use extracted entity and field names as input to the called function, “dx extract_dataset” and extract data\nAgain, we’ll use glue() here for cleaner string substitution.\nWe’ll extract the cohort information to a file called cohort_data.csv and work with this file for the rest of the notebook.\n\ncohort <- \"record-G5Ky4Gj08KQYQ4P810fJ8qPp\"\ncohort_template <- \"dx extract_dataset {cohort} --fields {field_list} -o cohort_data.csv\"\ncmd <- glue::glue(cohort_template)\n\ncmd\n\nsystem(cmd)\n\n\n4.1.9.1 Preview data in the retrieved data file\nWe’ll see that the retrieved data contains the integer and character codes. These must be decoded (see below):\n\ndata_df <- read_csv(\"cohort_data.csv\", show_col_types = FALSE)\nhead(data_df)"
  },
  {
    "objectID": "dx_extract_dataset_R.html#decoding-columns-with-xvhelper",
    "href": "dx_extract_dataset_R.html#decoding-columns-with-xvhelper",
    "title": "4  dx extract_dataset in R",
    "section": "4.2 Decoding columns with xvhelper",
    "text": "4.2 Decoding columns with xvhelper\nxvhelper is a little R package that will return the actual values of the returned data.\nTo use it, you build a coded_col_df using merge_coding_data_dict() and then translate the categorical columns to values using decode_categories(), and then change the column names to R friendly clean ones using decode_column_names().\nNote that we need to run decode_df() before we run decode_category()\n\n#install via remotes::install_github()\n#install.packages(\"remotes\")\nremotes::install_github(\"laderast/xvhelper\")\n\nlibrary(xvhelper)\n\n\ncoded_col_df <- xvhelper::merge_coding_data_dict(coding_dict = codings_df, data_dict = data_dict_df)\n\ndecoded <- data_df %>%\n    xvhelper::decode_single(coded_col_df) |>\n    xvhelper::decode_multi_purrr(coded_col_df) |>\n    xvhelper::decode_column_names(coded_col_df, r_clean_names = FALSE)\n    \nhead(decoded)\n\n\nwrite.csv(decoded, file=\"cohort_decoded.csv\")\n\n\n4.2.1 Save Output to Project\n\nsystem(\"dx upload *.csv --destination /users/tladeras/\")"
  },
  {
    "objectID": "01-Hail-Skills.html#learning-objectives",
    "href": "01-Hail-Skills.html#learning-objectives",
    "title": "5  Hail Skills and Concepts",
    "section": "5.1 Learning Objectives",
    "text": "5.1 Learning Objectives\nAfter reading this chapter, you should be able to:\n\nExplain basic Hail terms and concepts, including Hail Tables and Hail MatrixTables\nMonitor progress using the Spark UI\nExecute Hail operations on Tables and MatrixTables, including annotating, filtering, and joining\nCalculate Sample QC metrics and save as a Hail Table in DNAX\n\n\n\n\n\n\n\nThis chapter shows how to perform sample QC on a Hail MatrixTable as a pre-GWAS step and save it as a Table to an Apollo database (dnax://) on the DNAnexus platform. See documentation for guidance on launch specs for the JupyterLab with Spark Cluster app for different data sizes: https://documentation.dnanexus.com/science/using-hail-to-analyze-genomic-data\nNote: For population scale data, samples may be referred to as individuals. In this chapter, the word “sample” will be used."
  },
  {
    "objectID": "01-Hail-Skills.html#what-is-hail",
    "href": "01-Hail-Skills.html#what-is-hail",
    "title": "5  Hail Skills and Concepts",
    "section": "5.2 What is Hail?",
    "text": "5.2 What is Hail?\nHail is a genomics framework built on top of Spark to allow for scalable genomics queries. It uses many of the same concepts (partitions, executors, plans) but it is genomics focused.\nFor example, the Hail MatrixFrame can be QC’ed, and then filtered on these QC metrics.\n\n5.2.1 Vocabulary\nDNAnexus Specific:\n\nDNAX - S3 bucket for storing Apollo Databases and Tables - accessed with the dnax:// protocol\nDatabase Object - data object on platform that represents a parquet database - has a unique identifier.\n\nHail Specific Terminology:\n\nSpark Driver - the “boss” of the Spark cluster - hosts the Spark UI\nSpark Worker - individual nodes that house executors\nHail Executor - individual cores/CPUs within a worker - Executors are assigned tasks, identical to Spark.\nData Partition - portion of the data that is accessed by worker in parquet columnar format\nKey - unique identifier for rows or columns\n\nrow keys: rsid + alleles\ncolumn keys: sample IDs\n\n\nIn the rest of this chapter, we’ll discuss some of the operations that can be done on Hail Tables and MatrixTables, the two fundamental data structures of Hail."
  },
  {
    "objectID": "01-Hail-Skills.html#initialize-spark-and-hail",
    "href": "01-Hail-Skills.html#initialize-spark-and-hail",
    "title": "5  Hail Skills and Concepts",
    "section": "5.3 Initialize Spark and Hail",
    "text": "5.3 Initialize Spark and Hail\nMake sure to open up the Spark Interface at https://job-url:8081/jobs/\n\n# Running this cell will output a red-colored message- this is expected.\n# The 'Welcome to Hail' message in the output will indicate that Hail is ready to use in the notebook.\n\nfrom pyspark.sql import SparkSession\nimport hail as hl\n\nbuilder = (\n    SparkSession\n    .builder\n    .enableHiveSupport()\n)\nspark = builder.getOrCreate()\nhl.init(sc=spark.sparkContext)\n\npip-installed Hail requires additional configuration options in Spark referring\n  to the path to the Hail Python module directory HAIL_DIR,\n  e.g. /path/to/python/site-packages/hail:\n    spark.jars=HAIL_DIR/hail-all-spark.jar\n    spark.driver.extraClassPath=HAIL_DIR/hail-all-spark.jar\n    spark.executor.extraClassPath=./hail-all-spark.jarRunning on Apache Spark version 2.4.4\nSparkUI available at http://ip-172-31-34-162.ec2.internal:8081\nWelcome to\n     __  __     <>__\n    / /_/ /__  __/ /\n   / __  / _ `/ / /\n  /_/ /_/\\_,_/_/_/   version 0.2.78-b17627756568\nLOGGING: writing to /opt/notebooks/hail-20230418-1956-0.2.78-b17627756568.log\n\n\nNow we are ready to connect and do work with Hail.\nNote that you should only initialize one time in a notebook. If you try to initialize multiple times, then you will get an error.\nIf that happens, restart the notebook kernel (Notebook > Restart Kernel in the menu) and start executing all over again."
  },
  {
    "objectID": "01-Hail-Skills.html#important-data-structures-in-hail-tables-and-matrixtables",
    "href": "01-Hail-Skills.html#important-data-structures-in-hail-tables-and-matrixtables",
    "title": "5  Hail Skills and Concepts",
    "section": "5.4 Important Data Structures in Hail: Tables and MatrixTables",
    "text": "5.4 Important Data Structures in Hail: Tables and MatrixTables\nBefore we get started with doing a GWAS in Hail, let’s talk about the data structures we’ll work with. Hail Tables and Hail MatrixTables.\nOne of the hardest things to understand about Hail data structures is that they are actually linked data structures. It’s easiest to think of them as data tables with attached metadata."
  },
  {
    "objectID": "01-Hail-Skills.html#hail-table",
    "href": "01-Hail-Skills.html#hail-table",
    "title": "5  Hail Skills and Concepts",
    "section": "5.5 Hail Table",
    "text": "5.5 Hail Table\nThe fundamental data structure in Hail is the Hail Table.\nI think of Hail Tables as a regular data table with a set of metadata fields. These metadata fields are known as global fields. You can think of them as annotations for the entire table, such as the instrument used to generate the table, the software version used to process the data, etc.\nThe tabular data itself are called row fields."
  },
  {
    "objectID": "01-Hail-Skills.html#load-in-a-table-from-project-storage",
    "href": "01-Hail-Skills.html#load-in-a-table-from-project-storage",
    "title": "5  Hail Skills and Concepts",
    "section": "5.6 Load in a Table from Project Storage",
    "text": "5.6 Load in a Table from Project Storage\nThe first thing we’ll do in hail is load a pheno file from project storage.\nWe’ll use hl.import_table() for this. We have to specify our key for the file. In our table, we have both fid (family ID) and iid (individual ID). We will use iid (individual ID) as our key.\n\n# Import the pheno CSV file as a Hail Table\n\npheno_table = hl.import_table(\"file:///mnt/project/data/ukbb_100k_bmi_casecontrol.csv\",\n                              delimiter=',',\n                              impute=True,\n                              key='iid') # specify the column that will be the key (values must match what is in the MT 's' column)\n\n2023-04-18 19:57:56 Hail: INFO: Reading table to impute column types\n2023-04-18 19:58:01 Hail: INFO: Finished type imputation\n  Loading field 'fid' as type str (imputed)\n  Loading field 'iid' as type str (imputed)\n  Loading field 'father_iid' as type int32 (imputed)\n  Loading field 'mother_iid' as type int32 (imputed)\n  Loading field 'sex_code' as type int32 (imputed)\n  Loading field 'case_control_status' as type int32 (imputed)\n\n\n\n5.6.1 Quick Hail Table EDA\nWe want to confirm that we loaded the data correctly. There are four really helpful methods to help with this:\n\n.describe() - gives the fields and their data types for our Hail Table\n.count() - counts the number of rows in our Hail Table\n.show() - shows the first few rows of our dataset.\n.summarize() - gives us information about missing values and data types.\n\nLet’s start with .describe() on our loaded Pheno Table. This method will show the different fields associated with our Hail Table. Note that the actual table is referred to as Row fields, and you can see the names of the individual columns in the dataset.\n\npheno_table.describe()\n\n----------------------------------------\nGlobal fields:\n    None\n----------------------------------------\nRow fields:\n    'fid': str \n    'iid': str \n    'father_iid': int32 \n    'mother_iid': int32 \n    'sex_code': int32 \n    'case_control_status': int32 \n----------------------------------------\nKey: ['iid']\n----------------------------------------\n\n\nIf we do a count, we can see that it counts the number of rows in our table. This can be really helpful in confirming the data was loaded correctly.\nKeep in mind that with larger tables, the .count() operation can take some time to execute. That’s because it is a command that needs to be done on all of the partitions of the dataset.\n\npheno_table.count()\n\n19884\n\n\n.show() will show us the first few rows of our Hail Table. Similar to Spark, if we pass an integer to .show() (like .show(10)) it will show that number of rows instead.\n\npheno_table.show()\n\n\nfidiidfather_iidmother_iidsex_codecase_control_statusstrstrint32int32int32int32\n\"sample_1_10\"\"sample_1_10\"0012\n\"sample_1_10003\"\"sample_1_10003\"0022\n\"sample_1_10014\"\"sample_1_10014\"0021\n\"sample_1_10020\"\"sample_1_10020\"0012\n\"sample_1_10021\"\"sample_1_10021\"0021\n\"sample_1_10023\"\"sample_1_10023\"0011\n\"sample_1_10036\"\"sample_1_10036\"0021\n\"sample_1_10037\"\"sample_1_10037\"0011\n\"sample_1_10047\"\"sample_1_10047\"0011\n\"sample_1_10056\"\"sample_1_10056\"0012\nshowing top 10 rows\n\n\nCalling .summarize() on a Hail Table will give you an overview of missing values and data types for each of your columns in your Hail Table. This is always extremely helpful in order to confirm the data was loaded and parsed correctly.\n\npheno_table.summarize()\n\n\n\n5.6.2 Diving into individual row fields\nFor our pheno_table, we can also show a column by using the . accessor with .show(). By default, it will show the column along wi†h the row key for the dataset (iid)\n\npheno_table.case_control_status.show()\n\n\niidcase_control_statusstrint32\n\"sample_1_10\"2\n\"sample_1_10003\"2\n\"sample_1_10014\"1\n\"sample_1_10020\"2\n\"sample_1_10021\"1\n\"sample_1_10023\"1\n\"sample_1_10036\"1\n\"sample_1_10037\"1\n\"sample_1_10047\"1\n\"sample_1_10056\"2\nshowing top 10 rows\n\n\nAn additional method we can use on numeric columns is .summarize(), which will give a numeric summary of the column. Since we don’t have a numeric column, we’ll do it on the sex_code column:\n\npheno_table.sex_code.summarize()\n\n\n19884 records.sex_code (int32):Non-missing19884 (100.00%)Missing0Minimum1Maximum2Mean1.54Std Dev0.50"
  },
  {
    "objectID": "01-Hail-Skills.html#transforming-hail-tables",
    "href": "01-Hail-Skills.html#transforming-hail-tables",
    "title": "5  Hail Skills and Concepts",
    "section": "5.7 Transforming Hail Tables",
    "text": "5.7 Transforming Hail Tables\nThere are multiple operations we can do on a Hail Table (let’s call our Hail Table as ht here):\n\nht.filter()\nht.annotate()\n\nLet’s investigate these.\n\n5.7.1 Specifying new columns using .annotate()\nWe can define new columns in our Hail Table using the .annotate() method. This is a lot like the mutate() function in R/Tidyverse. We can compute new values based on other columns in our dataset.\nHere we’re subtracting 1 from both the sex_code and case_control_status columns so that we can use them as a phenotype and a covariate in our downstream analysis.\n\npheno_table2 = pheno_table.annotate(sc= pheno_table.sex_code -1, ccs=pheno_table.case_control_status -1)\npheno_table2.show()\n\n2023-04-18 21:01:24 Hail: INFO: Coerced sorted dataset\n\n\n\nfidiidfather_iidmother_iidsex_codecase_control_statusscccsstrstrint32int32int32int32int32int32\n\"sample_1_10\"\"sample_1_10\"001201\n\"sample_1_10003\"\"sample_1_10003\"002211\n\"sample_1_10014\"\"sample_1_10014\"002110\n\"sample_1_10020\"\"sample_1_10020\"001201\n\"sample_1_10021\"\"sample_1_10021\"002110\n\"sample_1_10023\"\"sample_1_10023\"001100\n\"sample_1_10036\"\"sample_1_10036\"002110\n\"sample_1_10037\"\"sample_1_10037\"001100\n\"sample_1_10047\"\"sample_1_10047\"001100\n\"sample_1_10056\"\"sample_1_10056\"001201\nshowing top 10 rows\n\n\n\n\n5.7.2 Filtering our pheno_table using .filter()\nThe .filter() method is used a lot in Hail, for subsetting tables. We’ll leverage this ability to filter on our QC tables and then subset our MatrixTable with our filtered table. Here we will select those in the table that have a .sc of 0. We assign it into the pheno3 object.\n\npheno3 = pheno_table2.filter(pheno_table2.sc == 0)\npheno3.show()\n\n2023-04-18 20:01:22 Hail: INFO: Coerced sorted dataset\n\n\n\nfidiidfather_iidmother_iidsex_codecase_control_statusscccsstrstrint32int32int32int32int32int32\n\"sample_1_10\"\"sample_1_10\"001201\n\"sample_1_10020\"\"sample_1_10020\"001201\n\"sample_1_10023\"\"sample_1_10023\"001100\n\"sample_1_10037\"\"sample_1_10037\"001100\n\"sample_1_10047\"\"sample_1_10047\"001100\n\"sample_1_10056\"\"sample_1_10056\"001201\n\"sample_1_10074\"\"sample_1_10074\"001201\n\"sample_1_10077\"\"sample_1_10077\"001100\n\"sample_1_10082\"\"sample_1_10082\"001100\n\"sample_1_10092\"\"sample_1_10092\"001100\nshowing top 10 rows"
  },
  {
    "objectID": "01-Hail-Skills.html#getting-categorical-breakdowns",
    "href": "01-Hail-Skills.html#getting-categorical-breakdowns",
    "title": "5  Hail Skills and Concepts",
    "section": "5.8 Getting Categorical Breakdowns",
    "text": "5.8 Getting Categorical Breakdowns\nWe can count categories using pheno3.aggregate() and hl.agg.counter(). Here we’re getting the categorical breakdown of the recoded Case Constrol Status variable pheno3.ccs.\nKeep in mind that these operations can be expensive to calculate in terms of CPU time. That doesn’t mean that you shouldn’t do it, but even getting descriptive statistics on a Hail Table can take some time.\nIn our case, our pheno3 Hail Table is pretty small, so we can go ahead and get information about the categorical breakdown for this variable.\n\npheno3.aggregate(hl.agg.counter(pheno3.ccs))\n\nfrozendict({0: 4637, 1: 4503})"
  },
  {
    "objectID": "01-Hail-Skills.html#plotting-the-hail-table",
    "href": "01-Hail-Skills.html#plotting-the-hail-table",
    "title": "5  Hail Skills and Concepts",
    "section": "5.9 Plotting the Hail Table",
    "text": "5.9 Plotting the Hail Table\nWe can plot the Hail Table using hl.plot.histogram() on our columns of interest. There are a number of different kinds of plots in hl.plot.\nThe first thing we need to do is to load some modules from Bokeh, which will make our graphs more interactive.\n\nfrom bokeh.io import output_notebook, show\noutput_notebook()\n\n\n    \n        \n        Loading BokehJS ...\n    \n\n\n\n\n\n\nNow we can use hl.plot.histogram() on columns in our Hail Table. We’ll see that the plotting works similarly for MatrixTables.\n\np = hl.plot.histogram(pheno_table2.ccs, title='Distribution of Cases/Controls', legend='ccs')\nshow(p)\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\np = hl.plot.histogram(pheno_table2.sc, range=(0,1), bins=100, title='Distribution of Gender', legend='sc')\nshow(p)"
  },
  {
    "objectID": "01-Hail-Skills.html#working-with-matrixtables",
    "href": "01-Hail-Skills.html#working-with-matrixtables",
    "title": "5  Hail Skills and Concepts",
    "section": "5.10 Working with MatrixTables",
    "text": "5.10 Working with MatrixTables\n\n\n\nimage.png\n\n\nMatrixTables are an extension of HailTables. They work like a 2-D matrix with two tables that are attached to them:\n\nColumns fields (Sample Based Operations), accessed using .cols()\nRow fields (Variant based operations. accessed using .rows()\n\nIf you remember this, it makes MatrixTable operations more understandable.\nWe’ll load in a MatrixTable that I’ve already created. This was created from the pVCF files in the Geno_Data/ directory using the hl.import_vcf() function, and then written to a dnax database using mt.write(). (see supplementary_notebooks/01_import_pVCF.ipynb).\nNote the url has the format dnax://database-id/mt-name, where database_id is the actual database id. You can find this from the database name in a project by the following:\n\n# read MT\nmt_url=\"dnax://database-GQYV2Bj04bPg9X3KfFyj2jyY/geno.mt\"\nmt = hl.read_matrix_table(mt_url)\n\nWe can see from using .describe() that there are Column and Row fields. There are a lot more row fields than column fields. When we merge in Pheno Data to our MatrixTable, we’ll see more column fields will be added by that operation.\n\n# View structure of MT before QC\n\nmt.describe()\n\n----------------------------------------\nGlobal fields:\n    None\n----------------------------------------\nColumn fields:\n    's': str\n----------------------------------------\nRow fields:\n    'locus': locus<GRCh38>\n    'alleles': array<str>\n    'rsid': str\n    'qual': float64\n    'filters': set<str>\n    'info': struct {\n        AC: array<int32>, \n        AF: array<float64>, \n        AN: int32, \n        BaseQRankSum: float64, \n        ClippingRankSum: float64, \n        DB: bool, \n        DP: int32, \n        FS: float64, \n        InbreedingCoeff: float64, \n        MQ: float64, \n        MQRankSum: float64, \n        QD: float64, \n        ReadPosRankSum: float64, \n        SOR: float64, \n        VQSLOD: float64, \n        VQSR_culprit: str, \n        VQSR_NEGATIVE_TRAIN_SITE: bool, \n        VQSR_POSITIVE_TRAIN_SITE: bool, \n        GQ_HIST_ALT: array<str>, \n        DP_HIST_ALT: array<str>, \n        AB_HIST_ALT: array<str>, \n        GQ_HIST_ALL: str, \n        DP_HIST_ALL: str, \n        AB_HIST_ALL: str, \n        AC_AFR: array<int32>, \n        AC_AMR: array<int32>, \n        AC_ASJ: array<int32>, \n        AC_EAS: array<int32>, \n        AC_FIN: array<int32>, \n        AC_NFE: array<int32>, \n        AC_OTH: array<int32>, \n        AC_SAS: array<int32>, \n        AC_Male: array<int32>, \n        AC_Female: array<int32>, \n        AN_AFR: int32, \n        AN_AMR: int32, \n        AN_ASJ: int32, \n        AN_EAS: int32, \n        AN_FIN: int32, \n        AN_NFE: int32, \n        AN_OTH: int32, \n        AN_SAS: int32, \n        AN_Male: int32, \n        AN_Female: int32, \n        AF_AFR: array<float64>, \n        AF_AMR: array<float64>, \n        AF_ASJ: array<float64>, \n        AF_EAS: array<float64>, \n        AF_FIN: array<float64>, \n        AF_NFE: array<float64>, \n        AF_OTH: array<float64>, \n        AF_SAS: array<float64>, \n        AF_Male: array<float64>, \n        AF_Female: array<float64>, \n        GC_AFR: array<int32>, \n        GC_AMR: array<int32>, \n        GC_ASJ: array<int32>, \n        GC_EAS: array<int32>, \n        GC_FIN: array<int32>, \n        GC_NFE: array<int32>, \n        GC_OTH: array<int32>, \n        GC_SAS: array<int32>, \n        GC_Male: array<int32>, \n        GC_Female: array<int32>, \n        AC_raw: array<int32>, \n        AN_raw: int32, \n        AF_raw: array<float64>, \n        GC_raw: array<int32>, \n        GC: array<int32>, \n        Hom_AFR: array<int32>, \n        Hom_AMR: array<int32>, \n        Hom_ASJ: array<int32>, \n        Hom_EAS: array<int32>, \n        Hom_FIN: array<int32>, \n        Hom_NFE: array<int32>, \n        Hom_OTH: array<int32>, \n        Hom_SAS: array<int32>, \n        Hom_Male: array<int32>, \n        Hom_Female: array<int32>, \n        Hom_raw: array<int32>, \n        Hom: array<int32>, \n        STAR_AC: int32, \n        STAR_AC_raw: int32, \n        STAR_Hom: int32, \n        POPMAX: array<str>, \n        AC_POPMAX: array<int32>, \n        AN_POPMAX: array<int32>, \n        AF_POPMAX: array<float64>, \n        DP_MEDIAN: array<int32>, \n        DREF_MEDIAN: array<float64>, \n        GQ_MEDIAN: array<int32>, \n        AB_MEDIAN: array<float64>, \n        AS_RF: array<float64>, \n        AS_FilterStatus: array<str>, \n        AS_RF_POSITIVE_TRAIN: array<int32>, \n        AS_RF_NEGATIVE_TRAIN: array<int32>, \n        AC_AFR_Male: array<int32>, \n        AC_AMR_Male: array<int32>, \n        AC_ASJ_Male: array<int32>, \n        AC_EAS_Male: array<int32>, \n        AC_FIN_Male: array<int32>, \n        AC_NFE_Male: array<int32>, \n        AC_OTH_Male: array<int32>, \n        AC_SAS_Male: array<int32>, \n        AC_AFR_Female: array<int32>, \n        AC_AMR_Female: array<int32>, \n        AC_ASJ_Female: array<int32>, \n        AC_EAS_Female: array<int32>, \n        AC_FIN_Female: array<int32>, \n        AC_NFE_Female: array<int32>, \n        AC_OTH_Female: array<int32>, \n        AC_SAS_Female: array<int32>, \n        AN_AFR_Male: int32, \n        AN_AMR_Male: int32, \n        AN_ASJ_Male: int32, \n        AN_EAS_Male: int32, \n        AN_FIN_Male: int32, \n        AN_NFE_Male: int32, \n        AN_OTH_Male: int32, \n        AN_SAS_Male: int32, \n        AN_AFR_Female: int32, \n        AN_AMR_Female: int32, \n        AN_ASJ_Female: int32, \n        AN_EAS_Female: int32, \n        AN_FIN_Female: int32, \n        AN_NFE_Female: int32, \n        AN_OTH_Female: int32, \n        AN_SAS_Female: int32, \n        AF_AFR_Male: array<float64>, \n        AF_AMR_Male: array<float64>, \n        AF_ASJ_Male: array<float64>, \n        AF_EAS_Male: array<float64>, \n        AF_FIN_Male: array<float64>, \n        AF_NFE_Male: array<float64>, \n        AF_OTH_Male: array<float64>, \n        AF_SAS_Male: array<float64>, \n        AF_AFR_Female: array<float64>, \n        AF_AMR_Female: array<float64>, \n        AF_ASJ_Female: array<float64>, \n        AF_EAS_Female: array<float64>, \n        AF_FIN_Female: array<float64>, \n        AF_NFE_Female: array<float64>, \n        AF_OTH_Female: array<float64>, \n        AF_SAS_Female: array<float64>, \n        GC_AFR_Male: array<int32>, \n        GC_AMR_Male: array<int32>, \n        GC_ASJ_Male: array<int32>, \n        GC_EAS_Male: array<int32>, \n        GC_FIN_Male: array<int32>, \n        GC_NFE_Male: array<int32>, \n        GC_OTH_Male: array<int32>, \n        GC_SAS_Male: array<int32>, \n        GC_AFR_Female: array<int32>, \n        GC_AMR_Female: array<int32>, \n        GC_ASJ_Female: array<int32>, \n        GC_EAS_Female: array<int32>, \n        GC_FIN_Female: array<int32>, \n        GC_NFE_Female: array<int32>, \n        GC_OTH_Female: array<int32>, \n        GC_SAS_Female: array<int32>, \n        Hemi_AFR: array<int32>, \n        Hemi_AMR: array<int32>, \n        Hemi_ASJ: array<int32>, \n        Hemi_EAS: array<int32>, \n        Hemi_FIN: array<int32>, \n        Hemi_NFE: array<int32>, \n        Hemi_OTH: array<int32>, \n        Hemi_SAS: array<int32>, \n        Hemi: array<int32>, \n        Hemi_raw: array<int32>, \n        STAR_Hemi: int32\n    }\n----------------------------------------\nEntry fields:\n    'GT': call\n    'AD': array<int32>\n    'DP': int32\n    'GQ': int32\n    'PL': array<int32>\n----------------------------------------\nColumn key: ['s']\nRow key: ['locus', 'alleles']\n----------------------------------------\n\n\nWe can see the actual genotype calls for each locus and each sample using mt.GT.show():\n\nmt.GT.show()\n\n\n'sample_1_0''sample_1_1''sample_1_2''sample_1_3'locusallelesGTGTGTGTlocus<GRCh38>array<str>callcallcallcall\nchr1:12198[\"G\",\"C\"]0/10/01/10/0\nchr1:12237[\"G\",\"A\"]1/10/00/10/0\nchr1:12259[\"G\",\"C\"]1/10/00/10/0\nchr1:12266[\"G\",\"A\"]0/11/10/10/0\nchr1:12272[\"G\",\"A\"]0/00/10/00/1\nchr1:12554[\"A\",\"G\"]0/00/00/00/0\nchr1:12559[\"G\",\"A\"]0/00/00/00/0\nchr1:12573[\"T\",\"C\"]0/00/00/00/0\nchr1:12586[\"C\",\"T\"]0/00/00/00/0\nchr1:12596[\"C\",\"A\"]0/00/00/00/0\nshowing top 10 rows\nshowing the first 4 of 100000 columns\n\n\nIf we do a .show() operation on mt.row_key we will retrieve the keys used in the row operations. Here they are a combination of locus position and the allele at that position.\n\nmt.row_key.show()\n\n\nlocusalleleslocus<GRCh38>array<str>\nchr1:12198[\"G\",\"C\"]\nchr1:12237[\"G\",\"A\"]\nchr1:12259[\"G\",\"C\"]\nchr1:12266[\"G\",\"A\"]\nchr1:12272[\"G\",\"A\"]\nchr1:12554[\"A\",\"G\"]\nchr1:12559[\"G\",\"A\"]\nchr1:12573[\"T\",\"C\"]\nchr1:12586[\"C\",\"T\"]\nchr1:12596[\"C\",\"A\"]\nshowing top 10 rows\n\n\nIf we do a mt.rows().show(), we can see the row fields themselves. Remember our keys for this table are locus_id and allele.\n\nmt.rows().show()\n\n\n\n\nimage.png\n\n\nWe can access a row field (here AF = Allele Frequency) by first going into the .info slot and then accessing the AF field within it. Keep this in mind for when you are using .filter_rows(), since it’s how you will accesss the fields.\n\nmt.info.AF.show()\n\nIf we use mt.aggregate_entries() and hl.agg.counter() on mt.GT.n_alt_alleles(), we will get the distribution of zygosities for our entire MatrixTable.\n\nmt.aggregate_entries(hl.agg.counter(mt.GT.n_alt_alleles()))"
  },
  {
    "objectID": "01-Hail-Skills.html#run-hails-sample-qc-method-on-a-matrixtable",
    "href": "01-Hail-Skills.html#run-hails-sample-qc-method-on-a-matrixtable",
    "title": "5  Hail Skills and Concepts",
    "section": "5.11 Run Hail’s Sample QC method on a MatrixTable",
    "text": "5.11 Run Hail’s Sample QC method on a MatrixTable\nIf we use hl.sample_qc() on our MatrixTable, it will create a new MatrixTable that has the QC information attached. You’ll notice that we have an additional column field called mt.sample_qc. This is a nested column field - it will contain additional metrics within in.\nNote that we’re not going to create the sample QC table here - we will load it from the DNAX store when we do our GWAS. This is because it takes some time to calculate. For more about sample QC, refer to the OpenBio Sample QC notebook here: https://github.com/dnanexus/OpenBio/blob/master/hail_tutorial/sample_qc.ipynb\nAdditional documentation: https://hail.is/docs/0.2/methods/genetics.html#hail.methods.sample_qc\n\n# DON'T RUN in class\n# Run sample-level QC\n\nqc_mt = hl.sample_qc(mt)\n\n\n# View structure of MT after QC\n# Don't Run this cell\n\nqc_mt.describe()\n\n----------------------------------------\n\nGlobal fields:\n\n    None\n\n----------------------------------------\n\nColumn fields:\n\n    's': str\n\n    'sample_qc': struct {\n\n        dp_stats: struct {\n\n            mean: float64, \n\n            stdev: float64, \n\n            min: float64, \n\n            max: float64\n\n        }, \n\n        gq_stats: struct {\n\n            mean: float64, \n\n            stdev: float64, \n\n            min: float64, \n\n            max: float64\n\n        }, \n\n        call_rate: float64, \n\n        n_called: int64, \n\n        n_not_called: int64, \n\n        n_filtered: int64, \n\n        n_hom_ref: int64, \n\n        n_het: int64, \n\n        n_hom_var: int64, \n\n        n_non_ref: int64, \n\n        n_singleton: int64, \n\n        n_snp: int64, \n\n        n_insertion: int64, \n\n        n_deletion: int64, \n\n        n_transition: int64, \n\n        n_transversion: int64, \n\n        n_star: int64, \n\n        r_ti_tv: float64, \n\n        r_het_hom_var: float64, \n\n        r_insertion_deletion: float64\n\n    }\n\n----------------------------------------\n\nRow fields:\n\n    'locus': locus<GRCh38>\n\n    'alleles': array<str>\n\n    'rsid': str\n\n    'qual': float64\n\n    'filters': set<str>\n\n    'info': struct {\n\n        AC: array<int32>, \n\n        AF: array<float64>, \n\n        AN: int32, \n\n        BaseQRankSum: float64, \n\n        ClippingRankSum: float64, \n\n        DB: bool, \n\n        DP: int32, \n\n        FS: float64, \n\n        InbreedingCoeff: float64, \n\n        MQ: float64, \n\n        MQRankSum: float64, \n\n        QD: float64, \n\n        ReadPosRankSum: float64, \n\n        SOR: float64, \n\n        VQSLOD: float64, \n\n        VQSR_culprit: str, \n\n        VQSR_NEGATIVE_TRAIN_SITE: bool, \n\n        VQSR_POSITIVE_TRAIN_SITE: bool, \n\n        GQ_HIST_ALT: array<str>, \n\n        DP_HIST_ALT: array<str>, \n\n        AB_HIST_ALT: array<str>, \n\n        GQ_HIST_ALL: str, \n\n        DP_HIST_ALL: str, \n\n        AB_HIST_ALL: str, \n\n        AC_AFR: array<int32>, \n\n        AC_AMR: array<int32>, \n\n        AC_ASJ: array<int32>, \n\n        AC_EAS: array<int32>, \n\n        AC_FIN: array<int32>, \n\n        AC_NFE: array<int32>, \n\n        AC_OTH: array<int32>, \n\n        AC_SAS: array<int32>, \n\n        AC_Male: array<int32>, \n\n        AC_Female: array<int32>, \n\n        AN_AFR: int32, \n\n        AN_AMR: int32, \n\n        AN_ASJ: int32, \n\n        AN_EAS: int32, \n\n        AN_FIN: int32, \n\n        AN_NFE: int32, \n\n        AN_OTH: int32, \n\n        AN_SAS: int32, \n\n        AN_Male: int32, \n\n        AN_Female: int32, \n\n        AF_AFR: array<float64>, \n\n        AF_AMR: array<float64>, \n\n        AF_ASJ: array<float64>, \n\n        AF_EAS: array<float64>, \n\n        AF_FIN: array<float64>, \n\n        AF_NFE: array<float64>, \n\n        AF_OTH: array<float64>, \n\n        AF_SAS: array<float64>, \n\n        AF_Male: array<float64>, \n\n        AF_Female: array<float64>, \n\n        GC_AFR: array<int32>, \n\n        GC_AMR: array<int32>, \n\n        GC_ASJ: array<int32>, \n\n        GC_EAS: array<int32>, \n\n        GC_FIN: array<int32>, \n\n        GC_NFE: array<int32>, \n\n        GC_OTH: array<int32>, \n\n        GC_SAS: array<int32>, \n\n        GC_Male: array<int32>, \n\n        GC_Female: array<int32>, \n\n        AC_raw: array<int32>, \n\n        AN_raw: int32, \n\n        AF_raw: array<float64>, \n\n        GC_raw: array<int32>, \n\n        GC: array<int32>, \n\n        Hom_AFR: array<int32>, \n\n        Hom_AMR: array<int32>, \n\n        Hom_ASJ: array<int32>, \n\n        Hom_EAS: array<int32>, \n\n        Hom_FIN: array<int32>, \n\n        Hom_NFE: array<int32>, \n\n        Hom_OTH: array<int32>, \n\n        Hom_SAS: array<int32>, \n\n        Hom_Male: array<int32>, \n\n        Hom_Female: array<int32>, \n\n        Hom_raw: array<int32>, \n\n        Hom: array<int32>, \n\n        STAR_AC: int32, \n\n        STAR_AC_raw: int32, \n\n        STAR_Hom: int32, \n\n        POPMAX: array<str>, \n\n        AC_POPMAX: array<int32>, \n\n        AN_POPMAX: array<int32>, \n\n        AF_POPMAX: array<float64>, \n\n        DP_MEDIAN: array<int32>, \n\n        DREF_MEDIAN: array<float64>, \n\n        GQ_MEDIAN: array<int32>, \n\n        AB_MEDIAN: array<float64>, \n\n        AS_RF: array<float64>, \n\n        AS_FilterStatus: array<str>, \n\n        AS_RF_POSITIVE_TRAIN: array<int32>, \n\n        AS_RF_NEGATIVE_TRAIN: array<int32>, \n\n        AC_AFR_Male: array<int32>, \n\n        AC_AMR_Male: array<int32>, \n\n        AC_ASJ_Male: array<int32>, \n\n        AC_EAS_Male: array<int32>, \n\n        AC_FIN_Male: array<int32>, \n\n        AC_NFE_Male: array<int32>, \n\n        AC_OTH_Male: array<int32>, \n\n        AC_SAS_Male: array<int32>, \n\n        AC_AFR_Female: array<int32>, \n\n        AC_AMR_Female: array<int32>, \n\n        AC_ASJ_Female: array<int32>, \n\n        AC_EAS_Female: array<int32>, \n\n        AC_FIN_Female: array<int32>, \n\n        AC_NFE_Female: array<int32>, \n\n        AC_OTH_Female: array<int32>, \n\n        AC_SAS_Female: array<int32>, \n\n        AN_AFR_Male: int32, \n\n        AN_AMR_Male: int32, \n\n        AN_ASJ_Male: int32, \n\n        AN_EAS_Male: int32, \n\n        AN_FIN_Male: int32, \n\n        AN_NFE_Male: int32, \n\n        AN_OTH_Male: int32, \n\n        AN_SAS_Male: int32, \n\n        AN_AFR_Female: int32, \n\n        AN_AMR_Female: int32, \n\n        AN_ASJ_Female: int32, \n\n        AN_EAS_Female: int32, \n\n        AN_FIN_Female: int32, \n\n        AN_NFE_Female: int32, \n\n        AN_OTH_Female: int32, \n\n        AN_SAS_Female: int32, \n\n        AF_AFR_Male: array<float64>, \n\n        AF_AMR_Male: array<float64>, \n\n        AF_ASJ_Male: array<float64>, \n\n        AF_EAS_Male: array<float64>, \n\n        AF_FIN_Male: array<float64>, \n\n        AF_NFE_Male: array<float64>, \n\n        AF_OTH_Male: array<float64>, \n\n        AF_SAS_Male: array<float64>, \n\n        AF_AFR_Female: array<float64>, \n\n        AF_AMR_Female: array<float64>, \n\n        AF_ASJ_Female: array<float64>, \n\n        AF_EAS_Female: array<float64>, \n\n        AF_FIN_Female: array<float64>, \n\n        AF_NFE_Female: array<float64>, \n\n        AF_OTH_Female: array<float64>, \n\n        AF_SAS_Female: array<float64>, \n\n        GC_AFR_Male: array<int32>, \n\n        GC_AMR_Male: array<int32>, \n\n        GC_ASJ_Male: array<int32>, \n\n        GC_EAS_Male: array<int32>, \n\n        GC_FIN_Male: array<int32>, \n\n        GC_NFE_Male: array<int32>, \n\n        GC_OTH_Male: array<int32>, \n\n        GC_SAS_Male: array<int32>, \n\n        GC_AFR_Female: array<int32>, \n\n        GC_AMR_Female: array<int32>, \n\n        GC_ASJ_Female: array<int32>, \n\n        GC_EAS_Female: array<int32>, \n\n        GC_FIN_Female: array<int32>, \n\n        GC_NFE_Female: array<int32>, \n\n        GC_OTH_Female: array<int32>, \n\n        GC_SAS_Female: array<int32>, \n\n        Hemi_AFR: array<int32>, \n\n        Hemi_AMR: array<int32>, \n\n        Hemi_ASJ: array<int32>, \n\n        Hemi_EAS: array<int32>, \n\n        Hemi_FIN: array<int32>, \n\n        Hemi_NFE: array<int32>, \n\n        Hemi_OTH: array<int32>, \n\n        Hemi_SAS: array<int32>, \n\n        Hemi: array<int32>, \n\n        Hemi_raw: array<int32>, \n\n        STAR_Hemi: int32\n\n    }\n\n----------------------------------------\n\nEntry fields:\n\n    'GT': call\n\n    'AD': array<int32>\n\n    'DP': int32\n\n    'GQ': int32\n\n    'PL': array<int32>\n\n----------------------------------------\n\nColumn key: ['s']\n\nRow key: ['locus', 'alleles']\n\n----------------------------------------\n\n\nWe can see that a new column field called ‘sample_qc’ has been added the MT. Note that the sample_qc field is not calculated yet. If we do a .show(), then we will calculate the QC metrics.\nThis step took a long time to calculate, over 20 minutes on a six node instance (mem2_ssd1_v2_x8), so we won’t calculate it.\n\n%%timeit\nqc_mt.cols().show()\n\n2023-04-11 16:20:15 Hail: WARN: cols(): Resulting column table is sorted by 'col_key'.\n\n    To preserve matrix table column order, first unkey columns with 'key_cols_by()'\n\n\n\nsample_qcdp_statsgq_statssmeanstdevminmaxmeanstdevminmaxcall_raten_calledn_not_calledn_filteredn_hom_refn_hetn_hom_varn_non_refn_singletonn_snpn_insertionn_deletionn_transitionn_transversionn_starr_ti_tvr_het_hom_varr_insertion_deletionstrfloat64float64float64float64float64float64float64float64float64int64int64int64int64int64int64int64int64int64int64int64int64int64int64float64float64float64\n\"sample_1_0\"NaNNaNNANANaNNaNNANA9.90e-01336926134252033550429260495914219516921997137911724519702.26e+001.87e+007.23e-01\n\"sample_1_1\"NaNNaNNANANaNNaNNANA9.90e-01337000033513033558439177498014157116877949142511640523702.22e+001.84e+006.66e-01\n\"sample_1_10\"NaNNaNNANANaNNaNNANA9.90e-01337113832375033571049121491314034316724982135611596512802.26e+001.86e+007.24e-01\n\"sample_1_100\"NaNNaNNANANaNNaNNANA9.90e-013369744337690335574191134890140032166351004136811567506802.28e+001.86e+007.34e-01\n\"sample_1_1000\"NaNNaNNANANaNNaNNANA9.90e-01336966133852033556349190483714027316604995136111540506402.28e+001.90e+007.31e-01\n\"sample_1_10000\"NaNNaNNANANaNNaNNANA9.90e-01336896634547033548429237488714124016749986138711647510202.28e+001.89e+007.11e-01\n\"sample_1_10001\"NaNNaNNANANaNNaNNANA9.90e-013368745347680335468291604903140632167321006133711633509902.28e+001.87e+007.52e-01\n\"sample_1_10002\"NaNNaNNANANaNNaNNANA9.90e-01336865434859033545869102496614068416792973138911634515802.26e+001.83e+007.01e-01\n\"sample_1_10003\"NaNNaNNANANaNNaNNANA9.90e-01337101032503033569189221487114092316674998141311508516602.23e+001.89e+007.06e-01\n\"sample_1_10004\"NaNNaNNANANaNNaNNANA9.90e-013368964345490335490292054857140622166931029133211563513002.25e+001.90e+007.73e-01\nshowing top 10 rows\n\n\nWe see that there is a column in qc_mt.sample_qc called call_rate. We can access it using:\nqc_mt.sample_qc.call_rate or qc_mt[\"sample_qc\"][\"call_rate\"] if we want to do something with it."
  },
  {
    "objectID": "01-Hail-Skills.html#create-sample-qc-table-and-save-in-apollo-database",
    "href": "01-Hail-Skills.html#create-sample-qc-table-and-save-in-apollo-database",
    "title": "5  Hail Skills and Concepts",
    "section": "5.12 4) Create Sample QC Table and save in Apollo Database",
    "text": "5.12 4) Create Sample QC Table and save in Apollo Database\nNow that we have calculated our sample qc metrics, we can save them as a separate table. We are mostly doing this for convenience and speed.\nIn our case, this has already been done and calculated, so we’re just showing the process of storing a MatrixTable into dnax.\nNote we could also do mt.write(\"dnax://{db_name}/geno.mt\") and store the MatrixTable into dnax as well. Using dnax is helpful because Hail can read the MatrixTable directly from dnax.\n\n# Create Hail Table from MT\n# note we use .cols() to access the column (sample) fields\nqc_tb = qc_mt.cols()\n\n\n# Define database and table name\n\n# Note: It is recommended to only use lowercase letters for the database name.\n# If uppercase lettering is used, the database name will be lowercased when creating the database.\ndb_name = \"db_mt_test2\"\ntb_name = \"sample_qc.ht\"\n\n\n# DON'T RUN in class\n# Create database in DNAX\n\nstmt = f\"CREATE DATABASE IF NOT EXISTS {db_name} LOCATION 'dnax://'\"\nprint(stmt)\nspark.sql(stmt).show()\n\nCREATE DATABASE IF NOT EXISTS db_mt_test2 LOCATION 'dnax://'\n\n++\n\n||\n\n++\n\n++\n\n\n\n\n\n# DON'T RUN in class\n# Store Table in DNAXc\nimport dxpy\n\n# find database ID of newly created database using a dxpy method\ndb_uri = dxpy.find_one_data_object(name=f\"{db_name}\", classname=\"database\")['id']\nurl = f\"dnax://{db_uri}/{tb_name}\"\n\n# Before this step, the Hail Table is just an object in memory. To persist it and be able to access \n# it later, the notebook needs to write it into a persistent filesystem (in this case DNAX).\n# See https://hail.is/docs/0.2/hail.Table.html#hail.Table.write for additional documentation.\nqc_tb.write(url) # Note: output should describe size of Table (i.e. number of rows, partitions)"
  },
  {
    "objectID": "02-gwas.html#learning-objectives-of-this-session",
    "href": "02-gwas.html#learning-objectives-of-this-session",
    "title": "6  GWAS using Hail",
    "section": "6.1 Learning Objectives of this Session",
    "text": "6.1 Learning Objectives of this Session\n\nImport data into a Hail MatrixTable from pVCF and BGEN formats\nMerge phenotypic data into the MatrixTable\nQC and Filter Variant data for use in a GWAS using Hail\nQC and Filter Sample data for use in GWAS with Hail\nRun GWAS on a Hail MatrixTable\nAnnotate Hail MatrixTables with internal DNAnexus resources\nExport results from Hail to CSV format\n\n\nSuggested Configuration: mem2_ssd1_v1_x8 with at least 6 nodes (use Hail/VEP feature set)\n\nThis notebook shows how to perform a GWAS for 1 case–control trait using Firth’s logistic regression with Hail and save the results as a Hail Table to an Apollo database (dnax://) on the DNAnexus platform. See documentation for guidance on launch specs for the JupyterLab with Spark Cluster app for different data sizes: https://documentation.dnanexus.com/science/using-hail-to-analyze-genomic-data\nNote: For population scale data, samples may be referred to as individuals. In this notebook, the word “sample” will be used.\nPre-conditions for running this notebook successfully:\n\nThere is an existing Hail MatrixTable in DNAX (see pVCF Import Notebook and BGEN Import Notebook)\nThere is a Sample QC Hail Table in DNAX (see https://github.com/dnanexus/OpenBio/blob/master/hail_tutorial/sample_qc.ipynb)\nThere is a Variant QC Hail Table in DNAX (see https://github.com/dnanexus/OpenBio/blob/master/hail_tutorial/locus_qc.ipynb)\nThere is phenotypic data for the samples"
  },
  {
    "objectID": "02-gwas.html#basic-gwas-process",
    "href": "02-gwas.html#basic-gwas-process",
    "title": "6  GWAS using Hail",
    "section": "6.2 Basic GWAS process",
    "text": "6.2 Basic GWAS process\nThis table shows the basic GWAS process using Hail. We will cover each of these in the sections below.\n\n\n\n\n\n\n\n\nStep\nDescription\nCode Example\n\n\n\n\n0.\nInitiate Spark and Hail\nSee Notebook\n\n\n1.\nLoad pVCF/BGEN data, save as MatrixTable (MT) Section 6.4\nmt = hl.import_vcf(path) or mt = hl.import_bgen(path)\n\n\n2.\nLoad Pheno file, merge with MatrixTable Section 6.5\nphenogeno_mt = mt.annotate_cols(**pheno_table[mt.s])\n\n\n3.\nBuild Sample QC table from MT, use to filter MT Section 6.6\nsample_qc_tb = hl.sample_qc(mt); qc_mt = phenogeno_mt.semi_join_rows(locus_qc_tb)\n\n\n4.\nBuild Locus QC table from MT, use to filter MT Section 6.7\nlocus_qc_tb = hl.locus_qc(mt); qc_mt = qc_mt.semi_join_cols(sample_qc_tb)\n\n\n5.\nRun GWAS Section 6.8\ngwas_tb = hl.logistic_regression.rows()\n\n\n6.\nVisualize Results Section 6.9\nmanhattan_plot = hl.plot.manhattan(gwas_tb.p_value); show(manhattan_plot)\n\n\n7.\nAnnotate Results with VEP or annotation db Section 6.10\nann_gwas_tb = db.annotate_rows_db(gwas_tb, 'gencode'\n\n\n8.\nSave results to CSV, export chromosomes as BGEN file\nSee Notebook"
  },
  {
    "objectID": "02-gwas.html#initialize-spark-and-hail",
    "href": "02-gwas.html#initialize-spark-and-hail",
    "title": "6  GWAS using Hail",
    "section": "6.3 Initialize Spark and Hail",
    "text": "6.3 Initialize Spark and Hail\nOnce you run the code below, make sure to open up another tab at https://JOB-URL:8081/jobs to look at the Spark UI. It is extremely helpful in understanding what’s going on behind the scenes.\n\n# Running this cell will output a red-colored message- this is expected.\n# The 'Welcome to Hail' message in the output will indicate that Hail is ready to use in the notebook.\n\nfrom pyspark.sql import SparkSession\nimport hail as hl\n\nbuilder = (\n    SparkSession\n    .builder\n    .enableHiveSupport()\n)\nspark = builder.getOrCreate()\nhl.init(sc=spark.sparkContext)\n\ndb_name = \"db_mt_test2\"\n\npip-installed Hail requires additional configuration options in Spark referring\n  to the path to the Hail Python module directory HAIL_DIR,\n  e.g. /path/to/python/site-packages/hail:\n    spark.jars=HAIL_DIR/hail-all-spark.jar\n    spark.driver.extraClassPath=HAIL_DIR/hail-all-spark.jar\n    spark.executor.extraClassPath=./hail-all-spark.jarRunning on Apache Spark version 2.4.4\nSparkUI available at http://ip-172-31-34-200.ec2.internal:8081\nWelcome to\n     __  __     <>__\n    / /_/ /__  __/ /\n   / __  / _ `/ / /\n  /_/ /_/\\_,_/_/_/   version 0.2.78-b17627756568\nLOGGING: writing to /opt/notebooks/hail-20230419-2230-0.2.78-b17627756568.log"
  },
  {
    "objectID": "02-gwas.html#sec-loading_data",
    "href": "02-gwas.html#sec-loading_data",
    "title": "6  GWAS using Hail",
    "section": "6.4 Read Genotype MatrixTable",
    "text": "6.4 Read Genotype MatrixTable\nWe’ll read in the MatrixTable we loaded in from the pVCF files.\nSee (supplementary_notebooks/01_pVCF_import.ipynb for more info)\nThis was the code used to load the VCF files:\npath = \"file:///mnt/project/Geno_Data/*.vcf.gz\"\n\nmt = hl.import_vcf(path, force_bgz=True,\n                   reference_genome=\"GRCh38\",\n                   array_elements_required=False)\n\n# read MT\nmt_url=\"dnax://database-GQYV2Bj04bPg9X3KfFyj2jyY/geno.mt\"\nmt = hl.read_matrix_table(mt_url)\n\nWe can look at the row fields (Variant metadata) using .rows().show():\n\nmt.rows().show()\n\n\ninfolocusallelesrsidqualfiltersACAFANBaseQRankSumClippingRankSumDBDPFSInbreedingCoeffMQMQRankSumQDReadPosRankSumSORVQSLODVQSR_culpritVQSR_NEGATIVE_TRAIN_SITEVQSR_POSITIVE_TRAIN_SITEGQ_HIST_ALTDP_HIST_ALTAB_HIST_ALTGQ_HIST_ALLDP_HIST_ALLAB_HIST_ALLAC_AFRAC_AMRAC_ASJAC_EASAC_FINAC_NFEAC_OTHAC_SASAC_MaleAC_FemaleAN_AFRAN_AMRAN_ASJAN_EASAN_FINAN_NFEAN_OTHAN_SASAN_MaleAN_FemaleAF_AFRAF_AMRAF_ASJAF_EASAF_FINAF_NFEAF_OTHAF_SASAF_MaleAF_FemaleGC_AFRGC_AMRGC_ASJGC_EASGC_FINGC_NFEGC_OTHGC_SASGC_MaleGC_FemaleAC_rawAN_rawAF_rawGC_rawGCHom_AFRHom_AMRHom_ASJHom_EASHom_FINHom_NFEHom_OTHHom_SASHom_MaleHom_FemaleHom_rawHomSTAR_ACSTAR_AC_rawSTAR_HomPOPMAXAC_POPMAXAN_POPMAXAF_POPMAXDP_MEDIANDREF_MEDIANGQ_MEDIANAB_MEDIANAS_RFAS_FilterStatusAS_RF_POSITIVE_TRAINAS_RF_NEGATIVE_TRAINAC_AFR_MaleAC_AMR_MaleAC_ASJ_MaleAC_EAS_MaleAC_FIN_MaleAC_NFE_MaleAC_OTH_MaleAC_SAS_MaleAC_AFR_FemaleAC_AMR_FemaleAC_ASJ_FemaleAC_EAS_FemaleAC_FIN_FemaleAC_NFE_FemaleAC_OTH_FemaleAC_SAS_FemaleAN_AFR_MaleAN_AMR_MaleAN_ASJ_MaleAN_EAS_MaleAN_FIN_MaleAN_NFE_MaleAN_OTH_MaleAN_SAS_MaleAN_AFR_FemaleAN_AMR_FemaleAN_ASJ_FemaleAN_EAS_FemaleAN_FIN_FemaleAN_NFE_FemaleAN_OTH_FemaleAN_SAS_FemaleAF_AFR_MaleAF_AMR_MaleAF_ASJ_MaleAF_EAS_MaleAF_FIN_MaleAF_NFE_MaleAF_OTH_MaleAF_SAS_MaleAF_AFR_FemaleAF_AMR_FemaleAF_ASJ_FemaleAF_EAS_FemaleAF_FIN_FemaleAF_NFE_FemaleAF_OTH_FemaleAF_SAS_FemaleGC_AFR_MaleGC_AMR_MaleGC_ASJ_MaleGC_EAS_MaleGC_FIN_MaleGC_NFE_MaleGC_OTH_MaleGC_SAS_MaleGC_AFR_FemaleGC_AMR_FemaleGC_ASJ_FemaleGC_EAS_FemaleGC_FIN_FemaleGC_NFE_FemaleGC_OTH_FemaleGC_SAS_FemaleHemi_AFRHemi_AMRHemi_ASJHemi_EASHemi_FINHemi_NFEHemi_OTHHemi_SASHemiHemi_rawSTAR_Hemilocus<GRCh38>array<str>strfloat64set<str>array<int32>array<float64>int32float64float64boolint32float64float64float64float64float64float64float64float64strboolboolarray<str>array<str>array<str>strstrstrarray<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>int32int32int32int32int32int32int32int32int32int32array<float64>array<float64>array<float64>array<float64>array<float64>array<float64>array<float64>array<float64>array<float64>array<float64>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>int32array<float64>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>int32int32int32array<str>array<int32>array<int32>array<float64>array<int32>array<float64>array<int32>array<float64>array<float64>array<str>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>int32int32int32int32int32int32int32int32int32int32int32int32int32int32int32int32array<float64>array<float64>array<float64>array<float64>array<float64>array<float64>array<float64>array<float64>array<float64>array<float64>array<float64>array<float64>array<float64>array<float64>array<float64>array<float64>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>array<int32>int32\nchr1:12198[\"G\",\"C\"]\"chr1_12198_G_C\"9.88e+03{\"AC0\"}[0]NA00.00e+003.58e-01False92040.00e+009.80e-032.30e+017.36e-011.40e+017.36e-013.02e-011.01e+00\"MQ\"FalseFalse[\"14|79|1|25|7|4|0|5|2|0|0|0|1|0|0|0|0|0|0|0\"][\"131|7|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0\"][\"0|0|0|0|1|0|2|0|2|0|10|0|1|28|0|3|0|0|0|0\"]\"1859|505|25|28|8|4|0|5|2|0|0|0|1|0|0|0|0|0|0|0\"\"2413|24|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0\"\"0|0|0|0|1|0|2|0|2|0|10|0|1|28|0|3|0|0|0|0\"[0][0][0][0][0][0][0][0][0][0]0000000000NANANANANANANANANANA[0,0,0][0,0,0][0,0,0][0,0,0][0,0,0][0,0,0][0,0,0][0,0,0][0,0,0][0,0,0][229]4874[4.70e-02][2299,47,91][0,0,0][0][0][0][0][0][0][0][0][0][0][91][0]NANANANANANANA[2][4.01e-06][6][6.67e-01][1.27e-01][\"AC0\"]NANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANA\nchr1:12237[\"G\",\"A\"]\"chr1_12237_G_A\"8.20e+01{\"RF\",\"AC0\"}[0]NA07.36e-01-3.58e-01False160960.00e+00-1.44e-012.20e+01-3.58e-014.31e+003.58e-011.29e+001.57e+00\"QD\"FalseFalse[\"0|1|0|1|1|0|0|0|2|0|0|0|0|0|0|0|0|0|0|0\"][\"3|2|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0\"][\"0|0|0|0|0|0|0|0|0|0|1|0|2|1|0|0|0|0|0|0\"]\"3537|1722|177|100|15|1|0|0|2|0|0|0|0|0|0|0|0|0|0|0\"\"5428|126|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0\"\"0|0|0|0|0|0|0|0|0|0|1|0|2|1|0|0|0|0|0|0\"[0][0][0][0][0][0][0][0][0][0]0000000000NANANANANANANANANANA[0,0,0][0,0,0][0,0,0][0,0,0][0,0,0][0,0,0][0,0,0][0,0,0][0,0,0][0,0,0][6]11108[5.40e-04][5549,4,1][0,0,0][0][0][0][0][0][0][0][0][0][0][1][0]NANANANANANANA[4][4.01e-06][22][6.00e-01][8.80e-02][\"RF|AC0\"]NANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANA\nchr1:12259[\"G\",\"C\"]\"chr1_12259_G_C\"3.74e+01{\"RF\",\"AC0\"}[0][0.00e+00]2NANAFalse188140.00e+00-1.42e-012.00e+01NA9.36e+00NA6.93e-012.24e+00\"MQ\"FalseFalse[\"0|1|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0\"][\"1|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0\"][\"0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0\"]\"3885|1993|253|143|22|3|1|0|0|0|0|0|0|0|0|0|0|0|0|0\"\"6117|182|1|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0\"\"0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0\"[0][0][0][0][0][0][0][0][0][0]0000000220NANANANANANANA[0.00e+00][0.00e+00]NA[0,0,0][0,0,0][0,0,0][0,0,0][0,0,0][0,0,0][0,0,0][1,0,0][1,0,0][0,0,0][2]12600[1.59e-04][6299,0,1][1,0,0][0][0][0][0][0][0][0][0][0][0][1][0]NANANANANANANA[2][1.01e-05][6][4.62e-01][4.93e-02][\"RF|AC0\"]NANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANA\nchr1:12266[\"G\",\"A\"]\"chr1_12266_G_A\"2.72e+03{\"AC0\"}[0]NA08.04e-010.00e+00False183720.00e+00-1.38e-012.23e+01-7.27e-011.61e+014.06e-012.03e-011.27e+00\"MQ\"FalseFalse[\"0|15|0|0|0|2|0|2|5|0|1|1|1|3|0|6|0|0|0|0\"][\"33|3|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0\"][\"0|0|0|0|0|2|0|0|2|0|9|0|0|8|0|0|0|0|0|0\"]\"3755|1684|188|101|11|4|0|2|5|0|1|1|1|3|0|6|0|0|0|0\"\"5598|163|1|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0\"\"0|0|0|0|0|2|0|0|2|0|9|0|0|8|0|0|0|0|0|0\"[0][0][0][0][0][0][0][0][0][0]0000000000NANANANANANANANANANA[0,0,0][0,0,0][0,0,0][0,0,0][0,0,0][0,0,0][0,0,0][0,0,0][0,0,0][0,0,0][51]11524[4.43e-03][5726,21,15][0,0,0][0][0][0][0][0][0][0][0][0][0][15][0]NANANANANANANA[3][1.19e-08][39][5.00e-01][2.37e-01][\"AC0\"]NANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANA\nchr1:12272[\"G\",\"A\"]\"chr1_12272_G_A\"2.71e+03{\"AC0\"}[0][0.00e+00]28.04e-013.58e-01False176850.00e+00-1.34e-012.23e+010.00e+001.69e+010.00e+001.96e-013.60e-02\"MQ\"FalseFalse[\"0|14|0|0|0|2|0|2|5|0|1|1|1|3|0|6|0|0|0|0\"][\"31|4|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0\"][\"0|0|0|0|0|0|1|0|3|0|10|0|0|7|0|0|0|0|0|0\"]\"3715|1603|160|87|9|2|1|2|5|0|1|1|1|3|0|6|0|0|0|0\"\"5448|147|1|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0\"\"0|0|0|0|0|0|1|0|3|0|10|0|0|7|0|0|0|0|0|0\"[0][0][0][0][0][0][0][0][0][0]0200000002NA[0.00e+00]NANANANANANANA[0.00e+00][0,0,0][1,0,0][0,0,0][0,0,0][0,0,0][0,0,0][0,0,0][0,0,0][0,0,0][1,0,0][49]11192[4.38e-03][5561,21,14][1,0,0][0][0][0][0][0][0][0][0][0][0][14][0]NANANANANANANA[3][7.94e-09][39][5.00e-01][2.32e-01][\"AC0\"]NANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANA\nchr1:12554[\"A\",\"G\"]\"chr1_12554_A_G\"6.81e+01{\"RF\",\"AC0\"}[0][0.00e+00]30221.06e+000.00e+00False1384360.00e+00-8.07e-022.25e+013.22e-016.20e-01-6.70e-021.88e-011.94e+00\"QD\"FalseFalse[\"0|2|0|1|1|1|3|1|2|0|0|0|0|0|0|0|0|0|0|0\"][\"3|6|2|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0\"][\"0|0|0|2|2|2|0|0|2|0|1|0|0|0|0|0|0|0|0|0\"]\"4802|6590|2323|3118|1771|582|666|341|111|143|70|21|22|8|10|4|4|0|0|0\"\"13350|5670|1233|271|48|14|0|0|0|0|0|0|0|0|0|0|0|0|0|0\"\"0|0|0|2|2|2|0|0|2|0|1|0|0|0|0|0|0|0|0|0\"[0][0][0][0][0][0][0][0][0][0]374564223781654284104217861236[0.00e+00][0.00e+00][0.00e+00][0.00e+00][0.00e+00][0.00e+00][0.00e+00][0.00e+00][0.00e+00][0.00e+00][187,0,0][282,0,0][11,0,0][189,0,0][8,0,0][271,0,0][42,0,0][521,0,0][893,0,0][618,0,0][13]41172[3.16e-04][20575,9,2][1511,0,0][0][0][0][0][0][0][0][0][0][0][2][0]NANANANANANANA[7][3.98e-04][32][2.86e-01][8.10e-03][\"RF|AC0\"]NANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANA\nchr1:12559[\"G\",\"A\"]\"chr1_12559_G_A\"1.67e+03{\"RF\"}[15][5.08e-03]29521.50e+00-1.18e-01False1396400.00e+00-8.47e-022.24e+01-1.98e-011.65e+000.00e+001.74e-012.13e+00\"QD\"FalseFalse[\"3|8|7|16|9|13|13|23|10|3|1|4|2|0|1|0|0|0|0|2\"][\"30|56|28|1|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0\"][\"0|0|5|10|17|24|12|4|12|0|11|0|0|9|0|2|0|0|0|0\"]\"5100|6458|2253|3094|1723|523|669|372|104|139|59|21|24|5|9|1|3|0|0|2\"\"13224|5716|1280|271|52|16|0|0|0|0|0|0|0|0|0|0|0|0|0|0\"\"0|0|5|10|17|24|12|4|12|0|11|0|0|9|0|2|0|0|0|0\"[0][3][0][1][0][1][1][9][9][6]42655222376185648291217421210[0.00e+00][5.43e-03][0.00e+00][2.66e-03][0.00e+00][1.77e-03][1.22e-02][9.87e-03][5.17e-03][4.96e-03][213,0,0][273,3,0][11,0,0][187,1,0][9,0,0][281,1,0][40,1,0][447,9,0][862,9,0][599,6,0][124]41118[3.02e-03][20444,106,9][1461,15,0][0][0][0][0][0][0][0][0][0][0][9][0]NANANA[\"SAS\"][9][912][9.87e-03][7][3.16e-04][30][2.86e-01][7.73e-03][\"RF\"]NANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANA\nchr1:12573[\"T\",\"C\"]\"chr1_12573_T_C\"3.67e+02{}[2][4.80e-04]41681.56e-01-3.87e-01False1596600.00e+00-7.56e-022.20e+01-7.20e-014.03e+004.06e-015.91e-013.25e+00\"QD\"FalseFalse[\"0|0|0|2|0|0|1|1|0|0|0|1|0|0|0|0|0|0|0|2\"][\"2|3|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0\"][\"0|0|0|0|0|1|1|0|0|1|1|0|1|1|0|1|0|0|0|0\"]\"4681|6449|2465|3412|2043|675|838|436|128|204|107|32|44|1|14|0|6|1|1|2\"\"12932|6401|1677|420|84|22|3|0|0|0|0|0|0|0|0|0|0|0|0|0\"\"0|0|0|0|0|1|1|0|0|1|1|0|1|1|0|1|0|0|0|0\"[0][2][0][0][0][0][0][0][0][2]6227164452836850124124823481820[0.00e+00][2.79e-03][0.00e+00][0.00e+00][0.00e+00][0.00e+00][0.00e+00][0.00e+00][0.00e+00][1.10e-03][311,0,0][356,2,0][22,0,0][264,0,0][18,0,0][425,0,0][62,0,0][624,0,0][1174,0,0][908,2,0][7]43078[1.62e-04][21532,7,0][2082,2,0][0][0][0][0][0][0][0][0][0][0][0][0]NANANA[\"AMR\"][2][716][2.79e-03][8][1.54e-07][36][5.33e-01][2.70e-01][\"PASS\"]NANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANA\nchr1:12586[\"C\",\"T\"]\"chr1_12586_C_T\"2.24e+02{}[2][5.63e-04]35502.93e+00-1.24e-01False1497390.00e+00-7.86e-022.20e+012.89e-015.74e+002.89e-015.57e-012.25e+00\"MQ\"FalseFalse[\"0|0|0|0|0|0|1|0|0|0|0|0|0|0|0|0|0|0|0|2\"][\"0|1|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0\"][\"0|0|0|0|0|0|0|1|1|0|0|0|0|0|1|0|0|0|0|0\"]\"4597|6767|2443|3466|1994|601|758|390|126|150|87|21|20|5|8|4|3|0|0|2\"\"13424|6201|1423|326|52|15|1|0|0|0|0|0|0|0|0|0|0|0|0|0\"\"0|0|0|0|0|0|0|1|1|0|0|0|0|0|1|0|0|0|0|0\"[0][0][0][1][0][0][0][1][0][2]5306063443434692106111420501500[0.00e+00][0.00e+00][0.00e+00][2.30e-03][0.00e+00][0.00e+00][0.00e+00][8.98e-04][0.00e+00][1.33e-03][265,0,0][303,0,0][17,0,0][216,1,0][17,0,0][346,0,0][53,0,0][556,1,0][1025,0,0][748,2,0][3]42884[7.00e-05][21439,3,0][1773,2,0][0][0][0][0][0][0][0][0][0][0][0][0]NANANA[\"EAS\"][1][434][2.30e-03][13][5.01e-12][98][4.21e-01][3.83e-01][\"PASS\"]NANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANA\nchr1:12596[\"C\",\"A\"]\"chr1_12596_C_A\"4.48e+01{\"AC0\"}[0][0.00e+00]29322.22e+003.61e-01False1402420.00e+00-8.29e-022.20e+010.00e+004.97e+003.61e-018.92e-012.41e+00\"MQ\"FalseFalse[\"0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|1|0|0|0\"][\"0|1|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0\"][\"0|0|0|0|0|0|0|0|0|0|0|1|0|0|0|0|0|0|0|0\"]\"4727|7110|2504|3416|1817|550|609|333|79|110|46|6|19|4|0|3|1|0|0|0\"\"13901|5942|1199|251|36|5|0|0|0|0|0|0|0|0|0|0|0|0|0|0\"\"0|0|0|0|0|0|0|0|0|0|0|1|0|0|0|0|0|0|0|0\"[0][0][0][0][0][0][0][0][0][0]49043622360226027892216701262[0.00e+00][0.00e+00][0.00e+00][0.00e+00][0.00e+00][0.00e+00][0.00e+00][0.00e+00][0.00e+00][0.00e+00][245,0,0][218,0,0][11,0,0][180,0,0][11,0,0][301,0,0][39,0,0][461,0,0][835,0,0][631,0,0][1]42668[2.34e-05][21333,1,0][1466,0,0][0][0][0][0][0][0][0][0][0][0][0][0]NANANANANANANA[9][2.00e-11][80][5.56e-01][4.03e-01][\"AC0\"]NANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANA\nshowing top 10 rows\n\n\nIf we do a .describe(), we will see more info about the row fields.\nThere is only one column field, s, which is the key that maps to the Sample ID. We will add more column fields when we merge in the Pheno Data in the next section.\n\n# View structure of MT before adding pheno data\n\nmt.describe()\n\n----------------------------------------\nGlobal fields:\n    None\n----------------------------------------\nColumn fields:\n    's': str\n----------------------------------------\nRow fields:\n    'locus': locus<GRCh38>\n    'alleles': array<str>\n    'rsid': str\n    'qual': float64\n    'filters': set<str>\n    'info': struct {\n        AC: array<int32>, \n        AF: array<float64>, \n        AN: int32, \n        BaseQRankSum: float64, \n        ClippingRankSum: float64, \n        DB: bool, \n        DP: int32, \n        FS: float64, \n        InbreedingCoeff: float64, \n        MQ: float64, \n        MQRankSum: float64, \n        QD: float64, \n        ReadPosRankSum: float64, \n        SOR: float64, \n        VQSLOD: float64, \n        VQSR_culprit: str, \n        VQSR_NEGATIVE_TRAIN_SITE: bool, \n        VQSR_POSITIVE_TRAIN_SITE: bool, \n        GQ_HIST_ALT: array<str>, \n        DP_HIST_ALT: array<str>, \n        AB_HIST_ALT: array<str>, \n        GQ_HIST_ALL: str, \n        DP_HIST_ALL: str, \n        AB_HIST_ALL: str, \n        AC_AFR: array<int32>, \n        AC_AMR: array<int32>, \n        AC_ASJ: array<int32>, \n        AC_EAS: array<int32>, \n        AC_FIN: array<int32>, \n        AC_NFE: array<int32>, \n        AC_OTH: array<int32>, \n        AC_SAS: array<int32>, \n        AC_Male: array<int32>, \n        AC_Female: array<int32>, \n        AN_AFR: int32, \n        AN_AMR: int32, \n        AN_ASJ: int32, \n        AN_EAS: int32, \n        AN_FIN: int32, \n        AN_NFE: int32, \n        AN_OTH: int32, \n        AN_SAS: int32, \n        AN_Male: int32, \n        AN_Female: int32, \n        AF_AFR: array<float64>, \n        AF_AMR: array<float64>, \n        AF_ASJ: array<float64>, \n        AF_EAS: array<float64>, \n        AF_FIN: array<float64>, \n        AF_NFE: array<float64>, \n        AF_OTH: array<float64>, \n        AF_SAS: array<float64>, \n        AF_Male: array<float64>, \n        AF_Female: array<float64>, \n        GC_AFR: array<int32>, \n        GC_AMR: array<int32>, \n        GC_ASJ: array<int32>, \n        GC_EAS: array<int32>, \n        GC_FIN: array<int32>, \n        GC_NFE: array<int32>, \n        GC_OTH: array<int32>, \n        GC_SAS: array<int32>, \n        GC_Male: array<int32>, \n        GC_Female: array<int32>, \n        AC_raw: array<int32>, \n        AN_raw: int32, \n        AF_raw: array<float64>, \n        GC_raw: array<int32>, \n        GC: array<int32>, \n        Hom_AFR: array<int32>, \n        Hom_AMR: array<int32>, \n        Hom_ASJ: array<int32>, \n        Hom_EAS: array<int32>, \n        Hom_FIN: array<int32>, \n        Hom_NFE: array<int32>, \n        Hom_OTH: array<int32>, \n        Hom_SAS: array<int32>, \n        Hom_Male: array<int32>, \n        Hom_Female: array<int32>, \n        Hom_raw: array<int32>, \n        Hom: array<int32>, \n        STAR_AC: int32, \n        STAR_AC_raw: int32, \n        STAR_Hom: int32, \n        POPMAX: array<str>, \n        AC_POPMAX: array<int32>, \n        AN_POPMAX: array<int32>, \n        AF_POPMAX: array<float64>, \n        DP_MEDIAN: array<int32>, \n        DREF_MEDIAN: array<float64>, \n        GQ_MEDIAN: array<int32>, \n        AB_MEDIAN: array<float64>, \n        AS_RF: array<float64>, \n        AS_FilterStatus: array<str>, \n        AS_RF_POSITIVE_TRAIN: array<int32>, \n        AS_RF_NEGATIVE_TRAIN: array<int32>, \n        AC_AFR_Male: array<int32>, \n        AC_AMR_Male: array<int32>, \n        AC_ASJ_Male: array<int32>, \n        AC_EAS_Male: array<int32>, \n        AC_FIN_Male: array<int32>, \n        AC_NFE_Male: array<int32>, \n        AC_OTH_Male: array<int32>, \n        AC_SAS_Male: array<int32>, \n        AC_AFR_Female: array<int32>, \n        AC_AMR_Female: array<int32>, \n        AC_ASJ_Female: array<int32>, \n        AC_EAS_Female: array<int32>, \n        AC_FIN_Female: array<int32>, \n        AC_NFE_Female: array<int32>, \n        AC_OTH_Female: array<int32>, \n        AC_SAS_Female: array<int32>, \n        AN_AFR_Male: int32, \n        AN_AMR_Male: int32, \n        AN_ASJ_Male: int32, \n        AN_EAS_Male: int32, \n        AN_FIN_Male: int32, \n        AN_NFE_Male: int32, \n        AN_OTH_Male: int32, \n        AN_SAS_Male: int32, \n        AN_AFR_Female: int32, \n        AN_AMR_Female: int32, \n        AN_ASJ_Female: int32, \n        AN_EAS_Female: int32, \n        AN_FIN_Female: int32, \n        AN_NFE_Female: int32, \n        AN_OTH_Female: int32, \n        AN_SAS_Female: int32, \n        AF_AFR_Male: array<float64>, \n        AF_AMR_Male: array<float64>, \n        AF_ASJ_Male: array<float64>, \n        AF_EAS_Male: array<float64>, \n        AF_FIN_Male: array<float64>, \n        AF_NFE_Male: array<float64>, \n        AF_OTH_Male: array<float64>, \n        AF_SAS_Male: array<float64>, \n        AF_AFR_Female: array<float64>, \n        AF_AMR_Female: array<float64>, \n        AF_ASJ_Female: array<float64>, \n        AF_EAS_Female: array<float64>, \n        AF_FIN_Female: array<float64>, \n        AF_NFE_Female: array<float64>, \n        AF_OTH_Female: array<float64>, \n        AF_SAS_Female: array<float64>, \n        GC_AFR_Male: array<int32>, \n        GC_AMR_Male: array<int32>, \n        GC_ASJ_Male: array<int32>, \n        GC_EAS_Male: array<int32>, \n        GC_FIN_Male: array<int32>, \n        GC_NFE_Male: array<int32>, \n        GC_OTH_Male: array<int32>, \n        GC_SAS_Male: array<int32>, \n        GC_AFR_Female: array<int32>, \n        GC_AMR_Female: array<int32>, \n        GC_ASJ_Female: array<int32>, \n        GC_EAS_Female: array<int32>, \n        GC_FIN_Female: array<int32>, \n        GC_NFE_Female: array<int32>, \n        GC_OTH_Female: array<int32>, \n        GC_SAS_Female: array<int32>, \n        Hemi_AFR: array<int32>, \n        Hemi_AMR: array<int32>, \n        Hemi_ASJ: array<int32>, \n        Hemi_EAS: array<int32>, \n        Hemi_FIN: array<int32>, \n        Hemi_NFE: array<int32>, \n        Hemi_OTH: array<int32>, \n        Hemi_SAS: array<int32>, \n        Hemi: array<int32>, \n        Hemi_raw: array<int32>, \n        STAR_Hemi: int32\n    }\n----------------------------------------\nEntry fields:\n    'GT': call\n    'AD': array<int32>\n    'DP': int32\n    'GQ': int32\n    'PL': array<int32>\n----------------------------------------\nColumn key: ['s']\nRow key: ['locus', 'alleles']\n----------------------------------------\n\n\nWe can look at the first few rows and columns of the genotyping table with mt.GT.show(). Note that it includes the locus/alleles column. This is because the row key corresponds to a combination of both locus and alleles.\n\nmt.GT.show()\n\n\n'sample_1_0''sample_1_1''sample_1_2''sample_1_3'locusallelesGTGTGTGTlocus<GRCh38>array<str>callcallcallcall\nchr1:12198[\"G\",\"C\"]0/10/01/10/0\nchr1:12237[\"G\",\"A\"]1/10/00/10/0\nchr1:12259[\"G\",\"C\"]1/10/00/10/0\nchr1:12266[\"G\",\"A\"]0/11/10/10/0\nchr1:12272[\"G\",\"A\"]0/00/10/00/1\nchr1:12554[\"A\",\"G\"]0/00/00/00/0\nchr1:12559[\"G\",\"A\"]0/00/00/00/0\nchr1:12573[\"T\",\"C\"]0/00/00/00/0\nchr1:12586[\"C\",\"T\"]0/00/00/00/0\nchr1:12596[\"C\",\"A\"]0/00/00/00/0\nshowing top 10 rows\nshowing the first 4 of 100000 columns"
  },
  {
    "objectID": "02-gwas.html#sec-load_pheno",
    "href": "02-gwas.html#sec-load_pheno",
    "title": "6  GWAS using Hail",
    "section": "6.5 Load and Merge Pheno Table",
    "text": "6.5 Load and Merge Pheno Table\nPhenotypic data may come from an array of sources, such as a cohort from the Cohort Browser or a separate, stand-alone text file. In this notebook, we use phenotypic data from a CSV file, which was previously uploaded to a project. In this (very basic) example we use the phenotypic trait, is_case, for each sample. The values indicate if the sample is a case, is_case=true, or a control, is_case=false\nNote: Although this notebook provides an example using a stand-alone CSV file, another source of phenotypic data could be derived from an Apollo Dataset. Please refer to Table Exporter documentation (https://documentation.dnanexus.com/developer/apps/developing-spark-apps/table-exporter-application) or OpenBio notebooks (https://github.com/dnanexus/OpenBio/tree/master/dx-toolkit) with extract_dataset in the title for guidance on how to extract data from an Apollo Dataset.\nAll data uploaded to the project before running the JupyterLab app is mounted (https://documentation.dnanexus.com/user/jupyter-notebooks?#accessing-data) and can be accessed in /mnt/project/<path_to_data>. The file URL follows the format: file:///mnt/project/<path_to_data>\n\n# Import the pheno CSV file as a Hail Table\n\npheno_table = hl.import_table(\"file:///mnt/project/data/ukbb_100k_bmi_casecontrol.csv\",\n                              delimiter=',',\n                              impute=True,\n                              key='iid') # specify the column that will be the key (values must match what is in the MT 's' column)\n\npheno_table.show()\n\n2023-04-19 22:30:24 Hail: INFO: Reading table to impute column types\n2023-04-19 22:30:29 Hail: INFO: Finished type imputation\n  Loading field 'fid' as type str (imputed)\n  Loading field 'iid' as type str (imputed)\n  Loading field 'father_iid' as type int32 (imputed)\n  Loading field 'mother_iid' as type int32 (imputed)\n  Loading field 'sex_code' as type int32 (imputed)\n  Loading field 'case_control_status' as type int32 (imputed)\n\n\n\nfidiidfather_iidmother_iidsex_codecase_control_statusstrstrint32int32int32int32\n\"sample_1_10\"\"sample_1_10\"0012\n\"sample_1_10003\"\"sample_1_10003\"0022\n\"sample_1_10014\"\"sample_1_10014\"0021\n\"sample_1_10020\"\"sample_1_10020\"0012\n\"sample_1_10021\"\"sample_1_10021\"0021\n\"sample_1_10023\"\"sample_1_10023\"0011\n\"sample_1_10036\"\"sample_1_10036\"0021\n\"sample_1_10037\"\"sample_1_10037\"0011\n\"sample_1_10047\"\"sample_1_10047\"0011\n\"sample_1_10056\"\"sample_1_10056\"0012\nshowing top 10 rows\n\n\nOne thing I always do when I’m loading in a Hail Table is use .summarize() on the entire Table. This gives me a high level overview of the Table.\n\npheno_table.summarize()\n\n2023-04-19 18:58:14 Hail: INFO: Coerced sorted dataset\n\n\n\n19884 records.fid (str):Non-missing19884 (100.00%)Missing0Min Size10Max Size14Mean Size13.89Sample Values['sample_1_10', 'sample_1_10003', 'sample_1_10014', 'sample_1_10020', 'sample_1_10021']iid (str):Non-missing19884 (100.00%)Missing0Min Size10Max Size14Mean Size13.89Sample Values['sample_1_10', 'sample_1_10003', 'sample_1_10014', 'sample_1_10020', 'sample_1_10021']father_iid (int32):Non-missing19884 (100.00%)Missing0Minimum0Maximum0Mean0.00Std Dev0.00mother_iid (int32):Non-missing19884 (100.00%)Missing0Minimum0Maximum0Mean0.00Std Dev0.00sex_code (int32):Non-missing19884 (100.00%)Missing0Minimum1Maximum2Mean1.54Std Dev0.50case_control_status (int32):Non-missing19884 (100.00%)Missing0Minimum1Maximum2Mean1.50Std Dev0.50\n\n\nWe need to modify case_control_status in our table, because it needs to be 0/1 instead of 1/2 to work with Hail’s .logistic_regression() method.\nWe can do this with .annotate(), which will add a column to our table. We will subtract 1 from both case_control_status and sex_code.\n\npheno_fixed = pheno_table.annotate(ccs = pheno_table.case_control_status - 1, # <1>\n                                  sc = pheno_table.sex_code - 1) # <2>\npheno_fixed.show()\n\n2023-04-19 22:30:36 Hail: INFO: Coerced sorted dataset\n\n\n\nfidiidfather_iidmother_iidsex_codecase_control_statusccsscstrstrint32int32int32int32int32int32\n\"sample_1_10\"\"sample_1_10\"001210\n\"sample_1_10003\"\"sample_1_10003\"002211\n\"sample_1_10014\"\"sample_1_10014\"002101\n\"sample_1_10020\"\"sample_1_10020\"001210\n\"sample_1_10021\"\"sample_1_10021\"002101\n\"sample_1_10023\"\"sample_1_10023\"001100\n\"sample_1_10036\"\"sample_1_10036\"002101\n\"sample_1_10037\"\"sample_1_10037\"001100\n\"sample_1_10047\"\"sample_1_10047\"001100\n\"sample_1_10056\"\"sample_1_10056\"001210\nshowing top 10 rows\n\n\n\nSubtract 1 from the Case Control Status\nSubtract 2 from the Sex Code\n\nIf we .describe(), we’ll see that we added two row fields: ccs and sc.\n\n# View structure of pheno Table\n\npheno_fixed.describe()\n\n----------------------------------------\nGlobal fields:\n    None\n----------------------------------------\nRow fields:\n    'fid': str \n    'iid': str \n    'father_iid': int32 \n    'mother_iid': int32 \n    'sex_code': int32 \n    'case_control_status': int32 \n    'ccs': int32 \n    'sc': int32 \n----------------------------------------\nKey: ['iid']\n----------------------------------------\n\n\n\npheno_fixed.show(10)\n\n2023-04-11 18:45:20 Hail: INFO: Coerced sorted dataset\n\n\n\nfidiidfather_iidmother_iidsex_codecase_control_statusccsscstrstrint32int32int32int32int32int32\n\"sample_1_10\"\"sample_1_10\"001210\n\"sample_1_10003\"\"sample_1_10003\"002211\n\"sample_1_10014\"\"sample_1_10014\"002101\n\"sample_1_10020\"\"sample_1_10020\"001210\n\"sample_1_10021\"\"sample_1_10021\"002101\n\"sample_1_10023\"\"sample_1_10023\"001100\n\"sample_1_10036\"\"sample_1_10036\"002101\n\"sample_1_10037\"\"sample_1_10037\"001100\n\"sample_1_10047\"\"sample_1_10047\"001100\n\"sample_1_10056\"\"sample_1_10056\"001210\nshowing top 10 rows\n\n\n\n6.5.1 Annotate MT with pheno Table\n\n# Annotate the MT with pheno Table by matching the MT's column key ('s') with the pheno Table's key ('sample_id')\n\nphenogeno_mt = mt.annotate_cols(**pheno_fixed[mt.s])\n\nIf we look at what happened in the merge, we will see that there are missing values in the pheno data. That’s because the pheno file only had 20000 rows. This means we’ll drop a large amount of the population when we actually run the GWAS, because they have no case/control status nor do they have covariates.\n\nphenogeno_mt.col.show(20)\n\n\nsfidfather_iidmother_iidsex_codecase_control_statusccsscstrstrint32int32int32int32int32int32\n\"sample_1_0\"NANANANANANANA\n\"sample_1_1\"NANANANANANANA\n\"sample_1_2\"NANANANANANANA\n\"sample_1_3\"\"sample_1_3\"002101\n\"sample_1_4\"NANANANANANANA\n\"sample_1_5\"NANANANANANANA\n\"sample_1_6\"NANANANANANANA\n\"sample_1_7\"NANANANANANANA\n\"sample_1_8\"NANANANANANANA\n\"sample_1_9\"NANANANANANANA\n\"sample_1_10\"\"sample_1_10\"001210\n\"sample_1_11\"NANANANANANANA\n\"sample_1_12\"NANANANANANANA\n\"sample_1_13\"NANANANANANANA\n\"sample_1_14\"NANANANANANANA\n\"sample_1_15\"NANANANANANANA\n\"sample_1_16\"NANANANANANANA\n\"sample_1_17\"\"sample_1_17\"002101\n\"sample_1_18\"NANANANANANANA\n\"sample_1_19\"NANANANANANANA\nshowing top 20 rows\n\n\nAfter merging in the pheno table, we see that we’ve added a number of fields to Column Fields. This includes fid, sex_code, case_control_status and the columns we derived from them.\n\n# View structure of MT after annotating with pheno Table\n\nphenogeno_mt.describe()\n\n----------------------------------------\nGlobal fields:\n    None\n----------------------------------------\nColumn fields:\n    's': str\n    'fid': str\n    'father_iid': int32\n    'mother_iid': int32\n    'sex_code': int32\n    'case_control_status': int32\n    'ccs': int32\n    'sc': int32\n----------------------------------------\nRow fields:\n    'locus': locus<GRCh38>\n    'alleles': array<str>\n    'rsid': str\n    'qual': float64\n    'filters': set<str>\n    'info': struct {\n        AC: array<int32>, \n        AF: array<float64>, \n        AN: int32, \n        BaseQRankSum: float64, \n        ClippingRankSum: float64, \n        DB: bool, \n        DP: int32, \n        FS: float64, \n        InbreedingCoeff: float64, \n        MQ: float64, \n        MQRankSum: float64, \n        QD: float64, \n        ReadPosRankSum: float64, \n        SOR: float64, \n        VQSLOD: float64, \n        VQSR_culprit: str, \n        VQSR_NEGATIVE_TRAIN_SITE: bool, \n        VQSR_POSITIVE_TRAIN_SITE: bool, \n        GQ_HIST_ALT: array<str>, \n        DP_HIST_ALT: array<str>, \n        AB_HIST_ALT: array<str>, \n        GQ_HIST_ALL: str, \n        DP_HIST_ALL: str, \n        AB_HIST_ALL: str, \n        AC_AFR: array<int32>, \n        AC_AMR: array<int32>, \n        AC_ASJ: array<int32>, \n        AC_EAS: array<int32>, \n        AC_FIN: array<int32>, \n        AC_NFE: array<int32>, \n        AC_OTH: array<int32>, \n        AC_SAS: array<int32>, \n        AC_Male: array<int32>, \n        AC_Female: array<int32>, \n        AN_AFR: int32, \n        AN_AMR: int32, \n        AN_ASJ: int32, \n        AN_EAS: int32, \n        AN_FIN: int32, \n        AN_NFE: int32, \n        AN_OTH: int32, \n        AN_SAS: int32, \n        AN_Male: int32, \n        AN_Female: int32, \n        AF_AFR: array<float64>, \n        AF_AMR: array<float64>, \n        AF_ASJ: array<float64>, \n        AF_EAS: array<float64>, \n        AF_FIN: array<float64>, \n        AF_NFE: array<float64>, \n        AF_OTH: array<float64>, \n        AF_SAS: array<float64>, \n        AF_Male: array<float64>, \n        AF_Female: array<float64>, \n        GC_AFR: array<int32>, \n        GC_AMR: array<int32>, \n        GC_ASJ: array<int32>, \n        GC_EAS: array<int32>, \n        GC_FIN: array<int32>, \n        GC_NFE: array<int32>, \n        GC_OTH: array<int32>, \n        GC_SAS: array<int32>, \n        GC_Male: array<int32>, \n        GC_Female: array<int32>, \n        AC_raw: array<int32>, \n        AN_raw: int32, \n        AF_raw: array<float64>, \n        GC_raw: array<int32>, \n        GC: array<int32>, \n        Hom_AFR: array<int32>, \n        Hom_AMR: array<int32>, \n        Hom_ASJ: array<int32>, \n        Hom_EAS: array<int32>, \n        Hom_FIN: array<int32>, \n        Hom_NFE: array<int32>, \n        Hom_OTH: array<int32>, \n        Hom_SAS: array<int32>, \n        Hom_Male: array<int32>, \n        Hom_Female: array<int32>, \n        Hom_raw: array<int32>, \n        Hom: array<int32>, \n        STAR_AC: int32, \n        STAR_AC_raw: int32, \n        STAR_Hom: int32, \n        POPMAX: array<str>, \n        AC_POPMAX: array<int32>, \n        AN_POPMAX: array<int32>, \n        AF_POPMAX: array<float64>, \n        DP_MEDIAN: array<int32>, \n        DREF_MEDIAN: array<float64>, \n        GQ_MEDIAN: array<int32>, \n        AB_MEDIAN: array<float64>, \n        AS_RF: array<float64>, \n        AS_FilterStatus: array<str>, \n        AS_RF_POSITIVE_TRAIN: array<int32>, \n        AS_RF_NEGATIVE_TRAIN: array<int32>, \n        AC_AFR_Male: array<int32>, \n        AC_AMR_Male: array<int32>, \n        AC_ASJ_Male: array<int32>, \n        AC_EAS_Male: array<int32>, \n        AC_FIN_Male: array<int32>, \n        AC_NFE_Male: array<int32>, \n        AC_OTH_Male: array<int32>, \n        AC_SAS_Male: array<int32>, \n        AC_AFR_Female: array<int32>, \n        AC_AMR_Female: array<int32>, \n        AC_ASJ_Female: array<int32>, \n        AC_EAS_Female: array<int32>, \n        AC_FIN_Female: array<int32>, \n        AC_NFE_Female: array<int32>, \n        AC_OTH_Female: array<int32>, \n        AC_SAS_Female: array<int32>, \n        AN_AFR_Male: int32, \n        AN_AMR_Male: int32, \n        AN_ASJ_Male: int32, \n        AN_EAS_Male: int32, \n        AN_FIN_Male: int32, \n        AN_NFE_Male: int32, \n        AN_OTH_Male: int32, \n        AN_SAS_Male: int32, \n        AN_AFR_Female: int32, \n        AN_AMR_Female: int32, \n        AN_ASJ_Female: int32, \n        AN_EAS_Female: int32, \n        AN_FIN_Female: int32, \n        AN_NFE_Female: int32, \n        AN_OTH_Female: int32, \n        AN_SAS_Female: int32, \n        AF_AFR_Male: array<float64>, \n        AF_AMR_Male: array<float64>, \n        AF_ASJ_Male: array<float64>, \n        AF_EAS_Male: array<float64>, \n        AF_FIN_Male: array<float64>, \n        AF_NFE_Male: array<float64>, \n        AF_OTH_Male: array<float64>, \n        AF_SAS_Male: array<float64>, \n        AF_AFR_Female: array<float64>, \n        AF_AMR_Female: array<float64>, \n        AF_ASJ_Female: array<float64>, \n        AF_EAS_Female: array<float64>, \n        AF_FIN_Female: array<float64>, \n        AF_NFE_Female: array<float64>, \n        AF_OTH_Female: array<float64>, \n        AF_SAS_Female: array<float64>, \n        GC_AFR_Male: array<int32>, \n        GC_AMR_Male: array<int32>, \n        GC_ASJ_Male: array<int32>, \n        GC_EAS_Male: array<int32>, \n        GC_FIN_Male: array<int32>, \n        GC_NFE_Male: array<int32>, \n        GC_OTH_Male: array<int32>, \n        GC_SAS_Male: array<int32>, \n        GC_AFR_Female: array<int32>, \n        GC_AMR_Female: array<int32>, \n        GC_ASJ_Female: array<int32>, \n        GC_EAS_Female: array<int32>, \n        GC_FIN_Female: array<int32>, \n        GC_NFE_Female: array<int32>, \n        GC_OTH_Female: array<int32>, \n        GC_SAS_Female: array<int32>, \n        Hemi_AFR: array<int32>, \n        Hemi_AMR: array<int32>, \n        Hemi_ASJ: array<int32>, \n        Hemi_EAS: array<int32>, \n        Hemi_FIN: array<int32>, \n        Hemi_NFE: array<int32>, \n        Hemi_OTH: array<int32>, \n        Hemi_SAS: array<int32>, \n        Hemi: array<int32>, \n        Hemi_raw: array<int32>, \n        STAR_Hemi: int32\n    }\n----------------------------------------\nEntry fields:\n    'GT': call\n    'AD': array<int32>\n    'DP': int32\n    'GQ': int32\n    'PL': array<int32>\n----------------------------------------\nColumn key: ['s']\nRow key: ['locus', 'alleles']\n----------------------------------------\n\n\nWe see that the pheno traits have been added in the column fields of the MT"
  },
  {
    "objectID": "02-gwas.html#sec-sample_table",
    "href": "02-gwas.html#sec-sample_table",
    "title": "6  GWAS using Hail",
    "section": "6.6 Filter MT using Sample QC Tables",
    "text": "6.6 Filter MT using Sample QC Tables\n\n\n\nimage.png\n\n\n\n6.6.1 Filter sample QC Table\nThe above figure illustrates the basic process for filtering on Sample QC metrics. We first need to build the sample_qc table called pre_sample_qc_tb. Then we will extract the row fields using the .row() accessor and write it to DNAX using pre_sample_qc_table.write(). (see https://github.com/dnanexus/OpenBio/blob/master/hail_tutorial/sample_qc.ipynb for more info)\nThis table was built using:\npre_sample_qc_tb = hl.sample_qc(mt).cols()\n\n# Define sample QC Table url\n\nsample_qc_url = \"dnax://database-GQYV2Bj04bPg9X3KfFyj2jyY/sample_qc.ht\"\n\n\n# Read sample QC Table\n\npre_sample_qc_tb = hl.read_table(sample_qc_url)\n\n\n# View structure of sample QC Table\n\npre_sample_qc_tb.describe()\n\n----------------------------------------\nGlobal fields:\n    None\n----------------------------------------\nRow fields:\n    's': str \n    'sample_qc': struct {\n        dp_stats: struct {\n            mean: float64, \n            stdev: float64, \n            min: float64, \n            max: float64\n        }, \n        gq_stats: struct {\n            mean: float64, \n            stdev: float64, \n            min: float64, \n            max: float64\n        }, \n        call_rate: float64, \n        n_called: int64, \n        n_not_called: int64, \n        n_filtered: int64, \n        n_hom_ref: int64, \n        n_het: int64, \n        n_hom_var: int64, \n        n_non_ref: int64, \n        n_singleton: int64, \n        n_snp: int64, \n        n_insertion: int64, \n        n_deletion: int64, \n        n_transition: int64, \n        n_transversion: int64, \n        n_star: int64, \n        r_ti_tv: float64, \n        r_het_hom_var: float64, \n        r_insertion_deletion: float64\n    } \n----------------------------------------\nKey: ['s']\n----------------------------------------\n\n\nLet’s plot the call rate across the samples. This will help us decide what our cutoff should be.\n\nfrom bokeh.io import output_notebook, show\noutput_notebook()\n\np = hl.plot.histogram(pre_sample_qc_tb[\"sample_qc\"][\"call_rate\"])\nshow(p)\n\n\n    \n        \n        Loading BokehJS ...\n    \n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nLet’s filter for samples that have a call rate greater or equal to 0.99\n\n# Filter sample QC Table using expressions\n# Note: Viewing the structure of the sample QC table in from the cell above \n# shows us that the \"call_rate\" field is within the \"sample_qc\" struct field\n\nsample_qc_tb = pre_sample_qc_tb.filter(\n    pre_sample_qc_tb[\"sample_qc\"][\"call_rate\"] >= 0.99) \n\nIf we do a count before and after filtering, we’ll see we dropped roughly 50000 participants using this filtering step.\n\n# View number of samples in QC Table before and after filtering\n#\n# Note: running this cell can be computationally expensive and take\n# longer for bigger datasets (this cell can be commented out).\n\nprint(f\"Num samples before filtering: {pre_sample_qc_tb.count()}\")\nprint(f\"Num samples after filtering: {sample_qc_tb.count()}\")\n\nNum samples before filtering: 100000\nNum samples after filtering: 50062\n\n\n\n\n6.6.2 Filter MT with Sample QC Table\nNow we can use .semi_join_cols() to filter our phenogeno_mt with sample_qc_tb.\n\n# Filter the MT using the sample QC Table\n\nqc_mt = phenogeno_mt.semi_join_cols(sample_qc_tb)"
  },
  {
    "objectID": "02-gwas.html#sec-variant_table",
    "href": "02-gwas.html#sec-variant_table",
    "title": "6  GWAS using Hail",
    "section": "6.7 Build Variant QC table from MT, use to filter MT",
    "text": "6.7 Build Variant QC table from MT, use to filter MT\nSimilar to the Sample QC, we can calculate a variant QC table for our MatrixTable, and extract it as a separate table and store it in DNAX. Then we can filter on this table, and use .semi_join_rows() to filter our MatrixTable.\n\n6.7.1 4a) Extract Locus QC table\n\n\n\nimage.png\n\n\nThis table was built using\npre_locus_qc_tb = hl.variant_qc(mt).rows() \n(see https://github.com/dnanexus/OpenBio/blob/master/hail_tutorial/locus_qc.ipynb for more info)\n\n# Define locus QC Table url\n\nlocus_qc_url = \"dnax://database-GQYV2Bj04bPg9X3KfFyj2jyY/variant_qc.ht\"\n\n\n# Read locus QC Table\n\npre_locus_qc_tb = hl.read_table(locus_qc_url)\n\n\n# View structure of locus QC Table\n\npre_locus_qc_tb.describe()\n\n----------------------------------------\nGlobal fields:\n    None\n----------------------------------------\nRow fields:\n    'locus': locus<GRCh38> \n    'alleles': array<str> \n    'rsid': str \n    'qual': float64 \n    'filters': set<str> \n    'info': struct {\n        AC: array<int32>, \n        AF: array<float64>, \n        AN: int32, \n        BaseQRankSum: float64, \n        ClippingRankSum: float64, \n        DB: bool, \n        DP: int32, \n        FS: float64, \n        InbreedingCoeff: float64, \n        MQ: float64, \n        MQRankSum: float64, \n        QD: float64, \n        ReadPosRankSum: float64, \n        SOR: float64, \n        VQSLOD: float64, \n        VQSR_culprit: str, \n        VQSR_NEGATIVE_TRAIN_SITE: bool, \n        VQSR_POSITIVE_TRAIN_SITE: bool, \n        GQ_HIST_ALT: array<str>, \n        DP_HIST_ALT: array<str>, \n        AB_HIST_ALT: array<str>, \n        GQ_HIST_ALL: str, \n        DP_HIST_ALL: str, \n        AB_HIST_ALL: str, \n        AC_AFR: array<int32>, \n        AC_AMR: array<int32>, \n        AC_ASJ: array<int32>, \n        AC_EAS: array<int32>, \n        AC_FIN: array<int32>, \n        AC_NFE: array<int32>, \n        AC_OTH: array<int32>, \n        AC_SAS: array<int32>, \n        AC_Male: array<int32>, \n        AC_Female: array<int32>, \n        AN_AFR: int32, \n        AN_AMR: int32, \n        AN_ASJ: int32, \n        AN_EAS: int32, \n        AN_FIN: int32, \n        AN_NFE: int32, \n        AN_OTH: int32, \n        AN_SAS: int32, \n        AN_Male: int32, \n        AN_Female: int32, \n        AF_AFR: array<float64>, \n        AF_AMR: array<float64>, \n        AF_ASJ: array<float64>, \n        AF_EAS: array<float64>, \n        AF_FIN: array<float64>, \n        AF_NFE: array<float64>, \n        AF_OTH: array<float64>, \n        AF_SAS: array<float64>, \n        AF_Male: array<float64>, \n        AF_Female: array<float64>, \n        GC_AFR: array<int32>, \n        GC_AMR: array<int32>, \n        GC_ASJ: array<int32>, \n        GC_EAS: array<int32>, \n        GC_FIN: array<int32>, \n        GC_NFE: array<int32>, \n        GC_OTH: array<int32>, \n        GC_SAS: array<int32>, \n        GC_Male: array<int32>, \n        GC_Female: array<int32>, \n        AC_raw: array<int32>, \n        AN_raw: int32, \n        AF_raw: array<float64>, \n        GC_raw: array<int32>, \n        GC: array<int32>, \n        Hom_AFR: array<int32>, \n        Hom_AMR: array<int32>, \n        Hom_ASJ: array<int32>, \n        Hom_EAS: array<int32>, \n        Hom_FIN: array<int32>, \n        Hom_NFE: array<int32>, \n        Hom_OTH: array<int32>, \n        Hom_SAS: array<int32>, \n        Hom_Male: array<int32>, \n        Hom_Female: array<int32>, \n        Hom_raw: array<int32>, \n        Hom: array<int32>, \n        STAR_AC: int32, \n        STAR_AC_raw: int32, \n        STAR_Hom: int32, \n        POPMAX: array<str>, \n        AC_POPMAX: array<int32>, \n        AN_POPMAX: array<int32>, \n        AF_POPMAX: array<float64>, \n        DP_MEDIAN: array<int32>, \n        DREF_MEDIAN: array<float64>, \n        GQ_MEDIAN: array<int32>, \n        AB_MEDIAN: array<float64>, \n        AS_RF: array<float64>, \n        AS_FilterStatus: array<str>, \n        AS_RF_POSITIVE_TRAIN: array<int32>, \n        AS_RF_NEGATIVE_TRAIN: array<int32>, \n        AC_AFR_Male: array<int32>, \n        AC_AMR_Male: array<int32>, \n        AC_ASJ_Male: array<int32>, \n        AC_EAS_Male: array<int32>, \n        AC_FIN_Male: array<int32>, \n        AC_NFE_Male: array<int32>, \n        AC_OTH_Male: array<int32>, \n        AC_SAS_Male: array<int32>, \n        AC_AFR_Female: array<int32>, \n        AC_AMR_Female: array<int32>, \n        AC_ASJ_Female: array<int32>, \n        AC_EAS_Female: array<int32>, \n        AC_FIN_Female: array<int32>, \n        AC_NFE_Female: array<int32>, \n        AC_OTH_Female: array<int32>, \n        AC_SAS_Female: array<int32>, \n        AN_AFR_Male: int32, \n        AN_AMR_Male: int32, \n        AN_ASJ_Male: int32, \n        AN_EAS_Male: int32, \n        AN_FIN_Male: int32, \n        AN_NFE_Male: int32, \n        AN_OTH_Male: int32, \n        AN_SAS_Male: int32, \n        AN_AFR_Female: int32, \n        AN_AMR_Female: int32, \n        AN_ASJ_Female: int32, \n        AN_EAS_Female: int32, \n        AN_FIN_Female: int32, \n        AN_NFE_Female: int32, \n        AN_OTH_Female: int32, \n        AN_SAS_Female: int32, \n        AF_AFR_Male: array<float64>, \n        AF_AMR_Male: array<float64>, \n        AF_ASJ_Male: array<float64>, \n        AF_EAS_Male: array<float64>, \n        AF_FIN_Male: array<float64>, \n        AF_NFE_Male: array<float64>, \n        AF_OTH_Male: array<float64>, \n        AF_SAS_Male: array<float64>, \n        AF_AFR_Female: array<float64>, \n        AF_AMR_Female: array<float64>, \n        AF_ASJ_Female: array<float64>, \n        AF_EAS_Female: array<float64>, \n        AF_FIN_Female: array<float64>, \n        AF_NFE_Female: array<float64>, \n        AF_OTH_Female: array<float64>, \n        AF_SAS_Female: array<float64>, \n        GC_AFR_Male: array<int32>, \n        GC_AMR_Male: array<int32>, \n        GC_ASJ_Male: array<int32>, \n        GC_EAS_Male: array<int32>, \n        GC_FIN_Male: array<int32>, \n        GC_NFE_Male: array<int32>, \n        GC_OTH_Male: array<int32>, \n        GC_SAS_Male: array<int32>, \n        GC_AFR_Female: array<int32>, \n        GC_AMR_Female: array<int32>, \n        GC_ASJ_Female: array<int32>, \n        GC_EAS_Female: array<int32>, \n        GC_FIN_Female: array<int32>, \n        GC_NFE_Female: array<int32>, \n        GC_OTH_Female: array<int32>, \n        GC_SAS_Female: array<int32>, \n        Hemi_AFR: array<int32>, \n        Hemi_AMR: array<int32>, \n        Hemi_ASJ: array<int32>, \n        Hemi_EAS: array<int32>, \n        Hemi_FIN: array<int32>, \n        Hemi_NFE: array<int32>, \n        Hemi_OTH: array<int32>, \n        Hemi_SAS: array<int32>, \n        Hemi: array<int32>, \n        Hemi_raw: array<int32>, \n        STAR_Hemi: int32\n    } \n    'variant_qc': struct {\n        dp_stats: struct {\n            mean: float64, \n            stdev: float64, \n            min: float64, \n            max: float64\n        }, \n        gq_stats: struct {\n            mean: float64, \n            stdev: float64, \n            min: float64, \n            max: float64\n        }, \n        AC: array<int32>, \n        AF: array<float64>, \n        AN: int32, \n        homozygote_count: array<int32>, \n        call_rate: float64, \n        n_called: int64, \n        n_not_called: int64, \n        n_filtered: int64, \n        n_het: int64, \n        n_non_ref: int64, \n        het_freq_hwe: float64, \n        p_value_hwe: float64\n    } \n----------------------------------------\nKey: ['locus', 'alleles']\n----------------------------------------\n\n\nLet’s filter for loci that have: - an allele frequency (AF) value between 0.001-0.999, - a Hardy-Weinberg Equilibrium p-value (p_value_hwe) greater or equal to 1e-10, - a call rate (call_rate) greater or equal to 0.9\n\n# Filter QC Table using expressions\n# Note: Viewing the structure of the locus QC table in from the cell above \n# shows us that the \"AF\", \"p_value_hwe\", and \"call_rate\" fields are within\n# the \"variant_qc\" struct field.\n\nlocus_qc_tb = pre_locus_qc_tb.filter(\n    (pre_locus_qc_tb[\"variant_qc\"][\"AF\"][0] >= 0.001) & \n    (pre_locus_qc_tb[\"variant_qc\"][\"AF\"][0] <= 0.999) & \n    (pre_locus_qc_tb[\"variant_qc\"][\"p_value_hwe\"] >= 1e-10) & \n    (pre_locus_qc_tb[\"variant_qc\"][\"call_rate\"] >= 0.9)\n)\n\n\n# DON'T RUN in class\n# View number of loci in QC Table before and after filtering\n#\n# Note: running this cell can be computationally expensive and take\n# longer for bigger datasets (this cell can be commented out).\n\nprint(f\"Num loci before filtering: {pre_locus_qc_tb.count()}\")\nprint(f\"Num loci after filtering: {locus_qc_tb.count()}\")\n\nNum loci before filtering: 3403513\n\nNum loci after filtering: 39952\n\n\n\n6.7.1.1 4b) Filter MT with variant QC Tables\nNow we can use .semi_join_rows() to filter our phenogeno_mt with locus_qc_tb.\n\n# Filter the MT using the locus QC Table\n\nqc_mt = qc_mt.semi_join_rows(locus_qc_tb)\n\nAfter filtereing, we have reduced the number of loci in our MatrixTable to be 39952 rows.\n\n# DON'T RUN in class\n# View MT after QC filters\n# \n# Note: running 'mt.rows().count()' or 'mt.cols().count()' can be computationally \n# expensive and take longer for bigger datasets (these lines can be commented out).\n\nprint(f\"Num partitions: {qc_mt.n_partitions()}\")\nprint(f\"Num loci: {qc_mt.rows().count()}\")\nprint(f\"Num samples: {qc_mt.cols().count()}\")\nqc_mt.describe()\n\nNum partitions: 228\nNum loci: 39952\n\n\n2023-04-19 22:33:57 Hail: WARN: cols(): Resulting column table is sorted by 'col_key'.\n    To preserve matrix table column order, first unkey columns with 'key_cols_by()'\n\n\nNum samples: 50062\n----------------------------------------\nGlobal fields:\n    None\n----------------------------------------\nColumn fields:\n    's': str\n    'fid': str\n    'father_iid': int32\n    'mother_iid': int32\n    'sex_code': int32\n    'case_control_status': int32\n    'ccs': int32\n    'sc': int32\n----------------------------------------\nRow fields:\n    'locus': locus<GRCh38>\n    'alleles': array<str>\n    'rsid': str\n    'qual': float64\n    'filters': set<str>\n    'info': struct {\n        AC: array<int32>, \n        AF: array<float64>, \n        AN: int32, \n        BaseQRankSum: float64, \n        ClippingRankSum: float64, \n        DB: bool, \n        DP: int32, \n        FS: float64, \n        InbreedingCoeff: float64, \n        MQ: float64, \n        MQRankSum: float64, \n        QD: float64, \n        ReadPosRankSum: float64, \n        SOR: float64, \n        VQSLOD: float64, \n        VQSR_culprit: str, \n        VQSR_NEGATIVE_TRAIN_SITE: bool, \n        VQSR_POSITIVE_TRAIN_SITE: bool, \n        GQ_HIST_ALT: array<str>, \n        DP_HIST_ALT: array<str>, \n        AB_HIST_ALT: array<str>, \n        GQ_HIST_ALL: str, \n        DP_HIST_ALL: str, \n        AB_HIST_ALL: str, \n        AC_AFR: array<int32>, \n        AC_AMR: array<int32>, \n        AC_ASJ: array<int32>, \n        AC_EAS: array<int32>, \n        AC_FIN: array<int32>, \n        AC_NFE: array<int32>, \n        AC_OTH: array<int32>, \n        AC_SAS: array<int32>, \n        AC_Male: array<int32>, \n        AC_Female: array<int32>, \n        AN_AFR: int32, \n        AN_AMR: int32, \n        AN_ASJ: int32, \n        AN_EAS: int32, \n        AN_FIN: int32, \n        AN_NFE: int32, \n        AN_OTH: int32, \n        AN_SAS: int32, \n        AN_Male: int32, \n        AN_Female: int32, \n        AF_AFR: array<float64>, \n        AF_AMR: array<float64>, \n        AF_ASJ: array<float64>, \n        AF_EAS: array<float64>, \n        AF_FIN: array<float64>, \n        AF_NFE: array<float64>, \n        AF_OTH: array<float64>, \n        AF_SAS: array<float64>, \n        AF_Male: array<float64>, \n        AF_Female: array<float64>, \n        GC_AFR: array<int32>, \n        GC_AMR: array<int32>, \n        GC_ASJ: array<int32>, \n        GC_EAS: array<int32>, \n        GC_FIN: array<int32>, \n        GC_NFE: array<int32>, \n        GC_OTH: array<int32>, \n        GC_SAS: array<int32>, \n        GC_Male: array<int32>, \n        GC_Female: array<int32>, \n        AC_raw: array<int32>, \n        AN_raw: int32, \n        AF_raw: array<float64>, \n        GC_raw: array<int32>, \n        GC: array<int32>, \n        Hom_AFR: array<int32>, \n        Hom_AMR: array<int32>, \n        Hom_ASJ: array<int32>, \n        Hom_EAS: array<int32>, \n        Hom_FIN: array<int32>, \n        Hom_NFE: array<int32>, \n        Hom_OTH: array<int32>, \n        Hom_SAS: array<int32>, \n        Hom_Male: array<int32>, \n        Hom_Female: array<int32>, \n        Hom_raw: array<int32>, \n        Hom: array<int32>, \n        STAR_AC: int32, \n        STAR_AC_raw: int32, \n        STAR_Hom: int32, \n        POPMAX: array<str>, \n        AC_POPMAX: array<int32>, \n        AN_POPMAX: array<int32>, \n        AF_POPMAX: array<float64>, \n        DP_MEDIAN: array<int32>, \n        DREF_MEDIAN: array<float64>, \n        GQ_MEDIAN: array<int32>, \n        AB_MEDIAN: array<float64>, \n        AS_RF: array<float64>, \n        AS_FilterStatus: array<str>, \n        AS_RF_POSITIVE_TRAIN: array<int32>, \n        AS_RF_NEGATIVE_TRAIN: array<int32>, \n        AC_AFR_Male: array<int32>, \n        AC_AMR_Male: array<int32>, \n        AC_ASJ_Male: array<int32>, \n        AC_EAS_Male: array<int32>, \n        AC_FIN_Male: array<int32>, \n        AC_NFE_Male: array<int32>, \n        AC_OTH_Male: array<int32>, \n        AC_SAS_Male: array<int32>, \n        AC_AFR_Female: array<int32>, \n        AC_AMR_Female: array<int32>, \n        AC_ASJ_Female: array<int32>, \n        AC_EAS_Female: array<int32>, \n        AC_FIN_Female: array<int32>, \n        AC_NFE_Female: array<int32>, \n        AC_OTH_Female: array<int32>, \n        AC_SAS_Female: array<int32>, \n        AN_AFR_Male: int32, \n        AN_AMR_Male: int32, \n        AN_ASJ_Male: int32, \n        AN_EAS_Male: int32, \n        AN_FIN_Male: int32, \n        AN_NFE_Male: int32, \n        AN_OTH_Male: int32, \n        AN_SAS_Male: int32, \n        AN_AFR_Female: int32, \n        AN_AMR_Female: int32, \n        AN_ASJ_Female: int32, \n        AN_EAS_Female: int32, \n        AN_FIN_Female: int32, \n        AN_NFE_Female: int32, \n        AN_OTH_Female: int32, \n        AN_SAS_Female: int32, \n        AF_AFR_Male: array<float64>, \n        AF_AMR_Male: array<float64>, \n        AF_ASJ_Male: array<float64>, \n        AF_EAS_Male: array<float64>, \n        AF_FIN_Male: array<float64>, \n        AF_NFE_Male: array<float64>, \n        AF_OTH_Male: array<float64>, \n        AF_SAS_Male: array<float64>, \n        AF_AFR_Female: array<float64>, \n        AF_AMR_Female: array<float64>, \n        AF_ASJ_Female: array<float64>, \n        AF_EAS_Female: array<float64>, \n        AF_FIN_Female: array<float64>, \n        AF_NFE_Female: array<float64>, \n        AF_OTH_Female: array<float64>, \n        AF_SAS_Female: array<float64>, \n        GC_AFR_Male: array<int32>, \n        GC_AMR_Male: array<int32>, \n        GC_ASJ_Male: array<int32>, \n        GC_EAS_Male: array<int32>, \n        GC_FIN_Male: array<int32>, \n        GC_NFE_Male: array<int32>, \n        GC_OTH_Male: array<int32>, \n        GC_SAS_Male: array<int32>, \n        GC_AFR_Female: array<int32>, \n        GC_AMR_Female: array<int32>, \n        GC_ASJ_Female: array<int32>, \n        GC_EAS_Female: array<int32>, \n        GC_FIN_Female: array<int32>, \n        GC_NFE_Female: array<int32>, \n        GC_OTH_Female: array<int32>, \n        GC_SAS_Female: array<int32>, \n        Hemi_AFR: array<int32>, \n        Hemi_AMR: array<int32>, \n        Hemi_ASJ: array<int32>, \n        Hemi_EAS: array<int32>, \n        Hemi_FIN: array<int32>, \n        Hemi_NFE: array<int32>, \n        Hemi_OTH: array<int32>, \n        Hemi_SAS: array<int32>, \n        Hemi: array<int32>, \n        Hemi_raw: array<int32>, \n        STAR_Hemi: int32\n    }\n----------------------------------------\nEntry fields:\n    'GT': call\n    'AD': array<int32>\n    'DP': int32\n    'GQ': int32\n    'PL': array<int32>\n----------------------------------------\nColumn key: ['s']\nRow key: ['locus', 'alleles']\n----------------------------------------"
  },
  {
    "objectID": "02-gwas.html#sec-gwas",
    "href": "02-gwas.html#sec-gwas",
    "title": "6  GWAS using Hail",
    "section": "6.8 Run GWAS",
    "text": "6.8 Run GWAS\nNow that we’ve done our filtering, we can now run our GWAS on our filtered MatrixTable.\nQuick reminder that our pheno file has a lot of missing values, so we’re going to be dropping a bunch of samples when we do our analysis.\n\nqc_mt.ccs.show()\n\n\nsccsstrint32\n\"sample_1_1\"NA\n\"sample_1_3\"0\n\"sample_1_6\"NA\n\"sample_1_9\"NA\n\"sample_1_10\"1\n\"sample_1_11\"NA\n\"sample_1_12\"NA\n\"sample_1_13\"NA\n\"sample_1_14\"NA\n\"sample_1_15\"NA\nshowing top 10 rows\n\n\nAdditional documentation: https://hail.is/docs/0.2/methods/stats.html#hail.methods.logistic_regression_rows\n\n# Run Hail's logistic regression method\n\ngwas = hl.logistic_regression_rows(test=\"firth\",\n                                   y=qc_mt.col.ccs, # the column field of the pheno trait we are looking at ('ccs')\n                                   x=qc_mt.GT.n_alt_alleles(), # n_alt_alleles() returns the count of non-reference alleles\n                                   covariates=[1, qc_mt.col.sc])\n\n2023-04-19 22:34:55 Hail: WARN: 40104 of 50062 samples have a missing phenotype or covariate.\n2023-04-19 22:34:55 Hail: INFO: logistic_regression_rows: running firth on 9958 samples for response variable y,\n    with input variable x, and 2 additional covariates...\n\n\n\n# View structure of GWAS results Table\n\ngwas.describe()\n\n----------------------------------------\nGlobal fields:\n    None\n----------------------------------------\nRow fields:\n    'locus': locus<GRCh38> \n    'alleles': array<str> \n    'beta': float64 \n    'chi_sq_stat': float64 \n    'p_value': float64 \n    'fit': struct {\n        n_iterations: int32, \n        converged: bool, \n        exploded: bool\n    } \n----------------------------------------\nKey: ['locus', 'alleles']\n----------------------------------------\n\n\n\ngwas.show()\n\n\nfitlocusallelesbetachi_sq_statp_valuen_iterationsconvergedexplodedlocus<GRCh38>array<str>float64float64float64int32boolbool\nchr1:12719[\"G\",\"C\"]1.96e-021.35e-029.08e-013TrueFalse\nchr1:13380[\"C\",\"G\"]-3.00e-019.08e-013.41e-015TrueFalse\nchr1:13494[\"A\",\"G\"]-3.33e-016.83e-014.09e-015TrueFalse\nchr1:13504[\"G\",\"A\"]1.50e-012.84e-015.94e-015TrueFalse\nchr1:17358[\"ACTT\",\"A\"]4.46e-011.30e+002.54e-015TrueFalse\nchr1:17373[\"A\",\"G\"]-3.88e-024.94e-028.24e-014TrueFalse\nchr1:17379[\"G\",\"A\"]4.65e-012.92e+008.72e-024TrueFalse\nchr1:17403[\"A\",\"G\"]1.51e-011.41e-017.07e-015TrueFalse\nchr1:17408[\"C\",\"G\"]-8.03e-021.80e-016.71e-014TrueFalse\nchr1:69735[\"A\",\"G\"]7.29e-023.18e-028.59e-015TrueFalse\nshowing top 10 rows\n\n\n\n\npandas_gwas = gwas.to_pandas()\npandas_gwas.to_csv(\"gwas_results.csv\")"
  },
  {
    "objectID": "02-gwas.html#sec-visualize",
    "href": "02-gwas.html#sec-visualize",
    "title": "6  GWAS using Hail",
    "section": "6.9 Visualize GWAS results",
    "text": "6.9 Visualize GWAS results\nBokeh is a Python library that is included in this JupyterLab environment- which makes it easy for us to import.\nWe’ll need the output_notebook and show modules to make our plots.\n\nfrom bokeh.io import output_notebook, show\noutput_notebook()\n\n\n    \n        \n        Loading BokehJS ...\n    \n\n\n\n\n\n\n\nqq_plot = hl.plot.qq(gwas.p_value)\nshow(qq_plot)\n\n2023-04-19 22:12:04 Hail: INFO: Ordering unsorted dataset with network shuffle\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nWe can generate a Manhattan Plot on our p-values using hl.plot.manhattan().\n\nmanhattan_plot = hl.plot.manhattan(gwas_tb.p_value)\nshow(manhattan_plot)"
  },
  {
    "objectID": "02-gwas.html#sec-annotate",
    "href": "02-gwas.html#sec-annotate",
    "title": "6  GWAS using Hail",
    "section": "6.10 Annotate GWAS results",
    "text": "6.10 Annotate GWAS results\nNow that we have our GWAS results, we’ll annotate our file using Hail’s experimental DB, which is hosted on Open Data on AWS.\nNote that this resource is not available in the RAP region. Here we’re going to add GENCODE annotations to our matrix. There are a lot of other annotations available in the DB as well.\nThere is the query builder if you have access to the experimental annotations. This will help you write your db.annotate_rows_db() statement.\nhttps://hail.is/docs/0.2/annotation_database_ui.html#database-query\n\ndb = hl.experimental.DB(region='us', cloud='aws')\nann_gwas_tb = db.annotate_rows_db(gwas, 'gencode')\nann_gwas_tb.show()\n\n\nfitlocusallelesbetachi_sq_statp_valuen_iterationsconvergedexplodedgencodelocus<GRCh38>array<str>float64float64float64int32boolboolarray<struct{ID: str, gene_id: str, gene_name: str, gene_type: str, level: str, type: str, gene_score: str, gene_strand: str, gene_phase: str}>\nchr1:12719[\"G\",\"C\"]1.96e-021.35e-029.08e-013TrueFalse[(\"ENSG00000223972.5\",\"ENSG00000223972.5\",\"DDX11L1\",\"transcribed_unprocessed_pseudogene\",\"2\",\"gene\",\".\",\"+\",\".\")]\nchr1:13380[\"C\",\"G\"]-3.00e-019.08e-013.41e-015TrueFalse[(\"ENSG00000223972.5\",\"ENSG00000223972.5\",\"DDX11L1\",\"transcribed_unprocessed_pseudogene\",\"2\",\"gene\",\".\",\"+\",\".\")]\nchr1:13494[\"A\",\"G\"]-3.33e-016.83e-014.09e-015TrueFalse[(\"ENSG00000223972.5\",\"ENSG00000223972.5\",\"DDX11L1\",\"transcribed_unprocessed_pseudogene\",\"2\",\"gene\",\".\",\"+\",\".\")]\nchr1:13504[\"G\",\"A\"]1.50e-012.84e-015.94e-015TrueFalse[(\"ENSG00000223972.5\",\"ENSG00000223972.5\",\"DDX11L1\",\"transcribed_unprocessed_pseudogene\",\"2\",\"gene\",\".\",\"+\",\".\")]\nchr1:17358[\"ACTT\",\"A\"]4.46e-011.30e+002.54e-015TrueFalse[(\"ENSG00000227232.5\",\"ENSG00000227232.5\",\"WASH7P\",\"unprocessed_pseudogene\",\"2\",\"gene\",\".\",\"-\",\".\")]\nchr1:17373[\"A\",\"G\"]-3.88e-024.94e-028.24e-014TrueFalse[(\"ENSG00000278267.1\",\"ENSG00000278267.1\",\"MIR6859-1\",\"miRNA\",\"3\",\"gene\",\".\",\"-\",\".\"),(\"ENSG00000227232.5\",\"ENSG00000227232.5\",\"WASH7P\",\"unprocessed_pseudogene\",\"2\",\"gene\",\".\",\"-\",\".\")]\nchr1:17379[\"G\",\"A\"]4.65e-012.92e+008.72e-024TrueFalse[(\"ENSG00000278267.1\",\"ENSG00000278267.1\",\"MIR6859-1\",\"miRNA\",\"3\",\"gene\",\".\",\"-\",\".\"),(\"ENSG00000227232.5\",\"ENSG00000227232.5\",\"WASH7P\",\"unprocessed_pseudogene\",\"2\",\"gene\",\".\",\"-\",\".\")]\nchr1:17403[\"A\",\"G\"]1.51e-011.41e-017.07e-015TrueFalse[(\"ENSG00000278267.1\",\"ENSG00000278267.1\",\"MIR6859-1\",\"miRNA\",\"3\",\"gene\",\".\",\"-\",\".\"),(\"ENSG00000227232.5\",\"ENSG00000227232.5\",\"WASH7P\",\"unprocessed_pseudogene\",\"2\",\"gene\",\".\",\"-\",\".\")]\nchr1:17408[\"C\",\"G\"]-8.03e-021.80e-016.71e-014TrueFalse[(\"ENSG00000278267.1\",\"ENSG00000278267.1\",\"MIR6859-1\",\"miRNA\",\"3\",\"gene\",\".\",\"-\",\".\"),(\"ENSG00000227232.5\",\"ENSG00000227232.5\",\"WASH7P\",\"unprocessed_pseudogene\",\"2\",\"gene\",\".\",\"-\",\".\")]\nchr1:69735[\"A\",\"G\"]7.29e-023.18e-028.59e-015TrueFalse[(\"ENSG00000186092.6\",\"ENSG00000186092.6\",\"OR4F5\",\"protein_coding\",\"2\",\"gene\",\".\",\"+\",\".\")]\nshowing top 10 rows\n\n\nOn RAP, we do not have access to this resource, as it isn’t available in the UKB RAP region, so we must use the Hail/VEP configuration (note we started with the Hail/VEP features).\nhl.vep() requires a config file in JSON format. I have used the one available here: https://documentation.dnanexus.com/user/jupyter-notebooks/dxjupyterlab-spark-cluster#using-vep-with-hail\n\nann_gwas_tb2 = hl.vep(ann_gwas_tb, \"file:///mnt/project/notebooks/config.json\")\nann_gwas_tb2.show()\n\n\nfitvepvep_proc_idlocusallelesbetachi_sq_statp_valuen_iterationsconvergedexplodedgencodeassembly_nameallele_stringancestralcolocated_variantscontextendidinputintergenic_consequencesmost_severe_consequencemotif_feature_consequencesregulatory_feature_consequencesseq_region_namestartstrandtranscript_consequencesvariant_classpart_idxblock_idxlocus<GRCh38>array<str>float64float64float64int32boolboolarray<struct{ID: str, gene_id: str, gene_name: str, gene_type: str, level: str, type: str, gene_score: str, gene_strand: str, gene_phase: str}>strstrstrarray<struct{aa_allele: str, aa_maf: float64, afr_allele: str, afr_maf: float64, allele_string: str, amr_allele: str, amr_maf: float64, clin_sig: array<str>, end: int32, eas_allele: str, eas_maf: float64, ea_allele: str, ea_maf: float64, eur_allele: str, eur_maf: float64, exac_adj_allele: str, exac_adj_maf: float64, exac_allele: str, exac_afr_allele: str, exac_afr_maf: float64, exac_amr_allele: str, exac_amr_maf: float64, exac_eas_allele: str, exac_eas_maf: float64, exac_fin_allele: str, exac_fin_maf: float64, exac_maf: float64, exac_nfe_allele: str, exac_nfe_maf: float64, exac_oth_allele: str, exac_oth_maf: float64, exac_sas_allele: str, exac_sas_maf: float64, id: str, minor_allele: str, minor_allele_freq: float64, phenotype_or_disease: int32, pubmed: array<int32>, sas_allele: str, sas_maf: float64, somatic: int32, start: int32, strand: int32}>strint32strstrarray<struct{allele_num: int32, consequence_terms: array<str>, impact: str, minimised: int32, variant_allele: str}>strarray<struct{allele_num: int32, consequence_terms: array<str>, high_inf_pos: str, impact: str, minimised: int32, motif_feature_id: str, motif_name: str, motif_pos: int32, motif_score_change: float64, strand: int32, variant_allele: str}>array<struct{allele_num: int32, biotype: str, consequence_terms: array<str>, impact: str, minimised: int32, regulatory_feature_id: str, variant_allele: str}>strint32int32array<struct{allele_num: int32, amino_acids: str, appris: str, biotype: str, canonical: int32, ccds: str, cdna_start: int32, cdna_end: int32, cds_end: int32, cds_start: int32, codons: str, consequence_terms: array<str>, distance: int32, domains: array<struct{db: str, name: str}>, exon: str, gene_id: str, gene_pheno: int32, gene_symbol: str, gene_symbol_source: str, hgnc_id: str, hgvsc: str, hgvsp: str, hgvs_offset: int32, impact: str, intron: str, lof: str, lof_flags: str, lof_filter: str, lof_info: str, minimised: int32, polyphen_prediction: str, polyphen_score: float64, protein_end: int32, protein_start: int32, protein_id: str, sift_prediction: str, sift_score: float64, strand: int32, swissprot: str, transcript_id: str, trembl: str, tsl: int32, uniparc: str, variant_allele: str}>strint32int32\nchr1:12719[\"G\",\"C\"]1.96e-021.35e-029.08e-013TrueFalse[(\"ENSG00000223972.5\",\"ENSG00000223972.5\",\"DDX11L1\",\"transcribed_unprocessed_pseudogene\",\"2\",\"gene\",\".\",\"+\",\".\")]\"GRCh38\"\"G/C\"NA[(NA,NA,NA,NA,\"G/C\",NA,NA,NA,12719,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,\"rs1410641955\",NA,NA,NA,NA,NA,NA,NA,12719,1)]NA12719\".\"\"chr1 12719   .   G   C   .   .   GT\"NA\"splice_region_variant\"NANA\"chr1\"127191[(1,NA,NA,\"transcribed_unprocessed_pseudogene\",NA,NA,NA,NA,NA,NA,NA,[\"intron_variant\",\"non_coding_transcript_variant\"],NA,NA,NA,\"ENSG00000223972\",NA,\"DDX11L1\",\"HGNC\",\"HGNC:37102\",\"ENST00000450305.2:n.182+22G>C\",NA,NA,\"MODIFIER\",\"3/5\",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,1,NA,\"ENST00000450305\",NA,NA,NA,\"C\"),(1,NA,NA,\"processed_transcript\",1,NA,466,466,NA,NA,NA,[\"splice_region_variant\",\"non_coding_transcript_exon_variant\"],NA,NA,\"2/3\",\"ENSG00000223972\",NA,\"DDX11L1\",\"HGNC\",\"HGNC:37102\",\"ENST00000456328.2:n.466G>C\",NA,NA,\"LOW\",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,1,NA,\"ENST00000456328\",NA,1,NA,\"C\"),(1,NA,NA,\"unprocessed_pseudogene\",1,NA,NA,NA,NA,NA,NA,[\"downstream_gene_variant\"],1685,NA,NA,\"ENSG00000227232\",NA,\"WASH7P\",\"HGNC\",\"HGNC:38034\",NA,NA,NA,\"MODIFIER\",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,-1,NA,\"ENST00000488147\",NA,NA,NA,\"C\"),(1,NA,NA,\"miRNA\",1,NA,NA,NA,NA,NA,NA,[\"downstream_gene_variant\"],4650,NA,NA,\"ENSG00000278267\",NA,\"MIR6859-1\",\"HGNC\",\"HGNC:50039\",NA,NA,NA,\"MODIFIER\",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,-1,NA,\"ENST00000619216\",NA,NA,NA,\"C\")]\"SNV\"00\nchr1:13380[\"C\",\"G\"]-3.00e-019.08e-013.41e-015TrueFalse[(\"ENSG00000223972.5\",\"ENSG00000223972.5\",\"DDX11L1\",\"transcribed_unprocessed_pseudogene\",\"2\",\"gene\",\".\",\"+\",\".\")]\"GRCh38\"\"C/G\"NA[(NA,NA,NA,NA,\"C/G\",NA,NA,NA,13380,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,\"rs571093408\",\"G\",8.20e-03,NA,NA,NA,NA,NA,13380,1)]NA13380\".\"\"chr1   13380   .   C   G   .   .   GT\"NA\"splice_region_variant\"[(1,[\"TF_binding_site_variant\"],\"N\",\"MODIFIER\",NA,\"ENSM00080464946\",\"ENSPFM0257\",19,-2.10e-02,-1,\"G\")][(1,\"promoter_flanking_region\",[\"regulatory_region_variant\"],\"MODIFIER\",NA,\"ENSR00001164745\",\"G\")]\"chr1\"133801[(1,NA,NA,\"transcribed_unprocessed_pseudogene\",NA,NA,NA,NA,NA,NA,NA,[\"splice_region_variant\",\"intron_variant\",\"non_coding_transcript_variant\"],NA,NA,NA,\"ENSG00000223972\",NA,\"DDX11L1\",\"HGNC\",\"HGNC:37102\",\"ENST00000450305.2:n.414+6C>G\",NA,NA,\"LOW\",\"5/5\",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,1,NA,\"ENST00000450305\",NA,NA,NA,\"G\"),(1,NA,NA,\"processed_transcript\",1,NA,628,628,NA,NA,NA,[\"non_coding_transcript_exon_variant\"],NA,NA,\"3/3\",\"ENSG00000223972\",NA,\"DDX11L1\",\"HGNC\",\"HGNC:37102\",\"ENST00000456328.2:n.628C>G\",NA,NA,\"MODIFIER\",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,1,NA,\"ENST00000456328\",NA,1,NA,\"G\"),(1,NA,NA,\"unprocessed_pseudogene\",1,NA,NA,NA,NA,NA,NA,[\"downstream_gene_variant\"],1024,NA,NA,\"ENSG00000227232\",NA,\"WASH7P\",\"HGNC\",\"HGNC:38034\",NA,NA,NA,\"MODIFIER\",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,-1,NA,\"ENST00000488147\",NA,NA,NA,\"G\"),(1,NA,NA,\"miRNA\",1,NA,NA,NA,NA,NA,NA,[\"downstream_gene_variant\"],3989,NA,NA,\"ENSG00000278267\",NA,\"MIR6859-1\",\"HGNC\",\"HGNC:50039\",NA,NA,NA,\"MODIFIER\",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,-1,NA,\"ENST00000619216\",NA,NA,NA,\"G\")]\"SNV\"00\nchr1:13494[\"A\",\"G\"]-3.33e-016.83e-014.09e-015TrueFalse[(\"ENSG00000223972.5\",\"ENSG00000223972.5\",\"DDX11L1\",\"transcribed_unprocessed_pseudogene\",\"2\",\"gene\",\".\",\"+\",\".\")]\"GRCh38\"\"A/G\"NA[(NA,NA,NA,NA,\"A/G\",NA,NA,NA,13494,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,\"rs574697788\",\"G\",1.40e-03,NA,NA,NA,NA,NA,13494,1)]NA13494\".\"\"chr1   13494   .   A   G   .   .   GT\"NA\"non_coding_transcript_exon_variant\"NA[(1,\"CTCF_binding_site\",[\"regulatory_region_variant\"],\"MODIFIER\",NA,\"ENSR00000344265\",\"G\"),(1,\"promoter_flanking_region\",[\"regulatory_region_variant\"],\"MODIFIER\",NA,\"ENSR00001164745\",\"G\")]\"chr1\"134941[(1,NA,NA,\"transcribed_unprocessed_pseudogene\",NA,NA,456,456,NA,NA,NA,[\"non_coding_transcript_exon_variant\"],NA,NA,\"6/6\",\"ENSG00000223972\",NA,\"DDX11L1\",\"HGNC\",\"HGNC:37102\",\"ENST00000450305.2:n.456A>G\",NA,NA,\"MODIFIER\",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,1,NA,\"ENST00000450305\",NA,NA,NA,\"G\"),(1,NA,NA,\"processed_transcript\",1,NA,742,742,NA,NA,NA,[\"non_coding_transcript_exon_variant\"],NA,NA,\"3/3\",\"ENSG00000223972\",NA,\"DDX11L1\",\"HGNC\",\"HGNC:37102\",\"ENST00000456328.2:n.742A>G\",NA,NA,\"MODIFIER\",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,1,NA,\"ENST00000456328\",NA,1,NA,\"G\"),(1,NA,NA,\"unprocessed_pseudogene\",1,NA,NA,NA,NA,NA,NA,[\"downstream_gene_variant\"],910,NA,NA,\"ENSG00000227232\",NA,\"WASH7P\",\"HGNC\",\"HGNC:38034\",NA,NA,NA,\"MODIFIER\",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,-1,NA,\"ENST00000488147\",NA,NA,NA,\"G\"),(1,NA,NA,\"miRNA\",1,NA,NA,NA,NA,NA,NA,[\"downstream_gene_variant\"],3875,NA,NA,\"ENSG00000278267\",NA,\"MIR6859-1\",\"HGNC\",\"HGNC:50039\",NA,NA,NA,\"MODIFIER\",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,-1,NA,\"ENST00000619216\",NA,NA,NA,\"G\")]\"SNV\"00\nchr1:13504[\"G\",\"A\"]1.50e-012.84e-015.94e-015TrueFalse[(\"ENSG00000223972.5\",\"ENSG00000223972.5\",\"DDX11L1\",\"transcribed_unprocessed_pseudogene\",\"2\",\"gene\",\".\",\"+\",\".\")]\"GRCh38\"\"G/A\"NA[(NA,NA,NA,NA,\"G/A\",NA,NA,NA,13504,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,\"rs199896944\",NA,NA,NA,NA,NA,NA,NA,13504,1)]NA13504\".\"\"chr1 13504   .   G   A   .   .   GT\"NA\"non_coding_transcript_exon_variant\"NA[(1,\"CTCF_binding_site\",[\"regulatory_region_variant\"],\"MODIFIER\",NA,\"ENSR00000344265\",\"A\"),(1,\"promoter_flanking_region\",[\"regulatory_region_variant\"],\"MODIFIER\",NA,\"ENSR00001164745\",\"A\")]\"chr1\"135041[(1,NA,NA,\"transcribed_unprocessed_pseudogene\",NA,NA,466,466,NA,NA,NA,[\"non_coding_transcript_exon_variant\"],NA,NA,\"6/6\",\"ENSG00000223972\",NA,\"DDX11L1\",\"HGNC\",\"HGNC:37102\",\"ENST00000450305.2:n.466G>A\",NA,NA,\"MODIFIER\",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,1,NA,\"ENST00000450305\",NA,NA,NA,\"A\"),(1,NA,NA,\"processed_transcript\",1,NA,752,752,NA,NA,NA,[\"non_coding_transcript_exon_variant\"],NA,NA,\"3/3\",\"ENSG00000223972\",NA,\"DDX11L1\",\"HGNC\",\"HGNC:37102\",\"ENST00000456328.2:n.752G>A\",NA,NA,\"MODIFIER\",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,1,NA,\"ENST00000456328\",NA,1,NA,\"A\"),(1,NA,NA,\"unprocessed_pseudogene\",1,NA,NA,NA,NA,NA,NA,[\"downstream_gene_variant\"],900,NA,NA,\"ENSG00000227232\",NA,\"WASH7P\",\"HGNC\",\"HGNC:38034\",NA,NA,NA,\"MODIFIER\",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,-1,NA,\"ENST00000488147\",NA,NA,NA,\"A\"),(1,NA,NA,\"miRNA\",1,NA,NA,NA,NA,NA,NA,[\"downstream_gene_variant\"],3865,NA,NA,\"ENSG00000278267\",NA,\"MIR6859-1\",\"HGNC\",\"HGNC:50039\",NA,NA,NA,\"MODIFIER\",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,-1,NA,\"ENST00000619216\",NA,NA,NA,\"A\")]\"SNV\"00\nchr1:17358[\"ACTT\",\"A\"]4.46e-011.30e+002.54e-015TrueFalse[(\"ENSG00000227232.5\",\"ENSG00000227232.5\",\"WASH7P\",\"unprocessed_pseudogene\",\"2\",\"gene\",\".\",\"-\",\".\")]\"GRCh38\"\"CTT/-\"NA[(NA,NA,NA,NA,\"CTTCTTCT/CTTCT\",NA,NA,NA,17366,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,\"rs749387668\",NA,NA,NA,NA,NA,NA,NA,17359,1)]NA17361\".\"\"chr1  17358   .   ACTT    A   .   .   GT\"NA\"non_coding_transcript_exon_variant\"NANA\"chr1\"173591[(1,NA,NA,\"transcribed_unprocessed_pseudogene\",NA,NA,NA,NA,NA,NA,NA,[\"downstream_gene_variant\"],3689,NA,NA,\"ENSG00000223972\",NA,\"DDX11L1\",\"HGNC\",\"HGNC:37102\",NA,NA,NA,\"MODIFIER\",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,1,NA,\"ENST00000450305\",NA,NA,NA,\"-\"),(1,NA,NA,\"processed_transcript\",1,NA,NA,NA,NA,NA,NA,[\"downstream_gene_variant\"],2950,NA,NA,\"ENSG00000223972\",NA,\"DDX11L1\",\"HGNC\",\"HGNC:37102\",NA,NA,NA,\"MODIFIER\",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,1,NA,\"ENST00000456328\",NA,1,NA,\"-\"),(1,NA,NA,\"unprocessed_pseudogene\",1,NA,582,584,NA,NA,NA,[\"non_coding_transcript_exon_variant\"],NA,NA,\"6/11\",\"ENSG00000227232\",NA,\"WASH7P\",\"HGNC\",\"HGNC:38034\",\"ENST00000488147.1:n.582_584del\",NA,NA,\"MODIFIER\",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,-1,NA,\"ENST00000488147\",NA,NA,NA,\"-\"),(1,NA,NA,\"miRNA\",1,NA,NA,NA,NA,NA,NA,[\"downstream_gene_variant\"],8,NA,NA,\"ENSG00000278267\",NA,\"MIR6859-1\",\"HGNC\",\"HGNC:50039\",NA,NA,NA,\"MODIFIER\",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,-1,NA,\"ENST00000619216\",NA,NA,NA,\"-\")]\"deletion\"00\nchr1:17373[\"A\",\"G\"]-3.88e-024.94e-028.24e-014TrueFalse[(\"ENSG00000278267.1\",\"ENSG00000278267.1\",\"MIR6859-1\",\"miRNA\",\"3\",\"gene\",\".\",\"-\",\".\"),(\"ENSG00000227232.5\",\"ENSG00000227232.5\",\"WASH7P\",\"unprocessed_pseudogene\",\"2\",\"gene\",\".\",\"-\",\".\")]\"GRCh38\"\"A/G\"NA[(NA,NA,NA,NA,\"A/G\",NA,NA,NA,17373,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,\"rs750111615\",NA,NA,NA,NA,NA,NA,NA,17373,1)]NA17373\".\"\"chr1  17373   .   A   G   .   .   GT\"NA\"splice_region_variant\"NANA\"chr1\"173731[(1,NA,NA,\"transcribed_unprocessed_pseudogene\",NA,NA,NA,NA,NA,NA,NA,[\"downstream_gene_variant\"],3703,NA,NA,\"ENSG00000223972\",NA,\"DDX11L1\",\"HGNC\",\"HGNC:37102\",NA,NA,NA,\"MODIFIER\",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,1,NA,\"ENST00000450305\",NA,NA,NA,\"G\"),(1,NA,NA,\"processed_transcript\",1,NA,NA,NA,NA,NA,NA,[\"downstream_gene_variant\"],2964,NA,NA,\"ENSG00000223972\",NA,\"DDX11L1\",\"HGNC\",\"HGNC:37102\",NA,NA,NA,\"MODIFIER\",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,1,NA,\"ENST00000456328\",NA,1,NA,\"G\"),(1,NA,NA,\"unprocessed_pseudogene\",1,NA,NA,NA,NA,NA,NA,[\"splice_region_variant\",\"intron_variant\",\"non_coding_transcript_variant\"],NA,NA,NA,\"ENSG00000227232\",NA,\"WASH7P\",\"HGNC\",\"HGNC:38034\",\"ENST00000488147.1:n.575-5T>C\",NA,NA,\"LOW\",\"5/10\",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,-1,NA,\"ENST00000488147\",NA,NA,NA,\"G\"),(1,NA,NA,\"miRNA\",1,NA,64,64,NA,NA,NA,[\"mature_miRNA_variant\"],NA,NA,\"1/1\",\"ENSG00000278267\",NA,\"MIR6859-1\",\"HGNC\",\"HGNC:50039\",\"ENST00000619216.1:n.64T>C\",NA,NA,\"MODIFIER\",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,-1,NA,\"ENST00000619216\",NA,NA,NA,\"G\")]\"SNV\"00\nchr1:17379[\"G\",\"A\"]4.65e-012.92e+008.72e-024TrueFalse[(\"ENSG00000278267.1\",\"ENSG00000278267.1\",\"MIR6859-1\",\"miRNA\",\"3\",\"gene\",\".\",\"-\",\".\"),(\"ENSG00000227232.5\",\"ENSG00000227232.5\",\"WASH7P\",\"unprocessed_pseudogene\",\"2\",\"gene\",\".\",\"-\",\".\")]\"GRCh38\"\"G/A\"NA[(NA,NA,NA,NA,\"G/A\",NA,NA,NA,17379,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,\"rs754322362\",NA,NA,NA,NA,NA,NA,NA,17379,1)]NA17379\".\"\"chr1   17379   .   G   A   .   .   GT\"NA\"mature_miRNA_variant\"NANA\"chr1\"173791[(1,NA,NA,\"transcribed_unprocessed_pseudogene\",NA,NA,NA,NA,NA,NA,NA,[\"downstream_gene_variant\"],3709,NA,NA,\"ENSG00000223972\",NA,\"DDX11L1\",\"HGNC\",\"HGNC:37102\",NA,NA,NA,\"MODIFIER\",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,1,NA,\"ENST00000450305\",NA,NA,NA,\"A\"),(1,NA,NA,\"processed_transcript\",1,NA,NA,NA,NA,NA,NA,[\"downstream_gene_variant\"],2970,NA,NA,\"ENSG00000223972\",NA,\"DDX11L1\",\"HGNC\",\"HGNC:37102\",NA,NA,NA,\"MODIFIER\",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,1,NA,\"ENST00000456328\",NA,1,NA,\"A\"),(1,NA,NA,\"unprocessed_pseudogene\",1,NA,NA,NA,NA,NA,NA,[\"intron_variant\",\"non_coding_transcript_variant\"],NA,NA,NA,\"ENSG00000227232\",NA,\"WASH7P\",\"HGNC\",\"HGNC:38034\",\"ENST00000488147.1:n.575-11C>T\",NA,NA,\"MODIFIER\",\"5/10\",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,-1,NA,\"ENST00000488147\",NA,NA,NA,\"A\"),(1,NA,NA,\"miRNA\",1,NA,58,58,NA,NA,NA,[\"mature_miRNA_variant\"],NA,NA,\"1/1\",\"ENSG00000278267\",NA,\"MIR6859-1\",\"HGNC\",\"HGNC:50039\",\"ENST00000619216.1:n.58C>T\",NA,NA,\"MODIFIER\",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,-1,NA,\"ENST00000619216\",NA,NA,NA,\"A\")]\"SNV\"00\nchr1:17403[\"A\",\"G\"]1.51e-011.41e-017.07e-015TrueFalse[(\"ENSG00000278267.1\",\"ENSG00000278267.1\",\"MIR6859-1\",\"miRNA\",\"3\",\"gene\",\".\",\"-\",\".\"),(\"ENSG00000227232.5\",\"ENSG00000227232.5\",\"WASH7P\",\"unprocessed_pseudogene\",\"2\",\"gene\",\".\",\"-\",\".\")]\"GRCh38\"\"A/G\"NA[(NA,NA,NA,NA,\"A/G\",NA,NA,NA,17403,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,\"rs111588939\",NA,NA,NA,NA,NA,NA,NA,17403,1)]NA17403\".\"\"chr1   17403   .   A   G   .   .   GT\"NA\"non_coding_transcript_exon_variant\"NANA\"chr1\"174031[(1,NA,NA,\"transcribed_unprocessed_pseudogene\",NA,NA,NA,NA,NA,NA,NA,[\"downstream_gene_variant\"],3733,NA,NA,\"ENSG00000223972\",NA,\"DDX11L1\",\"HGNC\",\"HGNC:37102\",NA,NA,NA,\"MODIFIER\",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,1,NA,\"ENST00000450305\",NA,NA,NA,\"G\"),(1,NA,NA,\"processed_transcript\",1,NA,NA,NA,NA,NA,NA,[\"downstream_gene_variant\"],2994,NA,NA,\"ENSG00000223972\",NA,\"DDX11L1\",\"HGNC\",\"HGNC:37102\",NA,NA,NA,\"MODIFIER\",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,1,NA,\"ENST00000456328\",NA,1,NA,\"G\"),(1,NA,NA,\"unprocessed_pseudogene\",1,NA,NA,NA,NA,NA,NA,[\"intron_variant\",\"non_coding_transcript_variant\"],NA,NA,NA,\"ENSG00000227232\",NA,\"WASH7P\",\"HGNC\",\"HGNC:38034\",\"ENST00000488147.1:n.575-35T>C\",NA,NA,\"MODIFIER\",\"5/10\",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,-1,NA,\"ENST00000488147\",NA,NA,NA,\"G\"),(1,NA,NA,\"miRNA\",1,NA,34,34,NA,NA,NA,[\"non_coding_transcript_exon_variant\"],NA,NA,\"1/1\",\"ENSG00000278267\",NA,\"MIR6859-1\",\"HGNC\",\"HGNC:50039\",\"ENST00000619216.1:n.34T>C\",NA,NA,\"MODIFIER\",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,-1,NA,\"ENST00000619216\",NA,NA,NA,\"G\")]\"SNV\"00\nchr1:17408[\"C\",\"G\"]-8.03e-021.80e-016.71e-014TrueFalse[(\"ENSG00000278267.1\",\"ENSG00000278267.1\",\"MIR6859-1\",\"miRNA\",\"3\",\"gene\",\".\",\"-\",\".\"),(\"ENSG00000227232.5\",\"ENSG00000227232.5\",\"WASH7P\",\"unprocessed_pseudogene\",\"2\",\"gene\",\".\",\"-\",\".\")]\"GRCh38\"\"C/G\"NA[(NA,NA,NA,NA,\"C/G\",NA,NA,NA,17408,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,\"rs747093451\",NA,NA,NA,NA,NA,NA,NA,17408,1)]NA17408\".\"\"chr1  17408   .   C   G   .   .   GT\"NA\"non_coding_transcript_exon_variant\"NANA\"chr1\"174081[(1,NA,NA,\"transcribed_unprocessed_pseudogene\",NA,NA,NA,NA,NA,NA,NA,[\"downstream_gene_variant\"],3738,NA,NA,\"ENSG00000223972\",NA,\"DDX11L1\",\"HGNC\",\"HGNC:37102\",NA,NA,NA,\"MODIFIER\",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,1,NA,\"ENST00000450305\",NA,NA,NA,\"G\"),(1,NA,NA,\"processed_transcript\",1,NA,NA,NA,NA,NA,NA,[\"downstream_gene_variant\"],2999,NA,NA,\"ENSG00000223972\",NA,\"DDX11L1\",\"HGNC\",\"HGNC:37102\",NA,NA,NA,\"MODIFIER\",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,1,NA,\"ENST00000456328\",NA,1,NA,\"G\"),(1,NA,NA,\"unprocessed_pseudogene\",1,NA,NA,NA,NA,NA,NA,[\"intron_variant\",\"non_coding_transcript_variant\"],NA,NA,NA,\"ENSG00000227232\",NA,\"WASH7P\",\"HGNC\",\"HGNC:38034\",\"ENST00000488147.1:n.575-40G>C\",NA,NA,\"MODIFIER\",\"5/10\",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,-1,NA,\"ENST00000488147\",NA,NA,NA,\"G\"),(1,NA,NA,\"miRNA\",1,NA,29,29,NA,NA,NA,[\"non_coding_transcript_exon_variant\"],NA,NA,\"1/1\",\"ENSG00000278267\",NA,\"MIR6859-1\",\"HGNC\",\"HGNC:50039\",\"ENST00000619216.1:n.29G>C\",NA,NA,\"MODIFIER\",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,-1,NA,\"ENST00000619216\",NA,NA,NA,\"G\")]\"SNV\"00\nchr1:69735[\"A\",\"G\"]7.29e-023.18e-028.59e-015TrueFalse[(\"ENSG00000186092.6\",\"ENSG00000186092.6\",\"OR4F5\",\"protein_coding\",\"2\",\"gene\",\".\",\"+\",\".\")]\"GRCh38\"\"A/G\"NA[(NA,NA,NA,NA,\"A/G\",NA,NA,NA,69735,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,\"rs1245478362\",NA,NA,NA,NA,NA,NA,NA,69735,1)]NA69735\".\"\"chr1  69735   .   A   G   .   .   GT\"NA\"synonymous_variant\"NANA\"chr1\"697351[(1,\"L\",\"P2\",\"protein_coding\",NA,\"CCDS30547.1\",681,681,645,645,\"ctA/ctG\",[\"synonymous_variant\"],NA,[(\"Gene3D\",\"1\"),(\"Pfam\",\"PF13853\"),(\"PROSITE_profiles\",\"PS50262\"),(\"PANTHER\",\"PTHR26451\"),(\"PANTHER\",\"PTHR26451\"),(\"Superfamily\",\"SSF81321\"),(\"Transmembrane_helices\",\"TMhelix\"),(\"CDD\",\"cd15226\")],\"1/1\",\"ENSG00000186092\",NA,\"OR4F5\",\"HGNC\",\"HGNC:14825\",\"ENST00000335137.4:c.645A>G\",\"ENSP00000334393.3:p.Leu215=\",NA,\"LOW\",NA,NA,NA,NA,NA,NA,NA,NA,215,215,\"ENSP00000334393\",NA,NA,1,NA,\"ENST00000335137\",NA,NA,NA,\"G\"),(1,\"L\",\"A2\",\"protein_coding\",1,NA,768,768,708,708,\"ctA/ctG\",[\"synonymous_variant\"],NA,[(\"Gene3D\",\"1\"),(\"PROSITE_profiles\",\"PS50262\"),(\"Transmembrane_helices\",\"TMhelix\"),(\"Superfamily\",\"SSF81321\"),(\"PANTHER\",\"PTHR26451\"),(\"PANTHER\",\"PTHR26451\"),(\"Pfam\",\"PF13853\"),(\"CDD\",\"cd15226\")],\"3/3\",\"ENSG00000186092\",NA,\"OR4F5\",\"HGNC\",\"HGNC:14825\",\"ENST00000641515.2:c.708A>G\",\"ENSP00000493376.2:p.Leu236=\",NA,\"LOW\",NA,NA,NA,NA,NA,NA,NA,NA,236,236,\"ENSP00000493376\",NA,NA,1,NA,\"ENST00000641515\",NA,NA,NA,\"G\")]\"SNV\"00\nshowing top 10 rows"
  },
  {
    "objectID": "02-gwas.html#export-gwas-table-export-mt-to-bgen",
    "href": "02-gwas.html#export-gwas-table-export-mt-to-bgen",
    "title": "6  GWAS using Hail",
    "section": "6.11 Export GWAS Table, Export MT to BGEN",
    "text": "6.11 Export GWAS Table, Export MT to BGEN\nWe can convert our Hail GWAS table and bring it into memory using the .to_pandas() method. Then we can write it to HDFS using .to_csv().\n\n# Convert to Pandas DataFrame\nann_gwas_pd = ann_gwas_tb.to_pandas()\n# Save as CSV file to Hadoop File System\nann_gwas_pd.to_csv(\"gwas-results.csv\")\n\nOur gwas-results.csv file is now in the Hadoop File System. Then we can run hdfs get to pull it onto the disk of the Driver node. Then we can use dx upload to get it into project storage.\n\n%%bash\n\n#Fetch results file from Hadoop File System and save to Driver Node Storage\nhdfs dfs -get gwas-results.csv\n#Upload resutls file to Project Storage\ndx upload gwas-results.csv --destination \"/users/tladeras/\"\n\nIf we want to output BGEN files, there is no direct way for the notebook to write data into the project, so we will first write into HDFS (see https://documentation.dnanexus.com/developer/apps/developing-spark-apps#spark-cluster-management-software).\nAfter writing out the BGEN files to HDFS, we can then move the data to the project in the next step as above.\nNote that we can also do this in a single step by using dxFUSE in -limitedWrite mode.\nAdditional documentation: https://hail.is/docs/0.2/methods/impex.html#hail.methods.export_bgen\n\n# Create a set of unique chromosomes found in MT\n\nchr_set = mt.aggregate_rows(hl.agg.collect_as_set(mt.locus.contig))\n\n\n# Filter MT to a single chromosome and write out as BGEN file to HDFS as a single file for each chromosome in the MT\nfor chrom in chr_set:\n    filtered_mt = hl.filter_intervals(mt, [hl.parse_locus_interval(chrom, reference_genome=\"GRCh38\"),])\n    hl.export_bgen(filtered_mt, f\"{chrom}\")\n\n\n%%bash\n# Copy BGEN files from HDFS to the JupyterLab execution environment file system\n\nhdfs dfs -get ./*.bgen .\n\n\n%%bash\n# Copy SAMPLE files from HDFS to the JupyterLab execution environment file system\n\nhdfs dfs -get ./*.sample .\n\n\n%%bash\n# Upload BGEN and SAMPLE files to project\n\ndx upload *.bgen\ndx upload *.sample\n\nFinally, we can save our filtered MatrixTable to dnax so we can use it later.\n\n# Store Table in DNAXc\n\nimport dxpy\ndb_name = \"db_mt_test2\"\ntb_name = \"filtered.mt\"\n\n# find database ID of newly created database using a dxpy methodc\ndb_uri = dxpy.find_one_data_object(name=f\"{db_name}\", classname=\"database\")['id']\nurl = f\"dnax://{db_uri}/{tb_name}\"\n\n# Before this step, the Hail Table is just an object in memory. To persist it and be able to access \n# it later, the notebook needs to write it into a persistent filesystem (in this case DNAX).\n# See https://hail.is/docs/0.2/hail.Table.html#hail.Table.write for additional documentation.\nqc_tb.write(url) # Note: output should describe size of Table (i.e. number of rows, partitions)"
  }
]