{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# `dx extract_dataset` in R {#sec-dxex-R}\n","<hr/>\n","***As-Is Software Disclaimer***\n","\n","This content in this repository is delivered “As-Is”. Notwithstanding anything to the contrary, DNAnexus will have no warranty, support, liability or other obligations with respect to Materials provided hereunder.\n","\n","<hr/>\n","\n","This notebook demonstrates usage of the dx command `extract_dataset` for:\n","* Retrieval of Apollo-stored data, as referenced within entities and fields of a Dataset or Cohort object on the platform\n","* Retrieval of the underlying data dictionary files used to generate a Dataset object on the platform\n","\n","<a href=\"https://github.com/dnanexus/OpenBio/blob/master/LICENSE.md\">MIT License</a> applies to this notebook."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Preparing your environment\n","### Launch spec:\n","\n","* App name: JupyterLab with Python, R, Stata, ML ()\n","* Kernel: R\n","* Instance type: Spark Cluster - mem1_ssd1_v2_x2, 4 nodes \n","* Snapshot: `/.Notebook_snapshots/jupyter_snapshot.gz`\n","* Cost: < $0.2\n","* Runtime: =~ 10 min\n","* Data description: Input for this notebook is a v3.0 Dataset or Cohort object ID"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Install dxpy\n","extract_dataset requires dxpy version >= 0.329.0. If running the command from your local environment (i.e. off of the DNAnexus platform), it may be required to also install pandas. For example, pip3 install -U dxpy[pandas]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["system(\"pip3 show dxpy\", intern = TRUE)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Install tidyverse for data processing\n","\n","Quick note - you will need to read the licenses for the tidyverse in order to make sure whether you and your group are comfortable with the licensing terms.\n","\n","If you loaded the snapshot in this project, all of these packages and dependencies are also installed, so you don't need to install them again."]},{"cell_type":"code","execution_count":1,"metadata":{"trusted":true,"vscode":{"languageId":"r"}},"outputs":[{"name":"stderr","output_type":"stream","text":["Installing packages into ‘/usr/local/lib/R/site-library’\n","(as ‘lib’ is unspecified)\n","\n","also installing the dependencies ‘bit’, ‘bit64’, ‘progress’, ‘hms’, ‘vroom’, ‘tzdb’, ‘vctrs’, ‘reactR’, ‘snakecase’\n","\n","\n","Downloading GitHub repo laderast/xvhelper@HEAD\n","\n"]},{"name":"stdout","output_type":"stream","text":["Rcpp       (1.0.9 -> 1.0.10) [CRAN]\n","utf8       (1.2.2 -> 1.2.3 ) [CRAN]\n","fansi      (1.0.3 -> 1.0.4 ) [CRAN]\n","stringi    (1.7.8 -> 1.7.12) [CRAN]\n","tibble     (3.1.8 -> 3.2.0 ) [CRAN]\n","purrr      (0.3.5 -> 1.0.1 ) [CRAN]\n","cli        (3.4.1 -> 3.6.0 ) [CRAN]\n","timechange (0.1.1 -> 0.2.0 ) [CRAN]\n","png        (NA    -> 0.1-8 ) [CRAN]\n","here       (NA    -> 1.0.1 ) [CRAN]\n","RcppTOML   (NA    -> 0.2.2 ) [CRAN]\n","tidyr      (1.2.1 -> 1.3.0 ) [CRAN]\n","lubridate  (1.9.0 -> 1.9.2 ) [CRAN]\n","reticulate (NA    -> 1.28  ) [CRAN]\n"]},{"name":"stderr","output_type":"stream","text":["Installing 14 packages: Rcpp, utf8, fansi, stringi, tibble, purrr, cli, timechange, png, here, RcppTOML, tidyr, lubridate, reticulate\n","\n","Installing packages into ‘/usr/local/lib/R/site-library’\n","(as ‘lib’ is unspecified)\n","\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m──\u001b[39m \u001b[36mR CMD build\u001b[39m \u001b[36m─────────────────────────────────────────────────────────────────\u001b[39m\n","* checking for file ‘/tmp/RtmpJskvRy/remotes734410c4e0/laderast-xvhelper-6e0873a/DESCRIPTION’ ... OK\n","* preparing ‘xvhelper’:\n","* checking DESCRIPTION meta-information ... OK\n","* checking for LF line-endings in source and make files and shell scripts\n","* checking for empty or unneeded directories\n","* building ‘xvhelper_0.0.100.tar.gz’\n","\n"]},{"name":"stderr","output_type":"stream","text":["Installing package into ‘/usr/local/lib/R/site-library’\n","(as ‘lib’ is unspecified)\n","\n"]}],"source":["install.packages(c(\"readr\", \"stringr\", \"dplyr\", \"glue\", \"reactable\", \"janitor\", \"remotes\"))\n","remotes::install_github(\"laderast/xvhelper\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Import packages"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["library(dplyr)\n","library(readr)\n","library(stringr)\n","library(glue)\n","library(reactable)\n","library(xvhelper)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 1. Assign environment variables"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["# The referenced Dataset is private and provided only to demonstrate an example input. The user will need to supply a permissible and valid record-id\n","# Assign project-id of dataset\n","# Assign dataset record-id\n","rid <- 'record-G5Ky4Gj08KQYQ4P810fJ8qPp'\n","# Assign joint dataset project-id:record-id\n","dataset <- rid"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 2. Call “dx extract_dataset” using a supplied dataset"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We'll use the `{glue}` package to put our bash commands together for `dx extract_dataset`, and use `system()` to execute our bash code.\n","\n","`glue::glue()` has the advantage of not needing to `paste()` together strings. The string substitution is cleaner."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["cmd <- glue::glue(\"dx extract_dataset {dataset} -ddd\")\n","\n","cmd"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Let's execute our command using `system()` and then we will list the files that result using `list.files()`. We generate three files in the directory in JupyterLab storage:\n","\n","- *dataset_name*`.codings.csv`\n","- *dataset_name*`.data_dictionary.csv`\n","- *dataset_name*`.entity_dictionary.csv`"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["system(cmd)\n","list.files()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Preview data in the three dictionary (*.csv) files"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["#codings_file <- system(\"ls *.codings.csv\", intern = TRUE)\n","codings_file <- list.files(pattern=\"*.codings.csv\")\n","codings_df <- read_csv(codings_file, show_col_types = FALSE)\n","head(codings_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["entity_dict_file <- system(\"ls *.entity_dictionary.csv\", intern=TRUE)\n","entity_dict_df <- read_csv(entity_dict_file, show_col_types = FALSE)\n","head(entity_dict_df)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Understanding the Data Dictionary File\n","\n","The data dictionary is the glue for the entire dataset. It maps:\n","\n","- Entity to Fields\n","- Fields to Codings\n","- Entity to Entity\n","\n","We'll use the data dictionary to understand how to building our list of fields, and later, we'll join it to the codings file to build a list of fields and their coded values.\n","\n","There are more columns to the data dictionary, but let's first see the `entity`, `name`, `title`, and `type` columns:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["#data_dict_file <- system(\"ls *.data_dictionary.csv\", intern=TRUE)\n","data_dict_file <- list.files(pattern=\"*.data_dictionary.csv\")\n","data_dict_df <- read_csv(data_dict_file, show_col_types = FALSE)\n","data_dict_df <- data_dict_df \n","\n","data_dict_df %>%\n","        select(entity, name, title, type) %>%\n","        head()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 3. Parse returned metadata and extract entity/field names"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Let's search for some fields. We want the following fields:\n","\n","- `Coffee intake | instance 0`\n","- `Sex` (Gender)\n","- `Smoked cigarette or pipe within last hour | Instance 0`\n","\n","We can use the `{reactable}` package to make a searchable table of the data dictionary. This will help in finding fields.\n","\n","Note the search box in the top right of the table - when we have many fields, we can use the search box to find fields of interest. Try searching for `Coffee intake` and see what fields pop up."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["data_dict_df <- data_dict_df %>%\n","    relocate(name, title) %>%\n","    mutate(ent_field = glue::glue(\"{entity}.{name}\"))\n","\n","basic_data_dict <- data_dict_df |>\n","                    select(title, name, entity, ent_field, coding_name, is_multi_select, is_sparse_coding)\n","\n","reactable::reactable(basic_data_dict, searchable = TRUE)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Another strategy for searching fields: we can use `grepl` within `dplyr::filter()` to search for fields that match our criteria.\n","\n","Note we're chaining the `grepl` statements with an OR `|`.\n","\n","We're also concatenating `entity` and `name` to a new variable, `ent_field`, which we'll use when we specify our list of fields."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["filtered_dict <- data_dict_df %>%\n","    filter(grepl(\"Coffee type\", title) | \n","           grepl(\"Sex\", title) | \n","           grepl(\"Smoked\", title) | \n","           grepl(\"Age at recruitment\", title) |\n","           grepl(\"main ICD10\", title) |\n","           grepl(\"Types of transport\", title)\n","          ) %>%\n","    arrange(title) \n","\n","filtered_dict %>%\n","    select(name, title, ent_field)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Let's use this subset of fields - we'll pull the `ent_field` column, and paste it together into a single comma delimited string using `paste`:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["field_list <- filtered_dict %>%\n","    pull(ent_field)\n","\n","#field_list <- field_list[200:210]\n","field_list <- paste(field_list, collapse = \",\")\n","field_list"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 4. Use extracted entity and field names as input to the called function, “dx extract_dataset” and extract data\n","\n","Again, we'll use `glue()` here for cleaner string substitution.\n","\n","We'll extract the cohort information to a file called `cohort_data.csv` and work with this file for the rest of the notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["cohort <- \"record-G5Ky4Gj08KQYQ4P810fJ8qPp\"\n","cohort_template <- \"dx extract_dataset {cohort} --fields {field_list} -o cohort_data.csv\"\n","cmd <- glue::glue(cohort_template)\n","\n","cmd\n","\n","system(cmd)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Preview data in the retrieved data file\n","\n","We'll see that the retrieved data contains the integer and character codes. These must be decoded (see below):"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["data_df <- read_csv(\"cohort_data.csv\", show_col_types = FALSE)\n","head(data_df)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Decoding columns with xvhelper\n","\n","xvhelper is a little R package that will return the actual values of the returned data.\n","\n","To use it, you build a `coded_col_df` using `merge_coding_data_dict()` and then translate the categorical columns to values using `decode_categories()`, and then change the column names to R friendly clean ones using `decode_column_names()`.\n","\n","Note that we need to run `decode_df()` before we run `decode_category()`"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["#install via remotes::install_github()\n","#install.packages(\"remotes\")\n","remotes::install_github(\"laderast/xvhelper\")\n","\n","library(xvhelper)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["coded_col_df <- xvhelper::merge_coding_data_dict(coding_dict = codings_df, data_dict = data_dict_df)\n","\n","decoded <- data_df %>%\n","    xvhelper::decode_single(coded_col_df) |>\n","    xvhelper::decode_multi_purrr(coded_col_df) |>\n","    xvhelper::decode_column_names(coded_col_df, r_clean_names = FALSE)\n","    \n","head(decoded)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["write.csv(decoded, file=\"cohort_decoded.csv\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Save Output to Project"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["system(\"dx upload *.csv --destination /users/tladeras/\")"]}],"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"codemirror_mode":"r","file_extension":".r","mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"4.2.0"}},"nbformat":4,"nbformat_minor":5}
