[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Best Practices on the DNAnexus Platform",
    "section": "",
    "text": "Preface\nThis is a companion book to Bash for Bioinformatics.\nIn this book we attempt to highlight best practices on the DNAnexus platform given some previous knowledge."
  },
  {
    "objectID": "index.html#learning-objectives",
    "href": "index.html#learning-objectives",
    "title": "Best Practices on the DNAnexus Platform",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAfter reading this book and doing the exercises, you should be able to:\n\nExplain the object based filesystem on the DNAnexus platform and how it differs from Linux/Unix POSIX filesystems.\nExplain key details of the UKB Research Analysis Platform and how these details impact your work.\nUtilize Projects and Organizations to effectively manage data for your group.\nUse Spark to connect, extract, and manipulate Apollo Datasets on the platform.\nUtilize Hail to load, query, model, annotate, and visualize large-scale genomics data such as the Exome Data on the UK Biobank Research Analysis Platform.\nBuild native DNAnexus apps effectively that manage inputs, outputs, and work with batch mode.\nUtilize existing and build native DNAnexus workflows using Workflow Description Language (WDL).\nExecute NextFlow pipelines on the DNAnexus Platform.\n\nOur goal is to bring information together in a task-oriented format to achieve things on the platform."
  },
  {
    "objectID": "index.html#four-levels-of-using-dnanexus",
    "href": "index.html#four-levels-of-using-dnanexus",
    "title": "Best Practices on the DNAnexus Platform",
    "section": "Four Levels of Using DNAnexus",
    "text": "Four Levels of Using DNAnexus\nOne way to approach learning DNAnexus is to think about the skills you need to process a number of files. Ben Busby has noted there are 4 main skill levels in processing files on the DNAnexus platform:\n\n\n\n\n\n\n\n\nLevel\n# of Files\nSkill\n\n\n\n\n1\n1\nInteractive Analysis (Cloud Workstation, JupyterLab)\n\n\n2\n1-50 Files\ndx run, Swiss Army Knife\n\n\n3\n50-1000 Files\nBuilding your own apps\n\n\n4\n1000+ Files, multiple steps\nUsing WDL (Workflow Description Language)\n\n\n\nWe’ll be covering mostly level 3 and 4 in this book. But you will need to be at level 2 before you can tackle these topics.\nThe key is to gradually build on your skills."
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Best Practices on the DNAnexus Platform",
    "section": "Prerequisites",
    "text": "Prerequisites\nBefore you tackle this book, you should be able to accomplish the following:\n\nUtilize Basic Bash Skills for DNAnexus\nUnderstand the basic architecture of Cloud Computing\nKnow how to edit and utilize JSON Files\nBe familiar with dx-toolkit commands, including:\n\ndx run\ndx find data and dx find jobs/dx watch\n\n\nWe recommend reviewing Bash for Bioinformatics if you need to brush up on these prerequisite skills."
  },
  {
    "objectID": "index.html#want-to-be-a-contributor",
    "href": "index.html#want-to-be-a-contributor",
    "title": "Best Practices on the DNAnexus Platform",
    "section": "Want to be a Contributor?",
    "text": "Want to be a Contributor?\nThis is the first draft of this book. It’s not going to be perfect, and we need help.\nIf you find a problem/issue or have a question, you can file it as an issue using this link.\nIn your issue, please note the following:\n\nYour Name\nWhat your issue was\nWhich section, and line you found problematic or wouldn’t run\n\nIf you’re quarto/GitHub savvy, you can fork and file a pull request for typos/edits. If you’re not, you can file an issue.\nJust be aware that this is not my primary job - I’ll try to be as responsive as I can."
  },
  {
    "objectID": "10-object-based-file-systems.html#terminology",
    "href": "10-object-based-file-systems.html#terminology",
    "title": "1  Object Based File Systems",
    "section": "1.1 Terminology",
    "text": "1.1 Terminology\n\nPOSIX - The filesystem that linux is based on. POSIX filesystems have paths and files.\nData - The actual file contents. For example, for a CSV file, the actual header and rows correspond to the data.\nMetadata - information that is not part of the data, but is associated with the data. For our CSV file, some examples of metadata are the file permissions (who can access the file), the creator, and the creation date.\nObject - A contiguous set of memory that contains both the data and metadata. Has a unique identifier.\nUnique Identifier - the actual “address” for accessing the file. Unique to the file object when it is created. Does not change for the entire lifecycle of an object.\nDatabase Engine - software that allows for rapid searching and retrieving objects."
  },
  {
    "objectID": "10-object-based-file-systems.html#review-posix-filesystems",
    "href": "10-object-based-file-systems.html#review-posix-filesystems",
    "title": "1  Object Based File Systems",
    "section": "1.2 Review: POSIX Filesystems",
    "text": "1.2 Review: POSIX Filesystems\nYou may be familar with directory based filesystems as a way to organize your data. The main way to find/refer to a file is through its path. What does this mean?\nFor example, if my file is called chr1.vcf.gz and it is located in the directory /Geno_Data, we’d refer to it using the full path:\n/Geno_Data/chr1.vcf.gz\nPaths are the main way we organize files for directory based systems. This information is external to a file. Most importantly, we use directories to organize and group files logically. For example, we might have our R script and the data it analyzes in the same directory.\nFor a file in a POSIX-based filesystem, the path needs to be unique. If they are not, there are rules for whether to replace that file with a new one, or to save both versions. For example, we can’t have two files named:\n/Geno_Data/chr1.vcf.gz\nIn the same folder. That violates our ability to find a file in the system.\nhttps://grimoire.carcano.ch/blog/posix-compliant-filesystems/\nhttps://www.kernel.org/doc/html/latest/filesystems/fuse.html"
  },
  {
    "objectID": "10-object-based-file-systems.html#object-based-filesystems-are-different",
    "href": "10-object-based-file-systems.html#object-based-filesystems-are-different",
    "title": "1  Object Based File Systems",
    "section": "1.3 Object Based Filesystems are different",
    "text": "1.3 Object Based Filesystems are different\nIn contrast, object-based filesystems do not organize data like a folder based system. Each file object (such as a csv file, or a BAM file) has a unique identifier that identifies the data object. This unique identifier (like file-aagejFHEJSEI) serves as the main way to locate the data object, rather than the path.\nHowever, file objects also have metadata that can be attached to them. This metadata can be:\n\n\n\nMetadata Type\nExample\nCode Example\n\n\n\n\nID\nproject-XXXXX:file-YYYYYY\ndx mv file-YYYY raw_data/\n\n\nname\nchr1.tar.gz\ndx find data --name chr1.tar.gz\n\n\nPath\n/raw_data/chr1.tar.gz\n\n\n\nCreation Date\n\n\n\n\nTags\nreport, run1\n\n\n\nProperties\neid = 1153xxx\n\n\n\n\nOn the DNAnexus platform, both the filename and its path are considered metadata. This metadata is part of the object, along with the data portion of the object.\nImportantly, folders are not considered objects on the platform - folders only exist within the metadata for the file objects.\nThe other issue is that the metadata for an object has no requirements to be unique. Which means you can have duplicates with the same file name in the same folder.\nI know, this can be very distressing for most people. You can have two objects with the same file name, but they are considered distinct objects because they have unique identifiers.\nhttps://www.ibm.com/cloud/blog/object-vs-file-vs-block-storage"
  },
  {
    "objectID": "10-object-based-file-systems.html#comparing-posix-and-object-based-file-systems",
    "href": "10-object-based-file-systems.html#comparing-posix-and-object-based-file-systems",
    "title": "1  Object Based File Systems",
    "section": "1.4 Comparing POSIX and Object Based File Systems",
    "text": "1.4 Comparing POSIX and Object Based File Systems\n\n\n\nConcept\nPOSIX File System\nObject-Based System\n\n\n\n\nFile ID\nRepresented by Full Path\nRepresented by Object ID\n\n\nStorage\nData with limited metadata\nMetadata+Data\n\n\nPath/Filename\nMust be Unique\nCan be duplicated\n\n\nMetadata\nLimited\nRich, can be freely modified"
  },
  {
    "objectID": "10-object-based-file-systems.html#tracing-the-journey-of-a-file-object-onto-the-platform",
    "href": "10-object-based-file-systems.html#tracing-the-journey-of-a-file-object-onto-the-platform",
    "title": "1  Object Based File Systems",
    "section": "1.5 Tracing the journey of a file object onto the platform",
    "text": "1.5 Tracing the journey of a file object onto the platform\nWhen a file uploaded, file objects go through three stages before they are available. These stages are:\n\nOpen - Files are still being transferred via dx upload or the Upload Agent ua.\nClosing - File objects stay in this state for no longer than 8-10 seconds.\nClosed - Files are now available to be utilized on the platform, either with an app, workflow, or downloaded.\n\n% dx describe file-FpQKQpQ0Fgk3gQZz3gPXQj7x\nResult 1:\nID                    file-FpQKQpQ0Fgk3gQZz3gPXQj7x\nClass                 file\nProject               project-GJ496B00FZfxPV5VG36FybvY\nFolder                /data\nName                  NA12878.bai\nState                 closed\nVisibility            visible\nTypes                 -\nProperties            -\nTags                  -\nOutgoing links        -\nCreated               Wed Apr 22 17:59:22 2020\nCreated by            emiai\n via the job          job-FpQGX780FgkG4bGz86zZk04V\nLast modified         Thu Oct 13 15:38:04 2022\nMedia type            application/octet-stream\narchivalState         \"live\"\nSize                  2.09 MB, sponsored by DNAnexus\ncloudAccount          \"cloudaccount-dnanexus\""
  },
  {
    "objectID": "10-object-based-file-systems.html#copying-files-from-one-project-to-another",
    "href": "10-object-based-file-systems.html#copying-files-from-one-project-to-another",
    "title": "1  Object Based File Systems",
    "section": "1.6 Copying Files from One Project to Another",
    "text": "1.6 Copying Files from One Project to Another\nCopying has an important definition on the platform: it means copying a file from one project to another project. It doesn’t refer to duplicating a file within a project.\nWhen we copy a file from one project to another, a new physical copy is not made. The reference to the original file is copied. The data is identical and points to the same location on the platform, and the metadata is copied to the new project.\nThis is quite nice in that you are not doubly charged for storage on the platform. You do have the ability to clone files into a different project - that way a physical copy of the data is created.\nOnce the metadata is copied into the new project, there is no syncing of the metadata between the two projects. User 1 is free to modify the metadata in Project A and changes are not made to the metadata in Project B.\n\n1.6.1 Advantages of Object Based Filesystems\nThe DNAnexus platform is only one example of an object-based filesystem. Other examples include Amazon S3 (Simple Storage Service), which is one of the biggest examples. Why does the world run on object based filesystems? There are a lot of advantages.\n\nHighly scalable. This is the main reason given for using an object-based system. Given that unique identifier, the data part of the object can be very large.\nFast Retrieval. Object-based filesystems let us work with arbitrarily large file sizes, and we can actually stream files to and from workers.\nImproved search speed. You can attach various database engines to a set of objects and rapidly search through them. An example of such an engine is Snowflake.\nFile operations are simplified. Compared to POSIX filesystems, there are only a few object filesystem commands: PUT, GET, DELETE, POST, HEAD.\n\n\n\n1.6.2 Disadvantages of Object Based Filesystems\nComing from folder based filesystems, it can be a bit of a mind-bender getting used to object-based filesystems. Some of the disadvantages of Object Based Filesystems include:\n\nObjects are immutable. You can’t edit an object in place. If you modify a file on a worker, you can’t overwrite the original file object. A new file object must be created.\nYou have to be careful when generating outputs. You can end up with two different objects with the same filename, and it can be some work to disambiguate these objects.\nIt’s confusing. You can actually have two files with the same filename in the same folder, because it is part of the changeable metadaa. Disambiguating these two files without using file-ids can be difficult. There are rules that govern this.\nMetadata is much more important with file management. Seriously, use tags for everything, including jobs and files. It will make working with multiple files much easier. And if you are on UKB RAP, leverage the file property metadata (eid and field_id) to help you select the files you want to process."
  },
  {
    "objectID": "10-object-based-file-systems.html#always-add-project-ids-to-your-file-ids",
    "href": "10-object-based-file-systems.html#always-add-project-ids-to-your-file-ids",
    "title": "1  Object Based File Systems",
    "section": "1.7 Always add project IDs to your File IDs",
    "text": "1.7 Always add project IDs to your File IDs\nIn Bash for Bioinformatics, we already discovered one way of working with files on the platform: dx find data (?sec-dx-find). This is our main tool for selecting files with metadata.\nWhen we work with files outside of our current project, we might reference it by a file-id. Using file IDs by themselves are a global operation and we need to be careful when we use this!\nWhy is this so? There is a search tree when we use a file ID that is not in our project and without a project context. The search over metadata is looking for a file object based on just file ID.\n\nLook for the file in the current project\nLook at all files across all other projects\n\nIf you want to use the platform effectively, you want to avoid 2. at all costs, especially when working with a lot of files. The metadata server will take much longer to find your files.\nThe lesson here is when using file-ids, it is safer to put the project-id in front of your file id such as below:\nproject-XXXXX:file-YYYYYYYYY"
  },
  {
    "objectID": "10-object-based-file-systems.html#batch-tagging",
    "href": "10-object-based-file-systems.html#batch-tagging",
    "title": "1  Object Based File Systems",
    "section": "1.8 Batch Tagging",
    "text": "1.8 Batch Tagging\nRemember, we can leverage xargs for tagging multiple files:\n\ndx find data --name \"*bam\" --brief | xargs -I% sh -c \"dx tag % 'bam'\"\n\nAfter we do this, we can check whether our operation was successful. We can run:\n\ndx find data --tag bam --brief\n\nAnd here is our response:\nproject-GJ496B00FZfxPV5VG36FybvY:file-BZ9YGzj0x05b66kqQv51011q\nproject-GJ496B00FZfxPV5VG36FybvY:file-BZ9YGpj0x05xKxZ42QPqZkJY\nproject-GJ496B00FZfxPV5VG36FybvY:file-BQbXVY0093Jk1KVY1J082y7v\nproject-GJ496B00FZfxPV5VG36FybvY:file-FpQKQk00FgkGV3Vb3jJ8xqGV\n\nUsing the dx find data/xargs/dx tag combination with various input parameters to dx find data such as --name, --created-before, --created-after, --class, will help us to batch tag files and other objects on the platform."
  },
  {
    "objectID": "10-object-based-file-systems.html#use-case-use-tags-for-archiving",
    "href": "10-object-based-file-systems.html#use-case-use-tags-for-archiving",
    "title": "1  Object Based File Systems",
    "section": "1.9 Use Case: Use tags for archiving",
    "text": "1.9 Use Case: Use tags for archiving\nLet’s do something concrete and useful in our project: tag files we no longer need for archiving.\nSay there are files we want to archive. We can use dx tag or the DNAnexus UI to tag these files with a specific tag, such as to_archive. This can be done by users.\nAn administrator can then run a monthly job that archives the files with these tags using dx api <project-id> archive --tag to_archive.\n\n\n\n\n\n\nWhat about dxFUSE?\n\n\n\nYou might ask about the role of dxFUSE with the Object Based Filesystem.\nIn short, dxFUSE makes the Object Based Filesystem of DNAnexus act like a POSIX filesystem. Specifically, if there are multiple objects with the same name within a folder, it provides a way to specify these objects using file paths.\n\n\n\n\n\n\n\n\nPar-what-now?\n\n\n\nYou may have heard of Parquet files and wondered how they relate to file objects on the platform. They are a way to store data in a format called columnar storage.\nIt turns out it is faster for retrieval to store data not by rows, but by columns. This is really helpful because the data is sorted by data type and it’s easier to traverse for this reason.\nThere are a number of database engines that are fast at searching and traversing these types of files. Some examples are Snowflake and Apache Arrow.\nOn the DNAnexus platform, certain data objects (called Datasets) are actually stored in Parquet format, for rapid searching and querying using Apache Spark."
  },
  {
    "objectID": "12-project_management.html",
    "href": "12-project_management.html",
    "title": "2  Project and File Management",
    "section": "",
    "text": "3 Project Administration\nIn this section, we will talk about roles and what operations can be enabled/disabled in a project.\nOne of the ways to become a power user on the DNAnexus platform is to utilize tags when you generate output. This is important for reproducible analysis."
  },
  {
    "objectID": "12-project_management.html#learning-objectives",
    "href": "12-project_management.html#learning-objectives",
    "title": "2  Project and File Management",
    "section": "2.1 Learning Objectives",
    "text": "2.1 Learning Objectives\n\nExplain the role of organizations in project and member management\nUtilize different management strategies for administering your data to users\nDescribe the roles that can be assigned to projects and how they interact with them\nAnnotate your data with essential metadata such as tags and properties"
  },
  {
    "objectID": "12-project_management.html#checklist-for-successful-project-management",
    "href": "12-project_management.html#checklist-for-successful-project-management",
    "title": "2  Project and File Management",
    "section": "2.2 Checklist for Successful Project Management",
    "text": "2.2 Checklist for Successful Project Management\nBased on past customers and how they interact with projects, I’ve tried to put together a checklist for new customers starting from scratch.\nWe assume that you will have multiple users, and that there is an administrator is in charge of the organization.\n\n\n\nTask\nPerson Responsible\n\n\n\n\nCreation of an Org\nDNAnexus Support\n\n\nSetting up billing for org\nDNAnexus Support\n\n\nAssigning an Admin for the Org\nOrg Admin\n\n\nEnable Smart Reuse in Org\nOrg Admin\n\n\nCreating a base project for incoming data\nOrg Admin\n\n\nUploading Data into base project\nOrg Admin/Uploader/Project Admin\n\n\nInitial Portal Setup\nxVantage Team\n\n\nEnable Single Sign On\nDNAnexus Team\n\n\nCreating User Accounts for the Org\nOrg Admin\n\n\nCreating projects/Copying Data from the base project\nProject Admin\n\n\nSetting Project Permissions (Downloads/Uploads/etc)\nProject Admin\n\n\nAssigning Users to projects with Roles\nProject Admin\n\n\n\n\n\n\n\n\n\nOrg/Project Terminology\n\n\n\nHere is a very useful link in case you get lost in terms of all of the terminology associated with projects and orgs: https://documentation.dnanexus.com/getting-started/key-concepts/organizations#glossary-of-org-terms"
  },
  {
    "objectID": "12-project_management.html#starting-point-the-organization-or-org",
    "href": "12-project_management.html#starting-point-the-organization-or-org",
    "title": "2  Project and File Management",
    "section": "2.3 Starting Point: the Organization (or Org)",
    "text": "2.3 Starting Point: the Organization (or Org)\nBefore anything, you will need to create or have an organization (also known as an org) created for your group. This is usually done through contacting support (support@dnanexus.com).\nWhat is an organization on the platform? It is an entity on the platform that is associated with multiple users. There are a lot of operations at the Org level that make effective data and project management possible.\nCreating an org is important because an organization owns and administers the following:\n\nControl membership within your organization and access levels\nSet up for centralized Billing for your org\nDefault app access level in the organization\nDefault project access level in the organization\nEnable Smart Reuse for Organization\nAdminister the portal for your own organization\nAccess to files within projects administered by the org\nCreating an Audit Trail for the Organization\n\nThe Org Administrator has power over all projects and members in the Org. They are able to:\n\nAdd and remove members to an org and change their access level\nMake themselves members of any project in the Org\nChange the Spending Limit of the Org\nEnable PHI Data Protections\nEnable Smart Reuse"
  },
  {
    "objectID": "12-project_management.html#org-owned-projects",
    "href": "12-project_management.html#org-owned-projects",
    "title": "2  Project and File Management",
    "section": "2.4 Org-owned projects",
    "text": "2.4 Org-owned projects\nIn general, you will want to create projects within your org. This can be done by the org administrators. This simplifies billing for both file storage and compute.\nUsers can also create their own projects, but if they want to tie their project to the org’s billing, they will need to transfer ownership to the organization.\n\n\n\n\n\n\nPHI, Projects, and Orgs\n\n\n\nThere are many cases, especially when protected health information (PHI) is involved, where having a single organization doesn’t make sense.\nControlling such data through its own organization may make much more sense for your group. For very large datasets, we recommend working with your Customer Success representative for implementation strategies.\nAdditionally, project admins can enable PHI Restrictions within a project.\nEnabling an audit trail for the org also becomes very important, in order to be compliant with regulations such as 21 CFR Part 11. In short, an org admin will create a project for which the audit logs will be stored,"
  },
  {
    "objectID": "12-project_management.html#a-base-project-a-data-project-for-all-your-orgs-data",
    "href": "12-project_management.html#a-base-project-a-data-project-for-all-your-orgs-data",
    "title": "2  Project and File Management",
    "section": "2.5 A Base Project: A Data Project for All Your Org’s Data",
    "text": "2.5 A Base Project: A Data Project for All Your Org’s Data\n\n\n\n\n\n\nflowchart LR\n  A(Admin) -- manages--> B[Base Project\\ncontains all\\ndata]\n  B --Dispensed\\nby Admin-->C[Project A]\n  B --Dispensed\\nby Admin-->D[Project B]\n  B --Dispensed\\nby Admin-->E[Project C]\n  F(Uploader) --Uploads Data --> B\n\n\n\n\n\n\n\n\nFigure 2.1: Using a base project to manage your Org’s data. All other projects will be derived from this base project.\n\n\nOne strategy for administering projects within your organization is to have a base project that contains all of your data (Figure 2.1). Tagging incoming data as they are uploaded can simplify project management. Then using these tags, you can copy the relevant data files to your separate projects.\nThe primary advantage of having a base project is that it allows for centralized data management. The org administrator / base project administrator controls access to all files within the org.\nA related advantage of using base projects has to do with file deletion. Once a file is deleted in a project, it is not recoverable, unless they are also in the base project. Thus, having a base project can provide an overall safety net for the underlying files.\nOne final advantage of having a base project is that you can grant upload access to specific users to the base project. This is really helpful for when you have a sequencing group that needs to get raw data into your project."
  },
  {
    "objectID": "12-project_management.html#uploading-batch-files-with-the-upload-agent",
    "href": "12-project_management.html#uploading-batch-files-with-the-upload-agent",
    "title": "2  Project and File Management",
    "section": "2.6 Uploading Batch Files with the Upload Agent",
    "text": "2.6 Uploading Batch Files with the Upload Agent\nNow that we’ve created a base project, we’ll need to get our files into it. In our checklist, this can be done by the org admin, or an org user who has uploader access to the base project.\nThe DNAnexus Upload Agent software can be downloaded, installed, and used for automated batch uploading.\nThe Upload Agent is recommended over dx upload for large sets of files, because it is multi-threaded, and supports resuming in case an upload is interrupted. It will upload 1000 files at a time.\nIn particular, when uploading batches of files, such as everything that is in a folder, we recommend using the --tag or --property options to set metadata, such as tags."
  },
  {
    "objectID": "12-project_management.html#copying-files-from-one-project-to-another",
    "href": "12-project_management.html#copying-files-from-one-project-to-another",
    "title": "2  Project and File Management",
    "section": "2.7 Copying Files from One Project to Another",
    "text": "2.7 Copying Files from One Project to Another\n[Figure Here]\nCopying has very specific definition on the DNAnexus platform: it means copying a file from one project to another project. Copying doesn’t refer to duplicating a file within a project. You may be used to creating aliases or symbolic links within a project.\nWhen we copy a file from one project to another, a new physical copy is not made. The reference to the original file is copied. The data is identical and points to the same location on the platform, and the metadata is copied to the new project.\nThis is quite nice in that you are not doubly charged for storage on the platform within your org. You do have the ability to clone files into a different project - that way a new physical copy of the data is created.\n\n\n\n\n\n\nA Metadata Multiverse\n\n\n\nOnce the metadata is copied into the new project, there is no syncing of the metadata between the two projects.\nUser 1 is free to modify the metadata in Project A and changes are not made to the metadata in Project B. However, the underlying data does not change.\nRemember that the metadata for file objects includes file names, so you can actually change the file name in project B for the file object that you copied. This is not recommended, but it is possible.\nRegardless, the underlying file id for the file object will remain the same.\n\n\n\n\n\n\n\n\nWill I still be charged?\n\n\n\nWhat happens when you have two copies of a file in two different org owned projects? What happens when one of these projects is deleted?\nThere is a process on the platform that is scans file-ids and whether they exist in a project for your org. If a reference to that file-id still exists in your project, then you will still be charged for it.\nThis is also why having a file archiving strategy is important when managing costs on the platform."
  },
  {
    "objectID": "12-project_management.html#full-diagram",
    "href": "12-project_management.html#full-diagram",
    "title": "2  Project and File Management",
    "section": "2.8 Full Diagram",
    "text": "2.8 Full Diagram\n\n\n\n\nflowchart LR\n  A(Admin) -- manages--> B[Base Project\\ncontains all\\ndata]\n  B --Dispensed\\nby Admin-->C[Project A]\n  B --Dispensed\\nby Admin-->D[Project B]\n  B --Dispensed\\nby Admin-->E[Project C]\n  C --viewer-->G(Person 1)\n  D --admin-->G\n  E --contributor-->H(Person 2)"
  },
  {
    "objectID": "12-project_management.html#some-rules-of-thumb-for-data-files-in-a-project",
    "href": "12-project_management.html#some-rules-of-thumb-for-data-files-in-a-project",
    "title": "2  Project and File Management",
    "section": "2.9 Some Rules of Thumb for Data Files in a Project",
    "text": "2.9 Some Rules of Thumb for Data Files in a Project\nGiven this structure, you will want to avoid having too many files within a project.\nA good rule of thumb is to shoot for around 10,000 file objects total in a project. Going larger than this may impact the speed of your file searches.\n\n\n\n\n\n\nDestructive Processes on the DNAnexus platform\n\n\n\nThere are certain operations on the platform that are destructive; that is, there is no ability to undo that operation:\n\nFile / App / Workflow deletion\nProject deletion\n\nMaking your org users aware of these operations is important.\nThere is also the option to disallow deletion within a project to avoid issues like this. This is recommended especially in audit log projects to insure integrity of the audit logs."
  },
  {
    "objectID": "12-project_management.html#project-level-roles",
    "href": "12-project_management.html#project-level-roles",
    "title": "2  Project and File Management",
    "section": "3.1 Project Level Roles",
    "text": "3.1 Project Level Roles\n\n\n\n\n\n\nflowchart LR\n  B[Base Project\\ncontains all data] --Dispensed\\nby Admin-->C[Project A]\n  B --Dispensed\\nby Admin-->D[Project B]\n  B --Dispensed\\nby Admin-->E[Project C]\n  C ---|viewer|G(Person 1)\n  D ---|admin|G\n  E ---|contributor|H(Person 2)\n\n\n\n\n\n\n\n\nFigure 3.1: Possible roles within a project include Admin, Contributor, Uploader, and Viewer.\n\n\nThere are multiple roles that can be assigned to members of a project. In order of access (with each role inheriting privileges of the ones above):\n\n\n\nRole\nDescription\n\n\n\n\nViewer\nCan View Files and Apps within a Project\n\n\nUploader\nCan upload files within a project; limited file management\n\n\nContributor\nCan manage files and run apps/workflows\n\n\nAdmin\nTop level - can manage membership and delete project\n\n\n\nProject Administrators can also modify project-related flags for a project under the Settings Tab in a project. This includes:\n\nEnable PHI restrictions for a project, which supersedes any other flags set below\nEnable / Disallow file operations, including copying, uploading, and downloading\nTransfer Project Billing\nProject Deletion"
  },
  {
    "objectID": "12-project_management.html#a-suggested-project-structure",
    "href": "12-project_management.html#a-suggested-project-structure",
    "title": "2  Project and File Management",
    "section": "3.2 A Suggested Project Structure",
    "text": "3.2 A Suggested Project Structure\nThe DNAnexus platform is an object-based filesystem. That technically means that folders aren’t needed. However, they are extremely helpful in helping you group your work.\nFor example, when I’m starting a project I usually have the following folder structure:\nraw_data/    ## raw files\noutputs/     ## processed Files\napplets/     ## project-specific applets\nworkflows/.  ## project-specific workflows\nnotebooks/.  ## Jupyter Notebooks"
  },
  {
    "objectID": "12-project_management.html#dx-tag",
    "href": "12-project_management.html#dx-tag",
    "title": "2  Project and File Management",
    "section": "4.1 dx tag",
    "text": "4.1 dx tag\nWell, we’ve uploaded our files but forgot to tag them. We can apply tags to files using the dx tag command:\n% dx tag file-FpQKQk00FgkGV3Vb3jJ8xqGV blah\nIf we do a dx describe:\n% dx describe file-FpQKQk00FgkGV3Vb3jJ8xqGV\nResult 1:\nID                  file-FpQKQk00FgkGV3Vb3jJ8xqGV\nTags:               blah"
  },
  {
    "objectID": "12-project_management.html#always-add-a-project-context-to-your-file-ids",
    "href": "12-project_management.html#always-add-a-project-context-to-your-file-ids",
    "title": "2  Project and File Management",
    "section": "4.2 Always Add a Project Context to your File IDs",
    "text": "4.2 Always Add a Project Context to your File IDs\nWe’ve already discovered one way of selecting multiple files on the platform: dx find data (sec-dx-find). This is our main tool for selecting files with metadata.\nWhen we work with files outside of our current project, we might reference it by using a bare file-id. In general, we need to be careful when we use bare file IDs!\nWhy is this so? There is a search tree when we use a file ID that is not in our project and without a project context. The search over metadata is looking for a file object based on just file ID. Usually, when we provide a bare file ID, this search goes like the following:\n\nLook for the file in the current project\nLook at all files across all other projects (Very computationally expensive)\n\nIf you want to use the platform effectively, you want to avoid 2) at all costs, especially when working with a lot of files. The metadata server will take much longer to find your files.\nThe lesson here is when using file-ids from a different project, it is much safer (and faster overall) to put the project-id in front of your file id such as below:\nproject-XXXXX:file-YYYYYYYYY"
  },
  {
    "objectID": "13-spark-on-dnanexus.html#do-i-need-spark",
    "href": "13-spark-on-dnanexus.html#do-i-need-spark",
    "title": "3  Spark on the DNAnexus Platform",
    "section": "3.1 Do I need Spark?",
    "text": "3.1 Do I need Spark?\nAs we’ll discover, Spark is made for working with very large datasets. The datasets themselves might be very long (have billions of rows), or very wide (millions of columns).\nA lot of datasets aren’t this big. Especially with tools such as Apache Arrow, interactive analysis can handle data in the billion row range. Arrow support exists for both R (within the DBI/tidyverse/arrow packages), and Python (Pandas 2.0 now uses Arrow for its backend).\nGenomic Data is often the exception for this. Whole Genome Sequencing outputs usually have hundreds of millions of variants, and with datasets such as UK Biobank, this is multiplied by a very large number of participants. We’ll see in the next chapter that a framework exists for genomic analysis built on Spark called Hail.\n\n\n\n\n\n\nSpark/Hail on The Core Platform\n\n\n\nPlease note that working with Spark requires an Apollo License if you are on the Core Platform. For more information, please contact DNAnexus sales.\nIf you are on UKB RAP, the Pheno Data is stored as a Spark Database, so you do not have to have a license on RAP. You also have access to the Hail configurations of Spark JupyterLab as well."
  },
  {
    "objectID": "13-spark-on-dnanexus.html#spark-concepts",
    "href": "13-spark-on-dnanexus.html#spark-concepts",
    "title": "3  Spark on the DNAnexus Platform",
    "section": "3.2 Spark Concepts",
    "text": "3.2 Spark Concepts\nIn this section, we will discuss the basic concepts that you need to know before using Spark successfully on the platform.\nOn the platform, Spark is used to work with ingested Pheno Data either indirectly using the dx extract_dataset functionality, or by working directly with the Spark Database using Libraries such as PySpark (for Python) or sparklyr (for R), or using Spark SQL to query the contents of a database.\n\n\n\n\n\n\nPar-what-now?\n\n\n\nYou may have heard of Parquet files and wondered how they relate to database objects on the platform. They are a way to store data in a format called columnar storage.\nIt turns out it is faster for retrieval and searching to store data not by rows, but by columns. This is really helpful because the data is sorted by data type and it’s easier to traverse for this reason.\nApache Spark is the scalable solution to using columnar formats such as Parquet. There are a number of database engines that are fast at searching and traversing these types of files. Some examples are Snowflake and Apache Arrow.\nOn the DNAnexus platform, certain data objects (called Dataset and Database objects) are actually stored in Parquet format, for rapid searching and querying using Apache Spark.\n\n\n\n3.2.1 What is Spark?\nApache Spark is a framework for working with large datasets that won’t fit into memory on a single computer. Instead, we use what’s called a Spark Cluster. Specifically, we request the Spark Cluster as part of a Spark JupyterLab configuration.\nThe main pieces of a Spark Cluster are the Driver Node, and the executors, which correspond to individual cores on a Worker Node. Individual worker nodes have processes known as Executors that will run on their portion of the data.\nSpark lets us analyse large datasets by partitioning the data into smaller pieces that are made available to the cluster via the Hadoop File System. Each executor gains access to a piece of the data, and executes tasks assigned to it on this piece of the data.\nThe Driver Node directs the individual executors by assigning tasks. These individual tasks are part of an overall execution plan.\nThis link is a good introduction to Spark Terminology."
  },
  {
    "objectID": "13-spark-on-dnanexus.html#spark-and-lazy-evaluation",
    "href": "13-spark-on-dnanexus.html#spark-and-lazy-evaluation",
    "title": "3  Spark on the DNAnexus Platform",
    "section": "3.3 Spark and Lazy Evaluation",
    "text": "3.3 Spark and Lazy Evaluation\nOne thing that is extremely important to know is that Spark executes operations in a Lazy manner. This is opposed to something like R or Python which executes commands right away (a greedy manner).\nWhen we run Spark operations, we are usually building chains of operations."
  },
  {
    "objectID": "13-spark-on-dnanexus.html#the-four-filesystems",
    "href": "13-spark-on-dnanexus.html#the-four-filesystems",
    "title": "3  Spark on the DNAnexus Platform",
    "section": "3.4 The Four Filesystems",
    "text": "3.4 The Four Filesystems\nUsing Spark on DNAnexus effectively requires us to be familiar with 4 different file systems (yes, you heard that right):\n\ndxFUSE file system - used to directly access project storage. We access this using file:///mnt/project/ in our scripts. For example, if we need to access a file named chr1.tar.gz, we refer to it as file:///mnt/project/chr1.tar.gz. Mounted at the start of JupyterLab; to access\nDriver Node File System - our waypoint for getting things into and out of the Hadoop File System and possibly into Project Storage. Since we run JupyterLab on the Driver Node, we use hdfs:// to transfer from the Driver Node to the Hadoop File system (see below), and use dxFUSE to work with Project Storage files, and dx upload to transfer files back into project storage.\nHadoop File System (HDFS) - Distributed File System for the cluster. We access this using the hdfs:// protocol.\ndnax S3 Bucket - contains ingested data. This is a separate S3 bucket. We interact with ingested Pheno and Geno Data using a special protocol called dnax://. Ingested data that is stored here is accessed through connecting to the DNAX database that holds it. Part of why Cohort Browser works with very large data is that the ingested data is stored in this bucket.\n\nThe TL;DR (too long; didn’t read) to this all is:\n\nTry to avoid transfers through the Driver node (hdfs dfs -get/dx upload), as you are limited to the memory/disk space of the Driver node.\nWhen possible, use dxFUSE to directly stream files into HDFS (Spark Cluster Storage) from project storage by using file:///mnt/project/ URLs. You will need to remount dxFUSE in the JupyterLab container to do this.\nIf you need to get files out of HDFS into Project Storage, use dxFUSE in -limitedWrite mode to write into project storage by passing in file:///mnt/project/ URLs to the .write() methods in Spark/Hail.\nIf you need to persist your Spark DataFrame, use dnax:///<db_name>/<table_name> with .write() to write it into DNAX storage. Then use SparkSQL to save it in the Spark Database in DNAX.\n\n\n\n\n\n\n\nAccessing Spark Databases on DNAnexus\n\n\n\nOne issue I see a lot of people get tripped up with is with Database Access. One thing to know is that Spark databases cannot be moved from project to project. Once the data is ingested into a database object, it can’t be copied to a new project.\nFor a user to access the data in a Spark Database, they must have at least VIEW level access to the project that contains it on the platform.\nNote that Datasets (not Databases) can be copied to new projects. However, to actually access the data in Cohort Browser, the project that contains the database needs to be viewable by the user.\n\n\n\n\n\n\n\n\nSpark Cluster JupyterLab vs. Spark Apps\n\n\n\nOur main way of using Spark on DNAnexus is using the Spark JupyterLab app. This app will let you specify the instance types used in the Spark Cluster, the number of nodes in your Spark cluster, as well as additional features such as Hail or Variant Effect Predictor. This should be your main way of working with Spark DataFrames that have been ingested into Apollo.\nYou can also build Spark Applets that request a Spark Cluster. This is a more complicated venture, as it may require installing software on the worker nodes using a bootstrapping script, distributing data using hdfs dfs -put, and submitting Spark Jobs using dx-spark-submit."
  },
  {
    "objectID": "13-spark-on-dnanexus.html#balancing-cores-and-memory",
    "href": "13-spark-on-dnanexus.html#balancing-cores-and-memory",
    "title": "3  Spark on the DNAnexus Platform",
    "section": "3.5 Balancing Cores and Memory",
    "text": "3.5 Balancing Cores and Memory\nCentral to both Spark and Hail is tuning the number of cores and memory in each of our instances. This also needs to be balanced with the partition size for our data. Data can be repartitioned, although this may be an expensive operation.\nIn general, the number of data partitions should be greater than the number of cores. Why? If there are less partitions than cores, then the remaining cores will be unused.\nThere are a lot of other tuning considerations to keep in mind."
  },
  {
    "objectID": "13-spark-on-dnanexus.html#the-spark-ui",
    "href": "13-spark-on-dnanexus.html#the-spark-ui",
    "title": "3  Spark on the DNAnexus Platform",
    "section": "3.6 The Spark UI",
    "text": "3.6 The Spark UI\nWhen you start up Spark JupyterLab, you will see that it will open at a particular url which has the format https://JOB-ID.dnanexus.com. To open the Spark UI, use that same URL, but with a :8081 at the end of it. For example, the UI url would be https://JOB-ID.dnanexus.com:8081.\nNote that the Spark UI is not launched until you connect to Spark or Hail.\nThe Spark UI is really helpful in understanding how Spark translates a series of operations into a Plan, and how it executes that plan."
  },
  {
    "objectID": "13-spark-on-dnanexus.html#extracting-phenodata-with-dx-extract_dataset",
    "href": "13-spark-on-dnanexus.html#extracting-phenodata-with-dx-extract_dataset",
    "title": "3  Spark on the DNAnexus Platform",
    "section": "3.7 Extracting PhenoData with dx extract_dataset",
    "text": "3.7 Extracting PhenoData with dx extract_dataset\nFirst of all, you may not need to use Spark JupyterLab if you just want to extract Pheno Data from a Dataset and you know the fields.\nThere is a utility within dx-toolkit called dx extract_dataset that will retrieve pheno data in the form of a CSV. It uses Spark, but the Spark is executed by the Thrift Server (the same Spark Cluster that serves the data for Cohort Browser).\nYou will need a list of field ids to retrieve them.\nHowever, there are good reasons to work with the Spark Database directly. Number one is that your Cohort query will not run on the Thrift Server. If your query is complex and takes two minutes to execute, the Thrift Server will timeout. In that case, you need to run your own Spark JupyterLab cluster and extract the data using Spark itself."
  },
  {
    "objectID": "13-spark-on-dnanexus.html#connecting-with-pyspark-for-python-users",
    "href": "13-spark-on-dnanexus.html#connecting-with-pyspark-for-python-users",
    "title": "3  Spark on the DNAnexus Platform",
    "section": "3.8 Connecting with PySpark (For Python Users)",
    "text": "3.8 Connecting with PySpark (For Python Users)\nThe first thing we’ll do to connect to the Spark database is connect to it by starting a Spark Session. Make sure you only do this once. If you try to connect twice, Spark will throw an error.\nIf this happens, make sure to restart your notebook kernel.\n```{python}\nimport pyspark\nsc = pyspark.SparkContext()\nspark = pyspark.sql.SparkSession(sc)\n```\n\n3.8.1 Running SparkSQL Queries in PySpark\nThe basic template for running Spark Queries is this:\n```{python}\nretrieve_sql = 'select .... from .... '\ndf = spark.sql(retrieve_sql)\n```\n\n\n3.8.2 Koalas is the Pandas version of Spark\nIf you are familiar with Pandas, the Koalas module (from Databricks) provides a Pandas-like interface to SparkSQL queries. This way, you don’t have to execute native Spark commands or SparkSQL queries.\nOnce you have a Spark DataFrame, you can convert it to a Koalas one by using the .to_koalas() method.\n```{python}\ndf_koalas = df.to_koalas()\n```"
  },
  {
    "objectID": "13-spark-on-dnanexus.html#connecting-with-sparklyr-for-r-users",
    "href": "13-spark-on-dnanexus.html#connecting-with-sparklyr-for-r-users",
    "title": "3  Spark on the DNAnexus Platform",
    "section": "3.9 Connecting with sparklyr (For R Users)",
    "text": "3.9 Connecting with sparklyr (For R Users)\nYou’ll need to install the package sparklyr along with its dependencies to work with the Spark DataFrames directly with R.\n```{r}\nlibrary(DBI)\nlibrary(sparklyr)\nport <- Sys.getenv(\"SPARK_MASTER_PORT\")\nmaster <- paste(\"spark://master:\", port, sep = '')\nsc = spark_connect(master)\n```\n```{r}\nretrieve_sql <- 'select .... from .... '\ndf = dbGetQuery(sc, retrieve_sql)\n```"
  },
  {
    "objectID": "14-hail-on-dnanexus.html#what-is-hail",
    "href": "14-hail-on-dnanexus.html#what-is-hail",
    "title": "4  Hail on DNAnexus",
    "section": "4.1 What is Hail?",
    "text": "4.1 What is Hail?\nHail is a genomics framework built on top of Spark to allow for scalable genomics queries. It uses many of the same concepts (partitions, executors, plans) but it is genomics focused.\nFor example, the Hail MatrixFrame can be QC’ed, and then filtered on these QC metrics."
  },
  {
    "objectID": "14-hail-on-dnanexus.html#important-data-structures-in-hail-tables-and-matrixtables",
    "href": "14-hail-on-dnanexus.html#important-data-structures-in-hail-tables-and-matrixtables",
    "title": "4  Hail on DNAnexus",
    "section": "4.2 Important Data Structures in Hail: Tables and MatrixTables",
    "text": "4.2 Important Data Structures in Hail: Tables and MatrixTables\nBefore we get started with doing a GWAS in Hail, let’s talk about the data structures we’ll work with. Hail Tables and Hail MatrixTables.\nOne of the hardest things to understand about Hail data structures is that they are actually linked data structures. It’s easiest to think of them as data tables with attached metadata.\n\n4.2.1 Hail Table\nThe fundamental data structure in Hail is the Hail Table.\nI think of Hail Tables as a regular data table with a set of metadata fields. These metadata fields are known as global fields.\n\n\n4.2.2 Hail MatrixTable\nIn contrast, Hail MatrixTables consist of the entries (genotypes) that have metadata attached to both the rows and columns.\nThe row metadata is called row fields, which correspond to the variants in the MatrixTable. Operations\nThe column metadata is called column fields, which correspond to the samples in the MatrixTable."
  },
  {
    "objectID": "14-hail-on-dnanexus.html#keep-in-mind",
    "href": "14-hail-on-dnanexus.html#keep-in-mind",
    "title": "4  Hail on DNAnexus",
    "section": "4.3 Keep in Mind",
    "text": "4.3 Keep in Mind\nThe cardinal rule of MatrixTables is this:\n\nRow operations operate on variants, while column operations operate on samples.\n\nIt is easy to get lost in a chain of Hail commands and not be able to understand what the code is doing unless you concentrate on this.\n\n\n\n\n\n\nStoring MatrixTables in DNAX\n\n\n\nIn general, if you are loading pVCF files, we suggest that you save the MatrixTable into DNAX. This is because loading pVCF files is an expensive operation compared to BGEN files.\nHowever, we suggest doing filtering and QC of your variants before you save your MatrixTable to DNAX. In general, you will be doubly charged for MatrixTable storage along with your pVCF file storage, so it is worth reducing the data stored in DNAX.\nNote that BGEN files can be loaded from"
  },
  {
    "objectID": "14-hail-on-dnanexus.html#section",
    "href": "14-hail-on-dnanexus.html#section",
    "title": "4  Hail on DNAnexus",
    "section": "4.4 ",
    "text": "4.4"
  }
]