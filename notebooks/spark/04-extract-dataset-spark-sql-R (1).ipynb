{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Using Spark/`{sparklyr}` to extract datasets\n","\n","Run this notebook with a Spark JupyterLab Cluster with at least:\n","\n","- 4 nodes (`mem1_ssd1_v2_v4`)\n","- `Hail` configuration\n","\n","If our query is takes longer than 2 minutes to execute, `dx extract_dataset` will timeout. In that case, we will start up a Spark instance of JupyterLab and run the SparkSQL directly in the notebook.\n","\n","1. Extract the relevant dictionary information\n","2. Connect to our Spark Cluster using `sparklyr`\n","3. Extract the dataset SQL using the `--sql` option with `dx extract_dataset` and save output to a file\n","4. Load query from file, clean the SQL\n","5. Use `dbGetQuery()` to load the dataset as a `data.frame`\n","6. Use `readr::write_csv()` to save `data.frame` as CSV file\n","7. Upload our CSV file back to project storage using `dx upload`.\n","\n","Optionally, we can:\n","\n","1. Make our Query a temporary table in Spark\n","2. Filter our table in Spark\n","3. Utilize Spark methods (Machine Learning) on our Spark table\n","4. Export our Spark table for further work"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["#install.packages(\"sparklyr\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## Generate SQL from `dx extract_dataset`\n","\n","If we use the `--sql` flag, the query will not execute, but the SparkSQL needed to execute the query will be saved. \n","\n","We'll use the `-o` option to save our query as `cohort.sql`."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["system(\"dx extract_dataset record-G5Ky4Gj08KQYQ4P810fJ8qPp -ddd\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Then we can pass in a list of entity/fields to `dx extract dataset` along with the `-sql` option to retrieve the SQL we need to retrieve our cohort. We'll use `glue::glue()` to build our `dx extract_dataset()` command and then execute our command in Bash using `system()`."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["field_list <- 'participant.eid,participant.p21022,participant.p1508_i0,participant.p1508_i1,participant.p1508_i2,participant.p41202,participant.p31,participant.p41226,participant.p3159_i0,participant.p3159_i1,participant.p3159_i2'\n","\n","cohort <- \"record-G5Ky4Gj08KQYQ4P810fJ8qPp\"\n","\n","cohort_template <- glue::glue(\"dx extract_dataset {cohort} --fields {field_list} --sql -o cohort.sql\")\n","\n","cohort_template\n","\n","system(cohort_template)\n","list.files()"]},{"cell_type":"markdown","metadata":{},"source":["## Connect to Spark Cluster\n","\n","The first thing we need to do is connect to the Spark Cluster. We'll use the `{sparklyr}` and `{DBI}` packages to connect.\n","\n","Make sure to only connect once - Spark will throw an error if you try to connect twice by rerunning this code block.\n","\n","To fix this, you will need to restart the kernel using **Kernel > Restart Kernel*"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["library(sparklyr)\n","library(DBI)\n","port <- Sys.getenv(\"SPARK_MASTER_PORT\")\n","master <- paste(\"spark://master:\", port, sep = '')\n","sc = spark_connect(master)"]},{"cell_type":"markdown","metadata":{},"source":["## Load SQL from file\n","\n","We'll read the `cohort.sql` file using `read_file()` and then execute the SQL using `dbGetQuery()`.\n","\n","Here's what the actual query looks like:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["library(readr)\n","retrieve_sql <-read_file(\"cohort.sql\")\n","retrieve_sql <- gsub(\"[;\\n]\", \"\", retrieve_sql)\n","retrieve_sql"]},{"cell_type":"markdown","metadata":{},"source":["## Execute Our Query with DBI/sparklyr\n","\n","Now we can execute our SQL with `dbGetQuery()`. Note we utilize our Spark Connection (`sc`) to execute it.\n","\n","Note that the `data.frame` returned by `dbGetQuery()` is not identical to the `.csv` file generated by `dx extract_dataset`. \n","\n","`dbGetQuery()` returns columns that have multiple values per lines as list columns. `{xvhelper}` can transform these columns into comma separated strings."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["df <- dbGetQuery(sc, retrieve_sql)\n","head(df)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["library(xvhelper)\n","\n","    \n","data_dict <- readr::read_csv(\"female_coffee_3.0.data_dictionary.csv\", show_col_types = FALSE)\n","coding_dict <- readr::read_csv(\"female_coffee_3.0.codings.csv\", show_col_types=FALSE)\n","\n","merged_dict <- xvhelper::merge_coding_data_dict(coding_dict, data_dict)\n","\n","decoded <- df |>\n"," xvhelper::decode_df(merged_dict)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["head(decoded)"]},{"cell_type":"markdown","metadata":{},"source":["## Decode Column Names\n","\n","We can also decode the column names:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["decoded_cols <- decoded |>\n","    xvhelper::decode_column_names(merged_dict)\n","\n","head(decoded_cols)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Working with Extracted Pheno Data in Spark\n","\n","We can also load our data as a temporary table in Spark, which can be very useful when working with very large datasets.\n","\n","This lets us leverage the power of Spark and the `{sparklyr}` package.\n","\n","We'll first take our SQL query and use `CREATE TEMPORARY VIEW` to create a temporary table as a Spark DataFrame. Then we can do our Spark operations on this temporary table."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["temp_table_sql <- glue::glue(\"CREATE TEMPORARY VIEW pheno AS {retrieve_sql}\")\n","\n","result <- DBI::dbSendQuery(sc, temp_table_sql)"]},{"cell_type":"markdown","metadata":{},"source":["Now we can register the data frame (`pheno`) as a Spark dataframe using `dplyr::tbl()`. \n","\n","Once we do that, we're able to leverage `sparklyr` to do `dplyr` like filtering:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["new_df <- dplyr::tbl(sc, \"pheno\")\n","\n","filtered_df <- new_df |> dplyr::filter(`participant.p21022` > 50)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["head(filtered_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["colnames(new_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["new_df |> \n","    dplyr::mutate(`participant.p31` = as.character(`participant.p31`)) |>\n","    dplyr::count(`participant.p31`)\n"]},{"cell_type":"markdown","metadata":{},"source":["Once we're done with working with `filtered_df` in spark, we can pull it into memory using `collect()`. Then we can annotate the data as usual."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["in_memory <- collect(filtered_df)\n","head(in_memory)"]}],"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"codemirror_mode":"r","file_extension":".r","mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"4.2.0"}},"nbformat":4,"nbformat_minor":4}
