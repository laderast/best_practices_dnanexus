{"metadata": {"kernelspec": {"name": "ir", "display_name": "R", "language": "R"}, "language_info": {"name": "R", "codemirror_mode": "r", "pygments_lexer": "r", "mimetype": "text/x-r-source", "file_extension": ".r", "version": "4.2.0"}}, "nbformat_minor": 5, "nbformat": 4, "cells": [{"cell_type": "markdown", "source": "# \u201cdx extract_dataset\u201d in R\n<hr/>\n***As-Is Software Disclaimer***\n\nThis content in this repository is delivered \u201cAs-Is\u201d. Notwithstanding anything to the contrary, DNAnexus will have no warranty, support, liability or other obligations with respect to Materials provided hereunder.\n\n<hr/>\n\nThis notebook demonstrates usage of the dx command `extract_dataset` for:\n* Retrieval of Apollo-stored data, as referenced within entities and fields of a Dataset or Cohort object on the platform\n* Retrieval of the underlying data dictionary files used to generate a Dataset object on the platform\n\n<a href=\"https://github.com/dnanexus/OpenBio/blob/master/LICENSE.md\">MIT License</a> applies to this notebook.", "metadata": {}}, {"cell_type": "markdown", "source": "## Preparing your environment\n### Launch spec:\n\n* App name: JupyterLab with Python, R, Stata, ML ()\n* Kernel: R\n* Instance type: Spark Cluster - mem1_ssd1_v2_x2, 3 nodes \n* Snapshot: `/.Notebook_snapshots/jupyter_snapshot.gz`\n* Cost: < $0.2\n* Runtime: =~ 10 min\n* Data description: Input for this notebook is a v3.0 Dataset or Cohort object ID", "metadata": {}}, {"cell_type": "markdown", "source": "### Install dxpy\nextract_dataset requires dxpy version >= 0.329.0. If running the command from your local environment (i.e. off of the DNAnexus platform), it may be required to also install pandas. For example, pip3 install -U dxpy[pandas]", "metadata": {}}, {"cell_type": "code", "source": "system(\"pip3 show dxpy\", intern = TRUE)", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "### Install tidyverse for data processing\n\nQuick note - you will need to read the licenses for the tidyverse in order to make sure whether you and your group are comfortable with the licensing terms.\n\nIf you loaded the snapshot in this project, all of these packages and dependencies are also installed, so you don't need to install them again.", "metadata": {}}, {"cell_type": "code", "source": "install.packages(c(\"readr\", \"stringr\", \"dplyr\", \"glue\", \"reactable\", \"janitor\", \"remotes\"))\nremotes::install_github(\"laderast/xvhelper\")", "metadata": {"trusted": true}, "execution_count": 1, "outputs": [{"name": "stderr", "text": "Installing packages into \u2018/usr/local/lib/R/site-library\u2019\n(as \u2018lib\u2019 is unspecified)\n\nalso installing the dependencies \u2018bit\u2019, \u2018bit64\u2019, \u2018progress\u2019, \u2018hms\u2019, \u2018vroom\u2019, \u2018tzdb\u2019, \u2018vctrs\u2019, \u2018reactR\u2019, \u2018snakecase\u2019\n\n\nDownloading GitHub repo laderast/xvhelper@HEAD\n\n", "output_type": "stream"}, {"name": "stdout", "text": "Rcpp       (1.0.9 -> 1.0.10) [CRAN]\nutf8       (1.2.2 -> 1.2.3 ) [CRAN]\nfansi      (1.0.3 -> 1.0.4 ) [CRAN]\nstringi    (1.7.8 -> 1.7.12) [CRAN]\ntibble     (3.1.8 -> 3.2.0 ) [CRAN]\npurrr      (0.3.5 -> 1.0.1 ) [CRAN]\ncli        (3.4.1 -> 3.6.0 ) [CRAN]\ntimechange (0.1.1 -> 0.2.0 ) [CRAN]\npng        (NA    -> 0.1-8 ) [CRAN]\nhere       (NA    -> 1.0.1 ) [CRAN]\nRcppTOML   (NA    -> 0.2.2 ) [CRAN]\ntidyr      (1.2.1 -> 1.3.0 ) [CRAN]\nlubridate  (1.9.0 -> 1.9.2 ) [CRAN]\nreticulate (NA    -> 1.28  ) [CRAN]\n", "output_type": "stream"}, {"name": "stderr", "text": "Installing 14 packages: Rcpp, utf8, fansi, stringi, tibble, purrr, cli, timechange, png, here, RcppTOML, tidyr, lubridate, reticulate\n\nInstalling packages into \u2018/usr/local/lib/R/site-library\u2019\n(as \u2018lib\u2019 is unspecified)\n\n", "output_type": "stream"}, {"name": "stdout", "text": "\u001b[36m\u2500\u2500\u001b[39m \u001b[36mR CMD build\u001b[39m \u001b[36m\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u001b[39m\n* checking for file \u2018/tmp/RtmpJskvRy/remotes734410c4e0/laderast-xvhelper-6e0873a/DESCRIPTION\u2019 ... OK\n* preparing \u2018xvhelper\u2019:\n* checking DESCRIPTION meta-information ... OK\n* checking for LF line-endings in source and make files and shell scripts\n* checking for empty or unneeded directories\n* building \u2018xvhelper_0.0.100.tar.gz\u2019\n\n", "output_type": "stream"}, {"name": "stderr", "text": "Installing package into \u2018/usr/local/lib/R/site-library\u2019\n(as \u2018lib\u2019 is unspecified)\n\n", "output_type": "stream"}]}, {"cell_type": "markdown", "source": "### Import packages", "metadata": {}}, {"cell_type": "code", "source": "library(dplyr)\nlibrary(readr)\nlibrary(stringr)\nlibrary(glue)\nlibrary(reactable)\nlibrary(xvhelper)", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "### 1. Assign environment variables", "metadata": {}}, {"cell_type": "code", "source": "# The referenced Dataset is private and provided only to demonstrate an example input. The user will need to supply a permissible and valid record-id\n# Assign project-id of dataset\n# Assign dataset record-id\nrid <- 'record-G5Ky4Gj08KQYQ4P810fJ8qPp'\n# Assign joint dataset project-id:record-id\ndataset <- rid", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "### 2. Call \u201cdx extract_dataset\u201d using a supplied dataset", "metadata": {}}, {"cell_type": "markdown", "source": "We'll use the `{glue}` package to put our bash commands together for `dx extract_dataset`, and use `system()` to execute our bash code.\n\n`glue::glue()` has the advantage of not needing to `paste()` together strings. The string substitution is cleaner.", "metadata": {}}, {"cell_type": "code", "source": "cmd <- glue::glue(\"dx extract_dataset {dataset} -ddd\")\n\ncmd", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "Let's execute our command using `system()` and then we will list the files that result using `list.files()`. We generate three files in the directory in JupyterLab storage:\n\n- *dataset_name*`.codings.csv`\n- *dataset_name*`.data_dictionary.csv`\n- *dataset_name*`.entity_dictionary.csv`", "metadata": {}}, {"cell_type": "code", "source": "system(cmd)\nlist.files()", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "#### Preview data in the three dictionary (*.csv) files", "metadata": {}}, {"cell_type": "code", "source": "#codings_file <- system(\"ls *.codings.csv\", intern = TRUE)\ncodings_file <- list.files(pattern=\"*.codings.csv\")\ncodings_df <- read_csv(codings_file, show_col_types = FALSE)\nhead(codings_df)", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "entity_dict_file <- system(\"ls *.entity_dictionary.csv\", intern=TRUE)\nentity_dict_df <- read_csv(entity_dict_file, show_col_types = FALSE)\nhead(entity_dict_df)", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "### Understanding the Data Dictionary File\n\nThe data dictionary is the glue for the entire dataset. It maps:\n\n- Entity to Fields\n- Fields to Codings\n- Entity to Entity\n\nWe'll use the data dictionary to understand how to building our list of fields, and later, we'll join it to the codings file to build a list of fields and their coded values.\n\nThere are more columns to the data dictionary, but let's first see the `entity`, `name`, `title`, and `type` columns:", "metadata": {}}, {"cell_type": "code", "source": "#data_dict_file <- system(\"ls *.data_dictionary.csv\", intern=TRUE)\ndata_dict_file <- list.files(pattern=\"*.data_dictionary.csv\")\ndata_dict_df <- read_csv(data_dict_file, show_col_types = FALSE)\ndata_dict_df <- data_dict_df \n\ndata_dict_df %>%\n        select(entity, name, title, type) %>%\n        head()", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "### 3. Parse returned metadata and extract entity/field names", "metadata": {}}, {"cell_type": "markdown", "source": "Let's search for some fields. We want the following fields:\n\n- `Coffee intake | instance 0`\n- `Sex` (Gender)\n- `Smoked cigarette or pipe within last hour | Instance 0`\n\nWe can use the `{reactable}` package to make a searchable table of the data dictionary. This will help in finding fields.\n\nNote the search box in the top right of the table - when we have many fields, we can use the search box to find fields of interest. Try searching for `Coffee intake` and see what fields pop up.", "metadata": {}}, {"cell_type": "code", "source": "data_dict_df <- data_dict_df %>%\n    relocate(name, title) %>%\n    mutate(ent_field = glue::glue(\"{entity}.{name}\"))\n\nbasic_data_dict <- data_dict_df |>\n                    select(title, name, entity, ent_field, coding_name, is_multi_select, is_sparse_coding)\n\nreactable::reactable(basic_data_dict, searchable = TRUE)", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "Another strategy for searching fields: we can use `grepl` within `dplyr::filter()` to search for fields that match our criteria.\n\nNote we're chaining the `grepl` statements with an OR `|`.\n\nWe're also concatenating `entity` and `name` to a new variable, `ent_field`, which we'll use when we specify our list of fields.", "metadata": {}}, {"cell_type": "code", "source": "filtered_dict <- data_dict_df %>%\n    filter(grepl(\"Coffee type\", title) | \n           grepl(\"Sex\", title) | \n           grepl(\"Smoked\", title) | \n           grepl(\"Age at recruitment\", title) |\n           grepl(\"main ICD10\", title) |\n           grepl(\"Types of transport\", title)\n          ) %>%\n    arrange(title) \n\nfiltered_dict %>%\n    select(name, title, ent_field)", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "Let's use this subset of fields - we'll pull the `ent_field` column, and paste it together into a single comma delimited string using `paste`:", "metadata": {}}, {"cell_type": "code", "source": "field_list <- filtered_dict %>%\n    pull(ent_field)\n\n#field_list <- field_list[200:210]\nfield_list <- paste(field_list, collapse = \",\")\nfield_list", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "### 4. Use extracted entity and field names as input to the called function, \u201cdx extract_dataset\u201d and extract data\n\nAgain, we'll use `glue()` here for cleaner string substitution.\n\nWe'll extract the cohort information to a file called `cohort_data.csv` and work with this file for the rest of the notebook.", "metadata": {}}, {"cell_type": "code", "source": "cohort <- \"record-G5Ky4Gj08KQYQ4P810fJ8qPp\"\ncohort_template <- \"dx extract_dataset {cohort} --fields {field_list} -o cohort_data.csv\"\ncmd <- glue::glue(cohort_template)\n\ncmd\n\nsystem(cmd)", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "#### Preview data in the retrieved data file\n\nWe'll see that the retrieved data contains the integer and character codes. These must be decoded (see below):", "metadata": {}}, {"cell_type": "code", "source": "data_df <- read_csv(\"cohort_data.csv\", show_col_types = FALSE)\nhead(data_df)", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "## Decoding columns with xvhelper\n\nxvhelper is a little R package that will return the actual values of the returned data.\n\nTo use it, you build a `coded_col_df` using `merge_coding_data_dict()` and then translate the categorical columns to values using `decode_categories()`, and then change the column names to R friendly clean ones using `decode_column_names()`.\n\nNote that we need to run `decode_df()` before we run `decode_category()`", "metadata": {}}, {"cell_type": "code", "source": "#install via remotes::install_github()\n#install.packages(\"remotes\")\nremotes::install_github(\"laderast/xvhelper\")\n\nlibrary(xvhelper)", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "coded_col_df <- xvhelper::merge_coding_data_dict(coding_dict = codings_df, data_dict = data_dict_df)\n\ndecoded <- data_df %>%\n    xvhelper::decode_single(coded_col_df) |>\n    xvhelper::decode_multi_purrr(coded_col_df) |>\n    xvhelper::decode_column_names(coded_col_df, r_clean_names = FALSE)\n    \nhead(decoded)", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "write.csv(decoded, file=\"cohort_decoded.csv\")", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "### Save Output to Project", "metadata": {}}, {"cell_type": "code", "source": "system(\"dx upload *.csv --destination /users/tladeras/\")", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}]}